[{"content":"你好！欢迎来到我的小星球。\n我是Selaphina。能来访这里的人并不多，谢谢你对我的好奇与关心。我想，每一个来客在此相遇都有独特的意义。\nIntroduction 2022.09-2026.07：中国传媒大学\t信息与通信工程学院\t数字媒体技术\n2026.09-2029.07：厦门大学\t电影学院\t计算机科学与技术\nProject 2024.08-2024.11: 基于unity开发的2D 横版平台冒险剧情向游戏 简介视频\n2025.06-2025.07：RPAIchat-虚拟角色AI双语对话系统 项目地址\n2025.05-2025.06：基于UE5的可交互风格化洞穴水体场景 演示视频\n封面来源：lof @-Syome-\n","date":"2025-09-25T22:10:30Z","image":"https://Selaphina.github.io/p/about-me/cover_hu_6f55dcf4c3ad57f3.jpg","permalink":"https://Selaphina.github.io/p/about-me/","title":"About me"},{"content":"Ae是什么？ AE 全称 Adobe After Effects，它是一个 视频特效与动态图形设计软件。 最主要完成的三件事：\n动画！（动画制作） 特效！（爆炸、粒子、灯光、抖动等） 合成！（把图片、视频、文字、素材组合成一个画面） 使用门槛：\n搞懂两个基本概念就可以。使用门槛其实比想象更低，学 30 分钟做出简单动画完全没问题滴。\n图层（PS/Procreate/Sai2等等，有多图层的工具都同理） 关键帧（剪映/PR/Unity/Blender等等，有时间轴的工具都同理） 强力推荐的入门教程 为什么要用Ae？ 招新宣传片\n社团/活动开场动画\n前后缀动态效果\n动态海报（招生、晚会、文创）\nLOGO 动画\n主视觉 Key visual 动画化\n活动倒计时、字幕条、弹幕效果\nUP 主、运营团队\n学会 AE 简历更亮\n给自推/OC/CP/CB/自设等等 拉磨产粮罢了。 高端的食材往往只需要最朴素的烹饪方式。\n以上是本人从纯萌新开始，边学边产出，用ae做的小样例。\nAE对于制作动态效果，是自由度非常高，非常称手的工具。\n可以和PS，AI ( Adobe Illustrator ) 等adobe家的工具配套使用，非常方便。\n那么不用AE可以做到吗？\n自然是可以的，工具只是工具，掌握了方法之后，用什么都可以做创意。 AE之外的工具不在本次教程的讨论范围内。 1.基本介绍： 1.什么是合成（ctrl+K 合成设置-主要是做长度控制） 2.常用尺寸1920x1080 / 前缀尺寸: 1079 x 607 3.背景颜色默认，是透明的，不会显示。 4.图层 5.锚点的概念（放缩/旋转的中心点） 6.快捷键： 位移（P） 缩放（S） 旋转（R） 不透明度（T） 所有有变化的属性关键帧 （U） 工具/快捷键 0.窗口设置\n1.按住alt键轻敲工具栏图标即可切换常用功能\n2.ctrl + shift + D 切断素材\n3.选中所有关键帧+alt键：等比例缩放\n4.顶部菜单栏——动画——关键帧辅助——时间反向关键帧\n5.ctrl+shift+C预合成\n6.消隐\n7.运动模糊\n2.文字动画 注意对齐方式，决定了动画的间距从何处放大/缩小.\n什么是线性动画?什么是非线性动画? 从图表可见, 关键帧动画默认是线性的,即匀速运动.\n再次点击图表,返回关键帧时间轴,选中所有关键帧并按F9, 加上缓动效果.\n此时的图表:\n夸张一点,动画效果更明显.\n将图表类型改为速度:\n注意：当选择了某个内容的同时,点击形状,此时添加到画布上的是蒙版,而非真正的形状.\n什么是蒙版? 就是:\n蒙版: 自行选择区域\u0026ndash;只展示这个区域.\n轨道遮罩 跟蒙版的作用类似,比蒙版更灵活一点.\n3. 随堂作业-水杯装水 超级简单的小测试。\n**限时：**5-10min\n目标：\n结合上述教程，完成一个水杯装水的小动画。\n4.效果控件 比如：高斯模糊\n加关键帧即可，实现一个模糊-清晰的入场。\n比如：发光效果\n设置发光半径和发光强度即可。\n5. 进阶操作 如何导入psd 这样就可以按照PS分好的图层进行移动了。\n图钉工具 大名鼎鼎的图钉工具。新手非常容易上手。可以做飘动的头发丝，叶子，衣服，丝带，挥动的手臂，抬腿，点头，呼吸等等，一切你能想到的，和你想不到的。\n什么是预合成？ 预合成, 就是把多个图层“打包成一个\u0026quot;小合成 \u0026ldquo;，让画面更干净、方便统一加动画或特效的一种分组操作。\n为了让叶子和影子一起动，先选中对应叶子+影子，shift + ctrl + C，预合成。\n接下来，对整个预合成打图钉，并给图钉添加关键帧即可。非常简单。\n怎么删除：\n在画面窗口中 点击要删除的图钉（选中后会变成黄色/高亮）\n按键盘 Delete / Backspace → 那个钉子会被删除。\n变形过程记得拖动时间轴，打上关键帧。\n循环表达式 将鼠标移动至关键帧动画对应的操纵点的【秒表标志】上，按住alt的同时点秒表标志。\n输入\n1 loopOut(\u0026#34;pingpong\u0026#34;) 或者\n1 loopOut(\u0026#34;cycle\u0026#34;) 即可实现来回循环（pingpong）或者单向循环（cycle）动画。\npingpong 和 cycle的区别\npingpong ： 1-2-3-4—4-3-2-1——1-2-3-4——……\ncycle : 1-2-3-4—1-2-3-4——1-2-3-4——……\n抖动表达式 1 wiggle(1, 12) 意思是：每秒抖动 1 次，最大偏移 12像素。 用于：手持镜头晃动感、抖动标题、光点晃动，等等，这里用来实现泳圈漂移。\n5.路径动画\n1.普通形状or图片图层\n正常k位移动画，选中位移关键帧后，图层——变换——自动定向\n2.文字路径动画\n步骤 1：创建文字\n新建文字图层 步骤 2：给文字图层画一条路径（Mask）\n选中文字图层 选择钢笔工具（G）, 注意，钢笔画出曲线：定点的同时，按住鼠标不松开并拖动。 在文字图层上画一条曲线/直线 画完后，你会看到一个 Mask（蒙版） 说明： 虽然你是在文字图层上画蒙版，但 AE 允许把蒙版作为文字路径来使用。\n步骤 3：启用文字的 Path Options\n展开文字图层：文本——路径选项 将 None 改成你的 蒙版1 文字就会自动贴着你画的那条路径了！\n步骤 4：做“沿路径移动”的动画（核心）\n仍在 Path Options 找到 First Margin（首端边距） 这是控制文字在路径上的位置的参数。\n6.导出 修改路径和文件名后，别忘了点右上角的渲染，然后才开始导出。\n注意，ae内只能导出avi文件，比较大，该尺寸10s视频大概到1-2G以上。\n转换格式： 下载ffmpeg：\nFFmpeg 是一个用于处理视频、音频等多媒体文件的开源工具包。它支持几乎所有的多媒体格式转换、剪辑和编辑，是开发者和多媒体工作者必备的工具。\n打开 Dpwnload FFmpeg 官网，选择安装包Windows builds from gyan.dev\n下滑找到release bulids部分，选择ffmpeg-7.0.2-essentials_build.zip\nffmpeg-8.0-essentials_build.7z 打开文件夹，复制bin文件的路径，并添加进系统的环境变量中。\n1 D:\\迅雷下载\\ffmpeg-8.0-essentials_build\\bin 打开avi所在文件的根目录，将avi视频压缩后再转换格式。\n1 ffmpeg -i input.avi -vf \u0026#34;fps=15,scale=640:-1:flags=lanczos\u0026#34; -loop 0 output.gif 原文链接：https://blog.csdn.net/Natsuago/article/details/143231558\n前后缀的规范： **前缀尺寸：**1079 x 607 **元素要求：**有学校的元素/CUC STU/48 th (届数)，颜色不宜过多（2-3种），其他可以相对自由发挥！ **时间要求：**时长7-10s是ok的，视觉上尽量快速，吸睛。 **文件大小：**10MB以内（微信公众平台限制） **1.后缀尾图：**1280 x 1416\n**2.往期回顾：**宽1080，高不限定。\n**3.内容要求：**配色和风格互相呼应；微博和公众号的二维码；参考往届的文字信息。\n4.注意事项：\n后缀一共4张图。 后缀尾图：1张 往期回顾：3张 往期回顾3张都是透明底。 设计的元素不要压在图片上，否则换图会很麻烦。 制作过程中，记得检查深色模式和浅色模式是不是都可以显示，用两种方式都比较显眼的灰色。 作业（选做） 1.上手试试做个前缀吧！7-10s。\n2.把自己手上现有的一张/多张图改成动态版，时长不限。\n一般流程：\n1.定主题颜色（2-3种）\n2.找参考，小红书/b站……拆解分析一个ae动态视频/动态效果/动态海报的各个环节用到了哪些技巧。对应找教程。\n3.写个简单的分镜草稿，1-3个画面即可，过审一下。\n4.ps/ai/ae开搞！\n附录 Me未安装（已经安装但AE找不到路径） 解决方案：\n注意：AE或者PR和AME的版本要一致，比如都是2020版的才能实现。\n使用mklink命令在命令提示符处创建符号链接\n1.左下角搜索框中输入cmd，右键以管理员身份运行（很重要不然会被拒绝访问）\n或者在文件夹C:\\WINDOWS\\system32中找到cmd.exe右键以管理员身份运行。\n2.在cmd窗口中输入命令行\n按照所安装的media encoder的版本全称和更改的安装路径输入，比如：\nAdobe Media Encoder 2020修改的安装地址是在D:\\Program Files\\Adobe\n1 mklink /J \u0026#34;C:\\Program Files\\Adobe\\Adobe Media Encoder 2020\u0026#34; \u0026#34;F:\\Program Files (x86)\\Adobe\\ME2021\\Adobe Media Encoder 2020\u0026#34; 3.操作成功时显示\n1 为 C:\\Program Files\\Adobe\\Adobe Media Encoder 2020 \u0026amp;lt;\u0026amp;lt;===\u0026amp;gt;\u0026amp;gt; D:\\Program Files\\Adobe\\Adobe Media Encoder 2020 创建的联接 说明已经创建成功.\n","date":"2025-11-14T10:12:30Z","image":"https://Selaphina.github.io/p/%E7%8C%B4%E5%AD%90%E4%B9%9F%E8%83%BD%E5%AD%A6%E4%BC%9A%E7%9A%84-ae%E5%85%A5%E9%97%A8%E7%BA%A7%E6%95%99%E7%A8%8B48th%E5%AE%A3%E4%BC%A0/cover_hu_969b77c6b924fd89.png","permalink":"https://Selaphina.github.io/p/%E7%8C%B4%E5%AD%90%E4%B9%9F%E8%83%BD%E5%AD%A6%E4%BC%9A%E7%9A%84-ae%E5%85%A5%E9%97%A8%E7%BA%A7%E6%95%99%E7%A8%8B48th%E5%AE%A3%E4%BC%A0/","title":"猴子也能学会的！ | Ae入门级教程（48th宣传）"},{"content":"图集（Atlas）：将多个小纹理（如UI图标、道具图片）打包到一张大图中。这样，渲染使用同一图集的不同精灵时，可以被合并到一个Draw Call中完成。\n注意渲染顺序：尽量让使用相同材质的节点在场景树中连续排列，避免不同材质的节点插入其中打断合批\n需求：是否可以项目自动化打包图集 工具推荐\nfree-tex-packer-cli（开源、易集成、支持常见 JSON 格式） 适合自动化打包，产出 atlas.png + atlas.json。 备选：TexturePacker（商业，功能强） 我可以写一个简单的自动化脚本 比如用 free-tex-packer-cli：\n输入：src/assets/ui/** 输出：src/assets/atlases/ui.png + src/assets/atlases/ui.json 加 npm script：npm run atlas:ui 我需要你确认这几点再动手（也需要你允许我写文件/装依赖，因为当前是只读）：\n资源目录和输出目录分别是？ 你需要的 JSON 格式（TexturePacker/Pixi/Hash）？ 是否要裁边、去透明边、强制 2 的幂尺寸、最大纹理尺寸？ 你会用在 Babylon 的 SpriteManager/SpritePackedManager 还是给网格手动 UV？ 1.SpriteManager/SpritePackedManager是什么 简短说法：它们都是 Babylon.js 里管理“二维精灵”的工具。\nSpriteManager：用一张 spritesheet 贴图 + 固定行列布局（等宽等高的格子）。你用 index 选格子。 SpritePackedManager：用“图集 + JSON”的打包结果（每个子图大小不一），根据 JSON 里的 UV/尺寸来取图。 如果你的图集是自动打包出来的（子图大小不一），就用 SpritePackedManager。如果是规则网格的 spritesheet，就用 SpriteManager。\n2. JSON 格式（TexturePacker/Pixi/Hash）之间有什么区别? 主要区别在 JSON 的结构和字段命名，内容大体相同（每张子图的 x/y/w/h、是否旋转、裁剪信息等）：\nTexturePacker JSON (Array) frames 是数组；每个元素里有 filename 和 frame（x,y,w,h），还包含 rotated/trimmed/spriteSourceSize/sourceSize。 TexturePacker JSON (Hash) frames 是对象（key=文件名），值里是 frame 等信息。数据同上，只是组织方式不同。 Pixi JSON 结构更偏 Pixi 生态，frames 通常是对象，meta 里会写 app: \u0026ldquo;Pixi\u0026rdquo;、scale 等。字段名和 TP 接近，但细节/命名可能略不同 （尤其 meta）。 简单选型：\n你要兼容 Babylon 的 SpritePackedManager，更稳的是 TexturePacker 的 JSON（Array 或 Hash）。 Pixi 格式更适合 Pixi.js 生态。 3.【是否要裁边、去透明边、强制 2 的幂尺寸、最大纹理尺寸】分别有什么用？ 它们都是为了减少纹理浪费、提高兼容性/性能：\n裁边（trim） 去掉每个子图四周的透明像素，只保留有内容的最小矩形。好处是图集更紧凑、尺寸更小。 注意：会改变子图原始尺寸，需要在 JSON 里保存偏移/原尺寸，渲染时要“还原”。 去透明边（extrude/bleed/alpha bleed） 在子图边缘向外扩展一圈像素（用边缘颜色填充），避免采样时因为双线性过滤/缩放出现“漏白/黑边”。 常用于图集防止“边缘渗色”。 强制 2 的幂尺寸（POT） 把整张图集尺寸调整为 2 的幂（比如 512/1024/2048）。 好处：兼容一些旧设备/管线，MipMap、压缩纹理更友好。 代价：可能变大、浪费空间。 最大纹理尺寸 限制图集最大边长（如 2048/4096）。 好处：避免超出 GPU 上限（移动端常见限制），也便于性能/内存控制。 代价：可能拆成多张图集。 移动端默认参数 1 2 3 4 5 6 7 - trim: true - extrude/bleed: 2 px - padding: 2 px - pot (power-of-two): true - maxSize: 2048 - allowRotation: false（避免旋转带来的采样/UV 心智负担） - scale: 1 桌面PC端 桌面/WebGL2，也可以把 maxSize 调到 4096，或把 pot 设为 false 来减少浪费。\n资源目录和输出目录分别怎么写比较好？ • 常见、清晰的结构是：\n资源输入（原始小图）：src/assets/ui/atlas-src/ 输出图集：src/assets/atlases/ui/（产出 ui.png + ui.json） 理由：原始素材和生成物分开，后续脚本清理、重建都更安全；图集集中在 atlases/ 便于引用和缓存。\n如果有多套图集（UI/FX/Icons），可以再分子目录：\nsrc/assets/ui/atlas-src/ src/assets/fx/atlas-src/ src/assets/atlases/ui/ src/assets/atlases/fx/ 你希望只做一套 UI 图集，还是要按类型拆多套？\n","date":"2025-11-14T10:12:30Z","image":"https://Selaphina.github.io/p/%E5%9B%BE%E9%9B%86%E6%89%93%E5%8C%85%E5%99%A8/CUC%E5%85%91%E6%8D%A2%E7%82%B92_hu_aee3a9923aa263e8.png","permalink":"https://Selaphina.github.io/p/%E5%9B%BE%E9%9B%86%E6%89%93%E5%8C%85%E5%99%A8/","title":"图集打包器"},{"content":"虚幻引擎 坐标系：\n单位：厘米\n从DCC软件导入UE的模型，导出设置的up axis要设置为Z向上。比如blender：\n引擎 坐标系手性 轴向约定（世界坐标） Unity 左手系 X 右，Y 上，Z 前 Unreal Engine 左手系 X 前，Y 右，Z 上 UE导出资产：\n1.选中资产，在细节窗口定位到内容浏览器。\n2.在内容浏览器右键——资产操作——导出-迁移\n嗯类似导出pakage。\n不放心引入的资源：\n右键——更新重定向器。\n可以修复/避免一些路径丢失的问题。\nNanite (纳米级模型) 概念： Nanite是虚幻引擎的虚拟化几何体系统，它采用全新的内部网格体格式和渲染技术来渲染像素级别的细节以及海量对象。\n它可以智能地仅处理你能够感受到的细节。 另外，Nanite采用高度压缩的数据格式，并且支持具有自动细节级别的细粒度流送。\n虚幻引擎中的Nanite虚拟几何体 | 虚幻引擎 5.7 文档 | Epic Developer Community\n处理简述： Nanite 不直接处理 Mesh / LOD，而是：\n把模型离线切分成大量 Cluster 每个 Cluster： ~128 个三角形 有包围盒（Bounds） 有误差度量（Error Metric） 1 2 3 4 Mesh └─ Cluster 0 └─ Cluster 1 └─ Cluster 2 本质上是 连续 LOD（Continuous LOD），不是离散 LOD。\n流程：\n1 2 3 4 1. CPU 提供相机 \u0026amp; 场景 2. GPU 进行可见性剔除（Cluster Culling） 3. GPU 选择合适精度的 Cluster 4. GPU 直接 Rasterize ⚠️ LOD 选择不在 CPU 上。\n使用场景： 适用于高面数模型。\n静态 / 准静态场景\n扫描模型\n建筑 / 地形\n高细节环境\n不适用于顶点有动态变化的模型。\n骨骼动画（复杂变形）\n顶点动画\n大量 World Position Offset\n透明物体\n高度动态拓扑\nNanite网格体和传统静态网格体的不同之处\nNanite网格体是一种启用了Nanite的特殊静态网格体。 Nanite网格体本质上仍是三角形网格体，但对其数据进行了大量细节和压缩处理。 此外，Nanite使用了一种全新系统，能以极高效的方式来渲染这种数据格式。\n要让静态网格体利用Nanite，只需一个标记来启用它即可。 编辑Nanite网格体的内容和传统网格体没太大不同，区别就在于相比使用传统方法渲染的几何体，Nanite能够渲染的三角形和实例要多出数个数量级。 将摄像机移到足够近的位置后，Nanite就会绘制出导入的原始源三角形。\nNanite网格体支持多重UV和顶点颜色。 材质可以被分配给网格体的不同分段，并且这些材质可以使用不同的着色模型和动态效果（在着色器中完成）。 材质指定可以动态切换，就像其他静态网格体一样。Nanite也无需任何烘焙材质的过程。\n虚拟纹理并非必须与Nanite一起使用，但强烈建议使用虚拟纹理。 虚拟纹理是正交虚幻引擎功能，它与纹理数据的关系类似于Nanite与网格体数据的关系。\nVT(虚拟纹理) 简述：\nVirtual Texture（VT）是一种“按需、分页、GPU 驱动”的纹理管理系统 它让你可以使用远大于显存容量的纹理数据，而不需要一次性加载整张贴图。\n一句更直白的：\nVT = 把纹理当成“内存页”来用\n虚幻引擎中的虚拟纹理 | 虚幻引擎 5.7 文档 | Epic Developer Community\n利用项目对 虚拟纹理 的支持，可在运行时以更低内存占用率和更高一致性创建和使用大尺寸纹理。\n虚幻引擎4(UE4)支持两种虚拟纹理方法：运行时虚拟纹理 (RVT) 和 流送虚拟纹理 (SVT)。\nNanite Virtual Texture 几何虚拟化 纹理虚拟化 GPU 驱动 GPU 驱动 按需加载 按需加载 Page / Cluster Page / Tile 解决规模问题 解决规模问题 VT 要解决什么问题？\n传统纹理系统的三个硬伤：\n1 2 3 4 5 6 1. **显存不够** - 8K / 16K / 多通道纹理 2. **重复浪费** - 相同区域不同 Mesh 重复加载 3. **带宽浪费** - 看不到的区域也在采样 VT的核心思想\nVT 的思想非常简单，但工程极其复杂：\n只把“当前屏幕真正用到的纹理小块”放进显存\n1️⃣ Page（页）—— VT 的基本单位\n一张大纹理会被拆成：\n1 Page（Tile） = 128×128 / 256×256 texels 例如：\n1 2 3 16K × 16K ↓ 4096 个 Pages 2️⃣ Virtual Address（虚拟地址）\n每个像素采样时：\n不直接访问物理纹理 而是访问一个 虚拟地址（Virtual UV） 3️⃣ Page Table（页表）\nVT 维护一张：\nVirtual Address → Physical Texture Page\n类似 CPU 的 MMU。\n工作流程 1 2 3 4 5 1. Pixel Shader 采样 VT 2. 发现某个 Page 不在显存 3. 记录 Page ID（Feedback） 4. CPU / GPU 请求加载该 Page 5. 下一帧 / 几帧后可用 VT和Texture Streaming区别\nTexture Streaming Virtual Texture 粒度 整张 Mip Page 控制方 CPU GPU 精度 粗 极细 显存利用 一般 极高 实时性 低 高 不适合 VT 的情况 UI 小尺寸贴图 高频动画采样 关于sRGB 一句话总结：\n给人看的颜色 → 勾 sRGB 给机器算的数值 → 不勾 sRGB\n核心事实 显示器 ≠ 线性 人眼 ≠ 线性 sRGB = 一种带 Gamma 的颜色编码 直观理解 数值 真实亮度 0.5（sRGB） 不是 50% 亮度 0.5（Linear） 是 50% 亮度 👉 渲染计算必须在线性空间， 👉 贴图存储通常在 sRGB 空间。\n首先，推荐的图片格式是TGA\u0026gt;PNG，压缩损失率小。JPG损失的数据多，如果需要无损导入不建议JPG。\n必须勾 sRGB（“颜色类”） 贴图类型 原因 Albedo / Base Color 给人看的颜色 Diffuse 同上 Emissive 发光颜色 UI / Sprite 直接显示 绝对不要勾 sRGB（“数据类”） 贴图类型 原因 Normal Map 法线向量 Roughness 数值曲线 Metallic 0–1 参数 AO 遮蔽强度 Height / Displacement 几何数据 Mask（RGBA） 任意通道数据 Opacity 线性插值 ❌ Normal Map 勾了 sRGB 结果： • 法线偏软 • 高光发灰 • 细节丢失 ❌ Roughness 勾了 sRGB 结果： • 表面“油腻 / 塑料感” • 高光范围不对 ❌ Mask 贴图勾 sRGB 结果： • 阈值不准 • 材质行为怪异\n快捷键Enter能打开纹理的编辑器。也可以对纹理贴图右键——编辑。\n注意：lightmap注意勾选灰阶（grayscale G8/16）\n其余，1）漫反射贴图（diffuse）； 2）Ramp图需要勾选sRGB。\n其他的法线/Mask/Metal等等都不必勾选sRGB。\n准备上下文：\n注意，这个transformvector节点，只需要搜索Transform再回车（或者下图）就可以找到……\n在金属cap的时候，相机空间是可以等同于观察空间的\n注意：虚幻的V和unity的V是相反的。(即xyz中的y是相反的。rgb的g是相反的。)\n原神法线贴图导入虚幻记得【翻转绿通道】\n附录 MatCap MatCap（Material Capture） 是一种基于法线方向的贴图着色技术。 其核心思想是：\n不做真实光照计算，而是用“表面法线在视空间中的方向”作为索引，从一张 2D 纹理 中直接查表得到最终颜色。\n你可以把 MatCap 理解为：\n把一个“已经算好光照效果的球体”拍成一张图 在渲染时，每个像素根据自身法线，去这张图上“取一个颜色” 因此 MatCap 不依赖灯光、不需要 BRDF、不需要光源遍历，成本极低。\n1. 核心输入：视空间法线（View Space Normal） MatCap 使用的不是：\n世界空间法线 切线空间法线 而是：\n视空间（View Space / Camera Space）中的法线\n原因很简单： MatCap 纹理本身就是以“相机视角”为基准烘焙的。\n2. 从法线到 UV 的映射 假设你已经有一个 单位化的视空间法线：\n1 N_view = (nx, ny, nz) 典型 MatCap UV 计算方式：\n1 UV = N_view.xy * 0.5 + 0.5 含义：\nN.xy ∈ [-1, 1] 映射到纹理坐标 [0, 1] nz 通常隐含在 MatCap 贴图的亮暗变化中，不直接参与采样。\n完整链路\n1 2 3 4 5 6 7 8 9 Normal Map（Tangent） ↓ TBN Normal（World） ↓ ViewMatrix Normal（View） ↓ UV = normal.xy * 0.5 + 0.5 ↓ Sample MatCap Texture MatCap 是“假光照”，但“看起来很像真光照”。\n","date":"0001-01-01T00:00:00Z","image":"https://Selaphina.github.io/p/%E8%99%9A%E5%B9%BB%E5%BC%95%E6%93%8Eue5.7.1/CUC%E5%85%91%E6%8D%A2%E7%82%B92_hu_aee3a9923aa263e8.png","permalink":"https://Selaphina.github.io/p/%E8%99%9A%E5%B9%BB%E5%BC%95%E6%93%8Eue5.7.1/","title":"虚幻引擎UE5.7.1"},{"content":"实现原理（PlanarShadowSystem） 基本思路：将 caster 的顶点沿光照方向投影到一个平面（plane normal + plane height）上，生成“扁平化”的投影几何来模拟阴 影。 投影发生在顶点着色器 src/game/systems/planar-shadow/shaders.ts： 根据 u_lightDir 和 u_planeNormal 求射线与平面的交点。 u_planeHeight 设定平面高度（目前来自 planeHeight + planeHeightOffset）。 使用 u_planeBias 做轻微的 z-bias，防止 z-fighting。 片元着色器输出固定色 u_shadowColor，不采样纹理，不做软边。 Stencil（可选）：将 receiver 写入模板缓冲，shadow 只绘制在 receiver 上，避免重叠变黑或穿帮。 工作流程 配置入口 src/config/data/shadows.json 里的 planarShadow 控制是否启用、平面法线/高度/偏移/颜色等。 初始化入口 src/game/App.ts 在 preload() 中调用 initPlanarShadowSystem()。 构建系统配置 initPlanarShadowSystem() 读取 planarShadow，拼成 PSPlanarShadowConfig： plane.height = planeHeight + planeHeightOffset（你要调的是这里） plane.normal、plane.bias、appearance.color、stencil excludePatterns 合并默认和 shadowMeshes.json 的排除项 注册 shader 需要时启用 stencil 创建 base shader material 在 onBeforeRender 每帧更新 uniform（lightDir/planeNormal/planeHeight/bias/color） 添加 caster/receiver Player 模型在 initPlanarShadowSystem() 中手动遍历 child mesh 加 caster 自动检测：enableAutoDetection() 按 name pattern 自动加入 Receiver：applyPlanarShadowReceivers() 从 shadowMeshes.json 匹配网格名 渲染阶段 影子网格是原网格 clone 的几何体（或骨骼 mesh 的特制材质） 渲染组/深度/模板设置确保阴影在 receiver 上、被主体遮挡 ———\n开放控制点 planeHeight / planeHeightOffset：阴影平面高度（位置不对主要调这个） planeBias：投影深度偏移（防止 z-fighting） planeNormal：平面朝向 shadowColor.a：阴影强度 useStencil：是否启用 stencil 遮罩 附录 漂浮在所有物体上的原因 1 2 3 4 5 平面阴影的渲染层级（RenderingGroupId）改回了默认值（0）。 之前的设置（1）可能导致阴影强制绘制在所有物体（层级0）之上，造成“漂浮在桌子上”的错觉（实际上是因为深度遮挡失效，阴影盖过了桌子）。 现在阴影应当能正确被桌子、收银台等物体遮挡（在它们下方）。请刷新查看效果。 stencil 遮罩有什么作用？ stencil 遮罩的作用是“限制阴影只画在指定接收面上，并避免重叠阴影叠加变黑”。具体在这套实现里有两点：\n只在接收面上显示：receiver 先把模板缓冲写成 1，shadow pass 只在 stencil == 1 的像素上绘制，防止阴影投到不该有阴影的 地方（比如角色身上或空中）。 避免叠加过黑：通过模板逻辑控制“同一像素只算一次阴影”，让多个 caster 重叠时不无限叠加变黑（更接近 Cocos 的 planar shadow 表现）。 如果不启用 stencil，阴影会渲染到所有通过深度测试的位置，看起来更容易“漂浮/穿帮”。\n投下的阴影属于网格体？ 每个 caster 会 clone 一份 shadow mesh（_planarShadow 后缀），挂在一个 shadow root 下，使用专用的 planar shadow shader；真正绘制的是这份投影网格，而不是原始网格本身。\nshadow mesh会和原始的网格重叠/导致游戏运行实时面数增加吗 顶点/三角形数增加：每个 caster 多一份几何，等于多画一次（含骨骼的话更贵）。\ndraw call 增加：影子网格是独立材质/渲染组，会多一次绘制。\nCPU 也有成本：每帧更新 uniform、跟随变换、管理 caster/receiver。\n减少 caster 数量、排除小物体（excludePatterns/minVolume）。\n只给关键角色投影。\n关闭 stencil（略省但可能穿帮）。\n对比传统 shadow mapping（深度贴图） 优势 更省性能：不需要渲染光源视角的 shadow map，也无需采样深度贴图。 稳定、无闪烁：没有阴影贴图分辨率/抖动问题，移动时也更“干净”。 控制简单：直接用高度/偏移/颜色调效果，适合移动端或风格化项目。 区别 / 局限 只适用于平面或近似平面：阴影被投到一个固定平面，无法贴合起伏地形或复杂表面。 不能产生真实遮挡关系：不会被其他物体遮挡，也没有自阴影。 阴影形状简单：本质是投影几何体的“扁平投影”，没有软硬边真实变化。 精度与物理真实性较弱：只适合“地面贴影子”的视觉提示用途。 一句话：planar shadow 是“便宜稳定的视觉提示”，shadow mapping 是“真实但更重、更复杂”的通用方案。\n","date":"2025-11-14T10:12:30Z","image":"https://Selaphina.github.io/p/planar%E9%98%B4%E5%BD%B1%E7%B3%BB%E7%BB%9F%E7%9A%84%E5%AE%9E%E7%8E%B0/CUC%E5%85%91%E6%8D%A2%E7%82%B92_hu_aee3a9923aa263e8.png","permalink":"https://Selaphina.github.io/p/planar%E9%98%B4%E5%BD%B1%E7%B3%BB%E7%BB%9F%E7%9A%84%E5%AE%9E%E7%8E%B0/","title":"Planar阴影系统的实现"},{"content":"c\n春节假期间，更替了一下笔记软件。主要用于教程指路： https://zhuanlan.zhihu.com/p/586431408\n电脑端： 整体需要：\n安装坚果云\nObsidian安装Remotely Save插件\n1.安装 Remotely Save 在社区插件市场搜索“Remotely Save”，安装并启用该插件。\n2.坚果云 打开坚果云，登录/注册账号。 创建一个个人同步文件夹，注意文件夹名称要和被同步的 Obsidian 库的库名一致。\n比如，我的Obsidian 库名为：MyCalendar，那么新建的同步文件夹也要命名为：MyCalendar。\n设置—— 第三方应用管理——添加应用密码\n复制生成的密码。（也可以在刚刚建立的应用处点击“显示密码”来查看密码）\n3 设置 Remotely Save 进入 Remotely Save 的设置界面，选择远程服务为 Webdav。\nWebdav 设置中的“服务器地址”、“用户名”、“密码”分别对应输入刚刚“用户信息-安全选项-第三方应用管理”界面中的“服务器地址”、“账号”、“应用密码”（刚刚复制的密码）。\n设置好后点击“检查”，测试服务器连接状况。\n参考设置：\nipad端 Obsidian 库就是一个文件夹，直接把它打包（必须zip）发送到移动端，然后解压——到移动端的obsidian打开即可。\n打开移动端 Obsidian，选择“Open folder as vault”，选择刚刚解压的文件夹作为 Obsidian 库，然后 Obsidian 会自动进入主界面。\n点击“同步”，测试服务器连接成功！\n","date":"2026-02-19T10:20:12Z","image":"https://Selaphina.github.io/p/obsidian%E5%A4%9A%E7%AB%AF%E5%90%8C%E6%AD%A5/cover1_hu_5e2014920a4d8f6.png","permalink":"https://Selaphina.github.io/p/obsidian%E5%A4%9A%E7%AB%AF%E5%90%8C%E6%AD%A5/","title":"Obsidian多端同步"},{"content":"Codex 适用于各种编码环境的单个智能体，已在 ChatGPT Plus、Pro、Business、Edu 和 Enterprise 套餐中提供。\nCodex | OpenAI\n1.切换一个适合的节点 2.Codex官网 Codex | OpenAI\n3.命令行下载 1 2 3 win+R cmd 使用npm包管理器进行下载：\n1 npm i -g @openai/codex 检查是否下载成功\n1 codex -- -v 成功则出现：\n4.中转站 https://uyilink.com/\n这个codex价格很便宜，Claude Code价格也算便宜，但是据说用的官方的API 用Codex的话先用这个，下面这个接入Codex时候发送请求体有点问题在修复\n5.下载cc-switch https://github.com/farion1231/cc-switch/releases/tag/v3.8.2\n根据你的电脑win/mac，拉到最后选择对应的安装包下载：CC-Switch（用来快速配置/快速切换中转站）.\n下一步\n","date":"2025-12-16T10:32:30Z","image":"https://Selaphina.github.io/p/codex%E6%89%8B%E8%AE%B0/CUC%E5%85%91%E6%8D%A2%E7%82%B92_hu_aee3a9923aa263e8.png","permalink":"https://Selaphina.github.io/p/codex%E6%89%8B%E8%AE%B0/","title":"Codex手记"},{"content":"复现一些有趣的场景shader效果。\n1.插片树 Blender 模型 1.Blender新建十字插片（2个平面）\n2.新建立方体，细分开到2。修改器应用细分后，tab进入编辑模式，选中面进行形状微调，做一个扁椭球状。最后平滑着色。\n3.新建粒子系统——毛发——勾选高级——\n重点：发射源要改为体积。\n否则最终的树叶团子会中空……\n其中，渲染勾选【物体】，选择十字插片。\n微调上述提及的几个参数即可。\n实例化这些面片，ctrl+j合并，作为一个十字插片的平面网格。\n调整立方体球，包裹住面片插片网格。\n给平面（十字插片）加上【数据传递-面拐数据】，选【自定义法向】，源物体选择的是立方体。这样面片的法线就是这个球体的信息了。\n记得把所有修改器都应用一下…^^\n将Blender做好的模型导出FBX，导入Unity开工。\n导出设置参考：\nUnity 导入一下ASE包，这是第一次正式使用ASE写shader，这里截图记录一下ASE的文件结构。 大佬整理出的贴图各个通道的作用。\n导入unity连上节点后：\n2.冰雪效果 参考：https://www.patreon.com/posts/snow-and-icicles-118211037\n这里仅作简单的重点记录。\n分成三个部分\n1.上层雪（Snow） 2.下层冰（Ice） 需要纹理贴图（2）： 1.主纹理（Main Texture）\n2.噪声纹理（Noise Texture）\nTriplanar 什么是Triplanar mapping？\nTriplanar Mapping（三平面映射）是一种不依赖UV展开的纹理映射方法，其核心思想是：\n沿 X / Y / Z 三个正交方向分别投影纹理，并根据法线方向对三次采样结果进行加权混合。\n数学原理 投影面 使用的坐标 X 平面（YZ 面） (pos.y, pos.z) Y 平面（XZ 面） (pos.x, pos.z) Z 平面（XY 面） (pos.x, pos.y) ↓\n权重计算（关键）\n使用法线的绝对值作为权重来源：\n1 2 float3 w = abs(normalWS); w = w / (w.x + w.y + w.z); 意义：\n面朝 X → w.x 大 面朝 Y → w.y 大 面朝 Z → w.z 大 ↓\n三次采样 + 加权混合\n1 2 3 4 color = texX * w.x + texY * w.y + texZ * w.z; 使用场景 Triplanar 是连续混合：\n消除 UV 接缝 平滑过渡 更适合自然材质（岩石、地形） 岩石、雪、苔藓、风化痕迹几乎都用 Triplanar Noise。 方法 优点 缺点 UV Mapping 快 接缝 Triplanar 无接缝 采样多 World UV 简单 轴向拉伸 Volume Texture 最干净 显存大 纹理拉伸的本质是：\n纹理坐标在表面某一方向上的变化速率，远小于几何在该方向上的变化速率。\n更通俗一点是：\n纹理采样方向 ≠ 表面变化方向\nTriplanar 避免拉伸的“核心原因”\nTriplanar 永远用“当前表面最接近正交的投影平面”进行采样。\n雪顶：用Triplanar Noise来定义冰\u0026amp;雪的噪声 用 Triplanar 方式采样 Noise 纹理 ，生成**“空间连续、不依赖 UV 的随机控制信号”**。\nWe start by creating some Triplanar noise,which we will use to make the edges between the borders of Ice and Snow look more natural. 我们先制作一些三平面噪点，用来让冰雪边界之间的边缘看起来更自然。\n噪声\u0026amp;法线插值：在【世界法线.y】mul【Triplanar Noise.y】与【世界法线.y】之间插值。\n雪顶部分：利用噪声法线 Lerp 结果作为smoothstep输入，设定雪顶的平滑度。\n当两个效果（雪顶\u0026lt;\u0026gt;主纹理）叠加时，在两者之间做lerp实现叠加与过渡。\n冰底：与雪顶同理 与雪顶的y方向位置正好相反，所以要对smoothstep后的结果取反（oneMinus）\n同样，对雪顶+主纹理的结果之上叠加冰底效果，仍然需要插值： 所以\n菲涅尔 Basic Fresnel node, with power control and multiplied by SnowRim color 基础的菲涅尔节点，带功率控制，并乘以 SnowRim 颜色\n连接到自发光输出点，描述一下冰冻的边缘光效果。由于上下冰\u0026amp;雪两层的边缘光不需要重叠处理，直接用Add加一起就行。\n积雪\u0026amp;冰柱-顶点偏移 The vertex displacement has two parts, the snow where vertices will be pushed up slightly, and the ice where the vertices will be pulled down 顶点位移分为两部分:\n1.雪顶点会被抬起\n2.冰层顶点会被下拉\nWorldspace 因为位移仅在顶部和底部，我们只需在 Worldspace 的.xz（即Y平面-俯视）坐标上做一个纹理采样，再乘以缩放。\n代码：\n1 float2 uv = positionWS.xz; shader graph\n取世界空间的xz作为UV（乘以缩放尺度），采样一下噪声图\n再对【世界法线】和【世界法线 mul 世界空间噪声】进行lerp，得到的【世界法线噪声】只取Y分量去step【冰雪的偏移】。\n接下来就是雪高度\u0026amp;冰柱长度\nWe once again take the world normals y value, this time negated to get the bottom of the model, and Smoothstep through this to control the sharpness of the icicles pulling down. 我们再次取世界法线的 y 值，取负值以获得模型底部，然后用 Smoothstep 控制冰柱向下拉动的锐利度。\n最后将顶点位移和顶点位移相加起来,只取Y 坐标。\n因为我们一直在世界空间中工作，现在需要【输出的Y通道】加上【世界位置】，并用Transform节点将其空间变换实现为【World—Object】，最后才能连到主节点的Vertex offset。\n","date":"2025-12-15T10:12:30Z","image":"https://Selaphina.github.io/p/unity%E6%8F%92%E7%89%87%E6%A0%91/CUC%E5%85%91%E6%8D%A2%E7%82%B92_hu_aee3a9923aa263e8.png","permalink":"https://Selaphina.github.io/p/unity%E6%8F%92%E7%89%87%E6%A0%91/","title":"Unity插片树"},{"content":"1. Blender导出带贴图模型 Unity MCP（Model Context Protocol）正在重构开发者的工作流。它像一座桥梁，让 AI 助手能 \u0026ldquo;读懂\u0026quot;Unity 项目的上下文 —— 从场景结构到脚本逻辑，从资源属性到运行时数据，从而实现智能代码生成、场景优化建议、角色行为设计等高效协作。\n想象一下：当你在 Unity 中选中一个卡顿的场景，AI 能通过 MCP 直接分析 Draw Call 数据并给出光照烘焙优化方案；当你设计 NPC 行为时，AI 能基于当前动画控制器结构生成状态机过渡代码。这些并非科幻，而是通过 Unity MCP 配置就能实现的开发体验。\n","date":"2026-02-04T10:20:12Z","image":"https://Selaphina.github.io/p/unity-mcp-%E5%85%A8%E6%B5%81%E7%A8%8B/cover1_hu_5e2014920a4d8f6.png","permalink":"https://Selaphina.github.io/p/unity-mcp-%E5%85%A8%E6%B5%81%E7%A8%8B/","title":"Unity MCP 全流程"},{"content":"1. Blender导出带贴图模型 资源预处理插件，目标是对 AnimationClip 做 关键帧冗余消除（Keyframe Reduction）。在资源大小压缩的角度来看，非常有必要写一个自动化插件来减轻对动画资源的压缩的工作：\n1 2 3 4 5 6 7 8 9 扫描 AnimationClip ↓ 分析所有动画曲线 ↓ 检测未变化通道 ↓ 删除无意义关键帧或整条曲线 ↓ 生成优化后的Clip 删帧算法思路 **核心思路：**删除整条无变化曲线（认为这是冗余关键帧）\n1 2 3 4 5 6 7 8 9 给 key[i-1], key[i], key[i+1] ↓ 三点线性误差检测 ↓ 如果： 通过 i-1 和 i+1 插值得到的值 ≈ key[i] ↓ 则删除 key[i] 关键风险点 1. Quaternion 曲线 旋转通常由：\n1 x y z w 四条曲线 必须成组处理，否则会破坏旋转。\n2. Humanoid动画 部分曲线是：\n1 Muscle曲线 不能随便删。\n3. Root Motion / 挂点骨骼（重要） 不能删除 root transform 曲线 / 挂点骨骼曲线 。\n特别高危的几类“被误删曲线” 🚨 1. 武器 / 道具挂点骨骼\nWeaponSocket Hand_R_Attach PropPoint Dummy / Helper Bone 👉 即使完全静止，也必须保留 TRS\n🚨 2. Root / Pelvis / Hips\nRoot Hips Pelvis 👉 这些是 全身空间基准\n🚨 3. Humanoid 的 Muscle 曲线（如果是 Humanoid）\nmuscle.* RootT.x / RootQ.y 👉 删了会直接破坏重定向\n误删这些帧之后，会出现：第一个正常的角色倒地或者大幅度运动后销毁——在这之后实例化的所有角色动画全部出错。这是因为Unity 在运行时对 Animator / Avatar / AnimationClip 做了共享缓存，删帧共享状态被“污染（State Corruption）”，\n4. Tangent信息 删除关键帧后要重新设置：\n1 curve.SmoothTangents() 否则动画会抖。\n总结： 优化效果大概是：\n每个带动画模型都能减少100k左右的话，所有模型加起来减少的大小就很可观，所谓不积跬步无以至千里……\n","date":"2026-02-04T10:20:12Z","image":"https://Selaphina.github.io/p/unity%E5%8A%A8%E7%94%BB%E5%88%A0%E5%B8%A7%E4%BC%98%E5%8C%96%E5%B7%A5%E5%85%B7/cover1_hu_5e2014920a4d8f6.png","permalink":"https://Selaphina.github.io/p/unity%E5%8A%A8%E7%94%BB%E5%88%A0%E5%B8%A7%E4%BC%98%E5%8C%96%E5%B7%A5%E5%85%B7/","title":"Unity动画删帧优化工具"},{"content":"进度报告 上周至本周的进度：\n1.STT集成：实现语音对话的功能。\n2.天空盒：利用nano banana工具，按照天空盒贴图标准，形成水墨风格环境映射。\n3.插片摇曳植物：去掉了性能开销较大的公共资产植物，通过十字插片实现植物的叶片效果，以及输入正弦波来驱动顶点偏移，实现随风摇曳的效果。\n一、 上周至本周工作进度 上周至本周工作主要是：\n1. STT（Speech-to-Text）语音交互模块集成 工作内容： 完成了语音输入对话功能。通过调用讯飞星火 STT 接口，实现了对用户麦克风输入音频的实时捕捉，并将其转化为文本字符串显示在 UI 界面上，初步实现了“语音输入-系统识别-反馈显示”的交互。 提升沉浸感： 用户无需脱离画面进行繁琐的打字操作，交互过程更加流畅。 技术铺垫： 为下一阶段结合大语言模型（LLM）的“情景化对话”功能提供了必要的输入接口。 2. 水墨风格天空盒（Skybox）与环境映射 工作内容： 先将普通的HDR环境贴图转成Nano Banana能读懂的图片格式，利用 Nano Banana AI工具生成符合等距柱状投影（Equirectangular Projection）标准的 360° 全景贴图，并对其进行水墨风格化处理。随后将处理后的贴图应用为场景 Skybox，并重新烘焙了场景的环境反射（Reflection Probe）和环境光（Ambient Light）。 统一视觉基调： 确立了场景整体的冷暖色温和水墨氛围，使天空与地面的衔接浑然一体。 优化光影融合： 通过环境映射，使场景中的模型（如建筑、石块）能反射出带有水墨质感的光泽，避免了“模型像是漂浮在背景上”的违和感。 3. 植被渲染优化：十字插片与动态摇曳 ![](GIF 2026-1-15 21-06-44.gif)\n工作内容：\n资产重构： 移除了资源商店中面数过万的高精度植物模型，改用由两个交叉平面组成的“十字插片（Cross-plane）”模型来模拟植物外观。 Shader 开发： 编写自定义 Shader，在顶点着色器（Vertex Shader）阶段引入时间变量和正弦波函数（Sine Wave），根据顶点的高度信息（UV 或 Y 轴坐标）控制偏移量，模拟风吹摆动的效果。 优化：\n性能瓶颈： 校园场景需要大量植被覆盖，之前的高模资产导致 Draw Call 激增，帧率严重下降，无法满足实时渲染要求。\n增强生动性： 随风摇曳的动态效果极大地增加了场景的“呼吸感”和韵味，利用数学模拟实现了低成本的视觉欺骗，更加符合水墨画中“风动草偃”的意境。\n二、下一阶段目标： 1.加入水墨风格水体\n2.加入亭台水榭等二次装饰元素，对虚拟环境进行进一步抽象和美化\n3.情景化对话脚本\n","date":"2026-01-15T10:12:30Z","image":"https://Selaphina.github.io/p/%E6%AF%95%E4%B8%9A%E8%AE%BE%E8%AE%A1-%E6%B0%B4%E5%A2%A8%E9%A3%8E%E6%A0%BC%E5%8C%96%E5%9C%BA%E6%99%AF%E6%B8%B2%E6%9F%93%E5%85%B6%E4%BA%94/cover1_hu_89d58f86b094ccaa.png","permalink":"https://Selaphina.github.io/p/%E6%AF%95%E4%B8%9A%E8%AE%BE%E8%AE%A1-%E6%B0%B4%E5%A2%A8%E9%A3%8E%E6%A0%BC%E5%8C%96%E5%9C%BA%E6%99%AF%E6%B8%B2%E6%9F%93%E5%85%B6%E4%BA%94/","title":"毕业设计 | 水墨风格化场景渲染（其五）"},{"content":" 进度报告 本次主要记录AI对话功能及用户界面的接入。\n1 Deepseek API 首次调用 API | DeepSeek API Docs\n根据Deepseek API 文档，将调用对话API写一个C#脚本。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # Please install OpenAI SDK first: `pip3 install openai` import os from openai import OpenAI client = OpenAI( api_key=os.environ.get(\u0026#39;DEEPSEEK_API_KEY\u0026#39;), base_url=\u0026#34;https://api.deepseek.com\u0026#34;) response = client.chat.completions.create( model=\u0026#34;deepseek-chat\u0026#34;, messages=[ {\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;You are a helpful assistant\u0026#34;}, {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Hello\u0026#34;}, ], stream=False ) print(response.choices[0].message.content) 将官方提供的python脚本给ai，让其改成C#版本的就可以。\nDeepSeekDialogueManager 由于后面要考虑对话的用户界面和动画状态机等等，这里直接贴出最终的LLM调用脚本。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 using UnityEngine; using UnityEngine.UI; using TMPro; using System.Collections; using System.Collections.Generic; using UnityEngine.Networking; using System.Text; using System.IO; using System; public class DeepSeekDialogueManager : MonoBehaviour { public string botMessage; // API 配置 [Header(\u0026#34;API Settings\u0026#34;)] [SerializeField] private string apiKey = \u0026#34;\u0026#34;; [SerializeField] private string apiUrl = \u0026#34;https://api.deepseek.com/v1/chat/completions\u0026#34;; //private const string modelName = \u0026#34;deepseek-reasoner\u0026#34;; private const string modelName = \u0026#34;deepseek-chat\u0026#34;; // UI 绑定 [Header(\u0026#34;UI References\u0026#34;)] [SerializeField] public TMP_InputField userInputField; [SerializeField] private TextMeshProUGUI chatOutputText; [SerializeField] private Button sendButton; [SerializeField] private AudioSource audioSource; [Header(\u0026#34;思考指示器\u0026#34;)] [SerializeField] public TextMeshProUGUI thinkingIndicator; [Header(\u0026#34;对话背景\u0026#34;)] [SerializeField] public TMP_InputField BGInputField; [SerializeField] private Button Button1; [Header(\u0026#34;角色名字\u0026#34;)] [SerializeField] public TMP_InputField RoleName; [SerializeField] private Button Button2; [Header(\u0026#34;人格设定\u0026#34;)] [SerializeField] public TMP_InputField PersonalityInputField; [SerializeField] private Button Button3; [Header(\u0026#34;动画\u0026#34;)] [SerializeField] private Animator m_Animator; // 动画控制器 // 对话参数 [Header(\u0026#34;对话设置\u0026#34;)] [Range(0, 2)] public float temperature = 0.7f; [Range(1, 1000)] public int maxTokens = 200; // 角色设定 //[SerializeField] private string RoleName; [System.Serializable] public class NPCCharacter { [TextArea(3, 10)] public string personalityPrompt = $\u0026#34;你是中国传媒大学的介绍精灵，你掌握一切关于中国传媒大学的历史和现在的知识。联网搜索。\u0026#34;; //public string personalityPrompt = ; } [SerializeField] public NPCCharacter npcCharacter; // 修复1：使用可序列化的数据结构 private List\u0026lt;MessageData\u0026gt; messages = new List\u0026lt;MessageData\u0026gt;(); private string tempAudioPath; private string sizePrompt = \u0026#34;只需要回答话语，不要有任何动作(比如*推眼镜*等等)。只需要回答150词以内即可。\u0026#34;; //void Start() void Start() { tempAudioPath = Path.Combine(Application.persistentDataPath, \u0026#34;tts_output.wav\u0026#34;); // 初始化角色设定 messages.Add(new MessageData { role = \u0026#34;system\u0026#34;, //content = npcCharacter.personalityPrompt + sizePrompt content = sizePrompt+BGInputField.text +\u0026#34;你的名字为：\u0026#34;+ RoleName.text+\u0026#34;。\u0026#34; + PersonalityInputField.text }) ; // 绑定发送事件 sendButton.onClick.AddListener(OnSendMessage); userInputField.onEndEdit.AddListener((text) =\u0026gt; { if (Input.GetKeyDown(KeyCode.Return)) OnSendMessage(); }); // 为设定按钮添加监听 // 添加UI变更监听 BGInputField.onValueChanged.AddListener(_ =\u0026gt; InitializeSystemMessage()); RoleName.onValueChanged.AddListener(_ =\u0026gt; InitializeSystemMessage()); PersonalityInputField.onValueChanged.AddListener(_ =\u0026gt; InitializeSystemMessage()); // 按钮绑定更新系统消息 Button1.onClick.AddListener(() =\u0026gt; InitializeSystemMessage()); Button2.onClick.AddListener(() =\u0026gt; InitializeSystemMessage()); Button3.onClick.AddListener(() =\u0026gt; InitializeSystemMessage()); } private void InitializeSystemMessage() { // 动态获取UI输入值 string dynamicPrompt = $\u0026#34;{BGInputField.text}{RoleName.text}{PersonalityInputField.text}{sizePrompt}\u0026#34;; // 清除旧系统消息 messages.RemoveAll(m =\u0026gt; m.role == \u0026#34;system\u0026#34;); // 添加新系统消息 messages.Add(new MessageData { role = \u0026#34;system\u0026#34;, content = string.IsNullOrWhiteSpace(dynamicPrompt) ? npcCharacter.personalityPrompt + sizePrompt // 默认值 : dynamicPrompt }); } // 发送消息 public void OnSendMessage() { InitializeSystemMessage(); // 每次发送前更新系统消息 string userMessage = userInputField.text.Trim(); if (string.IsNullOrEmpty(userMessage)) return; // 显示玩家消息 AppendToChat($\u0026#34;玩家: {userMessage}\u0026#34;,false); // 添加到历史记录 messages.Add(new MessageData { role = \u0026#34;user\u0026#34;, content = userMessage }); userInputField.text = \u0026#34;\u0026#34;; //调用api StartCoroutine(CallDeepSeekAPI()); } public void OnSendMessage(string audioMess) { string userMessage = audioMess; if (string.IsNullOrEmpty(userMessage)) return; // 显示玩家消息 AppendToChat($\u0026#34;玩家: {userMessage}\u0026#34;,false); // 添加到历史记录 messages.Add(new MessageData { role = \u0026#34;user\u0026#34;, content = userMessage }); userInputField.text = \u0026#34;\u0026#34;; //调用api StartCoroutine(CallDeepSeekAPI()); } //角色思考文本动画 private IEnumerator BlinkText() { while (true) { float alpha = Mathf.PingPong(Time.time, 1f); thinkingIndicator.color = new Color(1, 1, 1, alpha); yield return null; } } // API通信协程 private IEnumerator CallDeepSeekAPI() { //启动闪烁动画 thinkingIndicator.gameObject.SetActive(true); thinkingIndicator.text = RoleName.text+\u0026#34;思考中……\u0026#34;; SetAnimator(\u0026#34;state\u0026#34;, 1); // 设置动画状态为1 StartCoroutine(BlinkText()); // 构建符合API规范的结构体 var requestBody = new RequestBody { model = modelName, messages = messages.ToArray(), // 转换为数组 temperature = temperature, max_tokens = maxTokens, stream = false }; string jsonData = JsonUtility.ToJson(requestBody); byte[] bodyRaw = System.Text.Encoding.UTF8.GetBytes(jsonData); using (UnityWebRequest request = new UnityWebRequest(apiUrl, \u0026#34;POST\u0026#34;)) { request.uploadHandler = new UploadHandlerRaw(bodyRaw); request.downloadHandler = new DownloadHandlerBuffer(); request.SetRequestHeader(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;); request.SetRequestHeader(\u0026#34;Authorization\u0026#34;, $\u0026#34;Bearer {apiKey}\u0026#34;); request.SetRequestHeader(\u0026#34;Accept-Language\u0026#34;, \u0026#34;en-US\u0026#34;); yield return request.SendWebRequest(); // 3. 隐藏指示器 thinkingIndicator.gameObject.SetActive(false); StopCoroutine(BlinkText()); // 可选：停止动画 // 修复3：添加详细错误处理 if (request.result != UnityWebRequest.Result.Success) { Debug.LogError($\u0026#34;API错误: {request.error}\u0026#34;); Debug.LogError($\u0026#34;状态码: {request.responseCode}\u0026#34;); Debug.LogError($\u0026#34;响应内容: {request.downloadHandler.text}\u0026#34;); // 尝试解析错误详情 try { var errorResponse = JsonUtility.FromJson\u0026lt;ErrorResponse\u0026gt;( request.downloadHandler.text ); Debug.LogError($\u0026#34;错误类型: {errorResponse.error.type}\u0026#34;); Debug.LogError($\u0026#34;错误信息: {errorResponse.error.message}\u0026#34;); } catch { Debug.LogError(\u0026#34;无法解析错误响应\u0026#34;); } AppendToChat(\u0026#34;系统: 对话服务暂时不可用\u0026#34;,true); yield break; } // 解析响应 var response = JsonUtility.FromJson\u0026lt;DeepSeekResponse\u0026gt;( request.downloadHandler.text ); //string botMessage = response.choices[0].message.content; botMessage = response.choices[0].message.content; // 显示并存储AI回复 AppendToChat($\u0026#34;{RoleName.text}: {botMessage}\u0026#34;,true); messages.Add(new MessageData { role = \u0026#34;assistant\u0026#34;, content = botMessage }); SetAnimator(\u0026#34;state\u0026#34;, 2); // 设置动画状态为2 // 调用TTS生成语音 //StartCoroutine(CallTTSService(botMessage)); } } // 更新对话显示 private void AppendToChat(string text,bool isBot = false) { // 设置颜色 string colorTag = isBot ? \u0026#34;\u0026lt;color=#47A699\u0026gt;\u0026#34; : \u0026#34;\u0026lt;color=#FFFFFF\u0026gt;\u0026#34;; // 深蓝色为 #0000FF，白色为 #FFFFFF string closeTag = \u0026#34;\u0026lt;/color\u0026gt;\u0026#34;; // 添加颜色标签 text = $\u0026#34;{colorTag}{text}{closeTag}\u0026#34;; chatOutputText.text += $\u0026#34;\\n{text}\u0026#34;; Canvas.ForceUpdateCanvases(); chatOutputText.GetComponentInParent\u0026lt;ScrollRect\u0026gt;().verticalNormalizedPosition = 0; //SetAnimator(\u0026#34;state\u0026#34;, 0); // 设置动画状态为0 } // 设置动画参数 private void SetAnimator(string _para, int _value) { if (m_Animator == null) // 如果动画控制器为空 return; // 直接返回 m_Animator.SetInteger(_para, _value); // 设置动画控制器的整数参数 } // 修复4：定义符合API规范的数据结构 [System.Serializable] private class RequestBody { public string model; public MessageData[] messages; public float temperature; public int max_tokens; public bool stream; } [System.Serializable] private class MessageData { public string role; public string content; } [System.Serializable] private class DeepSeekResponse { public Choice[] choices; } [System.Serializable] private class Choice { public Message message; } [System.Serializable] private class Message { public string role; public string content; } // 新增：错误响应结构 [System.Serializable] private class ErrorResponse { public ErrorInfo error; } [System.Serializable] private class ErrorInfo { public string message; public string type; public string param; } } 新建空物体gameobj，把该脚本挂在上面。\n2 用户界面 Unity交互层采用UGUI组件构建用户界面，界面设计遵循易用性原则，采用清晰的布局结构与视觉层次，确保用户能够直观理解各功能模块的作用与操作方式。交互层包含对话框界面与角色设定面板。对话框界面主要由以下组件构成：\n对话输入框：使用InputField组件，支持文本输入与语音输入两种方式，用户可直接输入文本或通过语音识别填充文本。\n对话显示区域：使用TextMesh Pro组件，显示用户输入与AI回复的内容，通过不同颜色区分双方消息，并支持富文本格式显示。\n滚动视图：使用ScrollRect组件包裹对话显示区域，实现对话历史的自动滚动，确保用户始终看到最新消息。\n发送按钮：使用Button组件，点击后触发对话发送逻辑，将用户输入传递给智能对话模块。\n语音输入按钮：使用Button组件，点击后开启语音录制功能，调用讯飞语音识别服务。\n背景设置输入框：使用InputField组件，用户可输入对话场景配置描述，如\u0026quot;回答字数限制在100字以内\u0026quot;等。\n确认按钮：使用Button组件，点击后将用户输入的角色设定信息注入智能对话模块。\n用了M Studio的插件，给不同组件上色方便归纳和查看。\n3 脚本信息引用 如图，脚本DeepSeekDialogueManager中开放出来的公共变量可以填上，跨物体引用组件。这里主要：\n填上api key / url UI引用写入 输入框 输出框 发送键 思考指示器 …… 4 初步测试对话功能 可以正常对话。\n","date":"2025-12-20T10:12:30Z","image":"https://Selaphina.github.io/p/%E6%AF%95%E4%B8%9A%E8%AE%BE%E8%AE%A1-%E6%B0%B4%E5%A2%A8%E9%A3%8E%E6%A0%BC%E5%8C%96%E5%9C%BA%E6%99%AF%E6%B8%B2%E6%9F%93%E5%85%B6%E4%B8%89/cover1_hu_89d58f86b094ccaa.png","permalink":"https://Selaphina.github.io/p/%E6%AF%95%E4%B8%9A%E8%AE%BE%E8%AE%A1-%E6%B0%B4%E5%A2%A8%E9%A3%8E%E6%A0%BC%E5%8C%96%E5%9C%BA%E6%99%AF%E6%B8%B2%E6%9F%93%E5%85%B6%E4%B8%89/","title":"毕业设计 | 水墨风格化场景渲染（其三）"},{"content":"进度报告 本次主要记录TTS（文字转语音）的数据集收集，训练，以及推理功能实现。\n1 GPT-SoVITS 作品简介模板 音声来源：[训练集音声来源]\n**免责声明：**本作品仅作为毕业设计发布，可能造成的后果与使用的语音合成项目的作者、贡献者无关。\n[attention:] 最好发视频可以带上GPT-SoVITS的Tag\n项目地址：https://github.com/RVC-Boss/GPT-SoVITS\nGPT-SoVITS是花儿不哭研发的低成本AI音色合成软件。\nTTS（Text-To-Speech）这是一种文字转语音的语音合成。类似的还有SVC（歌声转换）、SVS（歌声合成）等。目前GPT-SoVITS实现了：\nGPT-SoVITS-V1实现了：\n由参考音频的情感、音色、语速控制合成音频的情感、音色、语速 可以少量语音微调训练，也可不训练直接推理 可以跨语种生成，即参考音频（训练集）和推理文本的语种为不同语种 GPT-SoVITS-V2新增特点：\n对低音质参考音频合成出来音质更好 底模训练集增加到5k小时，zero shot性能更好音色更像，所需数据集更少 增加韩粤两种语言，中日英韩粤5个语种均可跨语种合成 更好的文本前端：持续迭代更新。V2中英文加入多音字优化。 GPT-SoVITS-V3V4新增特点：\n音色相似度更像，需要更少训练集来逼近本人（甚至不需要训练SoVITS） GPT合成更稳定，重复漏字更少，也更容易跑出丰富情感 v4修复了v3非整数倍上采样可能导致的电音问题，原生输出48k音频防闷（而v3原生输出只有24k）。作者认为v4是v3的平替，更多还需测试。 2 音源训练集 1.前期训练集收集工作： 收集音源干声。总共收集大约10分钟的音频即可训练出较好的效果。\n2.音频切割： 将时长超过 显存数 秒的音频手动切分至 显存数 秒以下。比如显卡是4090 显存是24g，那么就要将超过24秒的音频手动切分至24s以下，音频时长太长的会爆显存。\n标注 fast whisper是目前最好的英语和日语识别，使用整合包里的ASR脚本即可对处理过的音频进行标注，给每个音频配上文字，这样才能让AI学习到每个字该怎么读。 训练 Windows支持 CUDA 的 NVIDIA 显卡，训练要求每张拥有至少 8G 以上显存。因为显存不够，所以到AutoDL上租卡训练。 训练之后会得到两个权重：\n3 实时推理 1.放好权重文件：\nGPT_weights_v2：放.ckpt文件\nSoVITS_weights_v2:放.pth文件\n2.参考音频（一定要准备好）\n上传一段参考音频，建议是数据集中的音频。最好5秒。参考音频很重要！会学习语速和语气。建议选择有参考文本模式，如果是无文本参考模式很容易出糟糕的推理结果。\n关于top_p,top_k和temperature 这三个值都是用来控制采样的。在推理的时候要挑出一个最好的token，但机器并不知道哪个是最好的。于是先按照top_k挑出前几个token，top_p在top_k的基础上筛选token。最后temperature控制随机性输出。\n比如总共有100个token，top_k设置5，top_p设置0.6，temperature设置为0.5。那么就会从100个token中先挑出5个概率最大的token，这五个token的概率分别是（0.3，0.3，0.2，0.2，0.1），那么再挑出累加概率不超过0.6的token（0.3和0.3），再从这两个token中随机挑出一个token输出，其中前一个token被挑选到的几率更大。以此类推。\n拉满当赌狗，拉低当复读机。\n4 效果 以下语音纯属TTS合成：\n效果：\n5 API接入Unity使用TTS推理 API使用TTS推理： 推理已经在http://localhost:9872/开启。此时参考API文档可以通过python脚本来调用推理进行测试。\nC#与Python之间UDP通信 1.C#作为发送端，通过UDP发送AI文本到Python端。 2.Python接收文本并调用TTS模型生成语音。 3.Python将生成的语音路径通过UDP发送回C#端。 4.C#端加载并播放语音文件。\n效果 1 你好！用英文向我问好！ (本机使用30系显卡，推理速度会稍微慢一点。10词推理10-16s是常有的事。)\n推理已经在http://localhost:9872/开启。此时参考API文档可以通过python脚本来调用推理进行测试。\n","date":"2025-12-20T10:12:30Z","image":"https://Selaphina.github.io/p/%E6%AF%95%E4%B8%9A%E8%AE%BE%E8%AE%A1-%E6%B0%B4%E5%A2%A8%E9%A3%8E%E6%A0%BC%E5%8C%96%E5%9C%BA%E6%99%AF%E6%B8%B2%E6%9F%93%E5%85%B6%E5%9B%9B/cover1_hu_89d58f86b094ccaa.png","permalink":"https://Selaphina.github.io/p/%E6%AF%95%E4%B8%9A%E8%AE%BE%E8%AE%A1-%E6%B0%B4%E5%A2%A8%E9%A3%8E%E6%A0%BC%E5%8C%96%E5%9C%BA%E6%99%AF%E6%B8%B2%E6%9F%93%E5%85%B6%E5%9B%9B/","title":"毕业设计 | 水墨风格化场景渲染（其四）"},{"content":"","date":"2025-12-15T10:12:30Z","image":"https://Selaphina.github.io/p/%E4%B8%80%E7%9F%B3%E4%BA%8C%E9%B8%9F%E9%83%A8%E7%BD%B2/CUC%E5%85%91%E6%8D%A2%E7%82%B92_hu_aee3a9923aa263e8.png","permalink":"https://Selaphina.github.io/p/%E4%B8%80%E7%9F%B3%E4%BA%8C%E9%B8%9F%E9%83%A8%E7%BD%B2/","title":"一石二鸟部署"},{"content":"1 项目简介 什么是 MMagic MMagic 是一个开源的 AIGC（生成式人工智能）工具箱，基于 PyTorch 构建，属于 OpenMMLab 生态系统的一部分。它整合了图像和视频生成、编辑、恢复、增强等一系列任务的算法与工具，目的是帮助研究者和工程师快速实验、训练与部署生成模型。\n项目地址： open-mmlab/mmagic: OpenMMLab Multimodal Advanced, Generative, and Intelligent Creation Toolbox. Unlock the magic 🪄: Generative-AI (AIGC), easy-to-use APIs, awsome model zoo, diffusion models, for text-to-image generation, image/video restoration/enhancement, etc.\n1.虚拟环境\n1 2 3 conda create -n mmagic python=3.9 conda create -n basicvsr python=3.8 conda activate mmagic 2.安装 依赖\nMMagic 依赖 PyTorch，MMEngine 和 MMCV，以下是安装的简要步骤。\n步骤 1. 依照官方教程安装 PyTorch 。\n步骤 2. 使用 MIM 安装 MMCV，MMEngine 和 MMagic 。\n1 2 3 4 pip3 install openmim mim install \u0026#39;mmcv\u0026gt;=2.0.0\u0026#39; mim install \u0026#39;mmengine\u0026#39; mim install \u0026#39;mmagic\u0026#39; 步骤 3. 验证 MMagic 安装成功。\n1 2 3 cd ~ python -c \u0026#34;import mmagic; print(mmagic.__version__)\u0026#34; # Example output: 1.0.0 3.阅读文档\n视频超分辨率 — MMagic 文档\n视频颜色迁移 视频\n21.14:\n1 2 3 4 5 6 python video_transfer.py \\ --content_video \u0026#34;./inputVideo/input_04.mp4\u0026#34; \\ --style \u0026#34;./examples/style/in03.jpg\u0026#34; \\ --output \u0026#34;./outputVideos/output_10.mp4\u0026#34; \\ --image_size 512 \\ --keep_ratio 总结一下，你对工程文件的修改在哪里，运行视频风格迁移的脚本步骤。\n● 工程修改总结\n新增文件 文件 路径 用途 video_transfer.py D:\\WSY\\VideoGrading\\Exp\\WCT2\\video_transfer.py 视频风格迁移主脚本 check_videos.py D:\\WSY\\VideoGrading\\Exp\\WCT2\\check_videos.py 视频信息检查工具（可删除） 新增目录 目录 用途 inputVideo/ 存放输入视频 outputVideos/ 存放输出视频 原有文件未修改 原项目代码（transfer.py, model.py, utils/）保持不变。\n视频风格迁移运行步骤 步骤1：激活环境 1 conda activate wct2 步骤2：运行脚本 1 cd D:\\WSY\\VideoGrading\\Exp\\WCT2 1 python video_transfer.py --content_video \u0026#34;./inputVideo/input_04.mp4\u0026#34; --style \u0026#34;./examples/style/in58.png\u0026#34; --output \u0026#34;./outputVideos/output.mp4\u0026#34; --image_size 512 --keep_ratio 参数说明 参数 必填 说明 示例 \u0026ndash;content_video ✓ 输入视频路径 ./inputVideo/input_01.mp4 \u0026ndash;style ✓ 风格图片路径 ./examples/style/in14.png \u0026ndash;output 输出视频路径 ./outputVideos/output.mp4 \u0026ndash;image_size 处理分辨率 512 (默认) \u0026ndash;alpha 风格强度 0-1 1.0 (默认) \u0026ndash;keep_ratio 保持原始宽高比 添加此参数启用 \u0026ndash;cpu 使用CPU运行 添加此参数启用 可用风格图 D:\\WSY\\VideoGrading\\Exp\\WCT2\\examples\\style\n├── in00.png ├── in02.jpg ├── in03.jpg ├── in14.png ├── in17.png ├── in34.png ├── in58.png └── in64.png\n处理速度参考 (RTX 3060) 512分辨率：约 2.5 帧/秒 30fps视频每秒内容需 12秒 处理 web app下次启动方式： WCT2/ ├── app.py # Flask 后端 ├── templates/index.html # 前端页面 ├── static/css/style.css # 样式 ├── static/js/app.js # 前端逻辑 └── requirements.txt # 依赖\nconda activate wct2 cd D:\\WSY\\VideoGrading\\Web\\WCT2 python app.py\n然后访问 http://localhost:5000\nFeaturize 安装BasicVSR\n1 conda activate basicvsr 跑项目\n1 python demo/restoration_video_demo.py configs/basicvsr_plusplus_reds4.py chkpts/basicvsr_plusplus_reds4.pth data/input000.mp4 results/output_000.mp4 1 python demo/restoration_video_demo.py configs/basicvsr_plusplus_reds4.py chkpts/basicvsr_plusplus_reds4.pth data/demo_000 results/output_000 36-40\n─ Worked for 32m 17s ────────────────────────────────────────────────────────────────────────────────────────── • 已按 conda 环境把依赖装好并下载了权重，但 demo 运行卡在 Windows 的 DLL 文件被占用（av 的 swscale-*.dll）。我 这边做过的步骤如下：\n已创建 basicvsr 环境并安装了 PyTorch 1.10.2+cu113（为匹配 Windows 的 mmcv-full 预编译包） 已安装 mmcv-full==1.5.0（cu113/torch1.10.0 轮子） 已 pip install -v -e . 安装项目依赖 当前错误： OSError: Error loading swscale-*.dll; 另一个程序正在使用此文件 请先做这几件事之一，然后重试 demo：\n关闭所有可能在用 Python/AV 的进程（IDE、Jupyter、Python 脚本） 重新打开终端（或重启电脑）再跑 如果还不行，重新安装 av： conda activate basicvsr pip uninstall -y av pip install av==12.3.0\n重试 demo 命令（推荐用激活环境跑）：\nconda activate basicvsr python demo/restoration_video_demo.py configs/basicvsr_plusplus_reds4.py chkpts/basicvsr_plusplus_reds4.pth data/demo_000 results/demo_000\n如果你愿意，我也可以继续帮你：\n排查 DLL 占用（看是哪个进程锁住了 av） 跑你自己的视频路径并调显存占用参数 › python demo/restoration_video_demo.py configs/basicvsr_plusplus_reds4.py chkpts/basicvsr_plusplus_reds4.pth data/demo_000 results/demo_000\n• 要我帮你在当前机器上执行这条 demo 命令吗？ 如果要运行，请确认你已先 conda activate basicvsr，我就直接跑。\n","date":"2025-12-14T10:12:30Z","image":"https://Selaphina.github.io/p/mmagic%E9%83%A8%E7%BD%B2-%E8%A7%86%E9%A2%91%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/CUC%E5%85%91%E6%8D%A2%E7%82%B92_hu_aee3a9923aa263e8.png","permalink":"https://Selaphina.github.io/p/mmagic%E9%83%A8%E7%BD%B2-%E8%A7%86%E9%A2%91%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/","title":"MMagic部署+视频超分辨率"},{"content":"1 记录 1.虚拟环境\n1 2 3 conda create -n wct2 python=3.6 -y conda activate wct2 2.查看NVIDIA-SMI\n1 nvidia-smi 得到\n1 2 CUDA Version: 12.8 GPU: NVIDIA GeForce RTX 3060 (6GB) GPU：RTX 3060（6GB）：显存 6GB，跑 512~1024 分辨率图片完全 OK\nCUDA Version: 12.8： 👉 只是说明： 你的显卡驱动支持 CUDA 12.8 👉 ❌ 并不等于：你必须安装 CUDA 12.8 的 PyTorch\n3.git clone\n1 git clone https://github.com/clovaai/WCT2.git 4.安装 WCT2 其他依赖\n1 pip install -r requirements.txt featurize 1.持久化环境\n1 conda create --prefix /home/featurize/work/wct2test python=3.8 1 conda activate /home/featurize/work/wct2test 1 git clone https://github.com/clovaai/WCT2.git 1 pip install -r requirements.txt 输入视频路径\n1 2 3 D:\\WSY\\VideoGrading\\Exp\\WCT2\\inputVideo\\input_01.mp4 D:\\WSY\\VideoGrading\\Exp\\WCT2\\inputVideo\\input_02.mp4 D:\\WSY\\VideoGrading\\Exp\\WCT2\\inputVideo\\input_03.mp4 风格图片路径\n1 D:\\WSY\\VideoGrading\\Exp\\WCT2\\examples\\style\\in14.png 输出视频路径\n1 D:\\WSY\\VideoGrading\\Exp\\WCT2\\outputVideos 视频颜色迁移 视频\n21.14:\n1 2 3 4 5 6 python video_transfer.py \\ --content_video \u0026#34;./inputVideo/input_04.mp4\u0026#34; \\ --style \u0026#34;./examples/style/in03.jpg\u0026#34; \\ --output \u0026#34;./outputVideos/output_10.mp4\u0026#34; \\ --image_size 512 \\ --keep_ratio 总结一下，你对工程文件的修改在哪里，运行视频风格迁移的脚本步骤。\n● 工程修改总结\n新增文件 文件 路径 用途 video_transfer.py D:\\WSY\\VideoGrading\\Exp\\WCT2\\video_transfer.py 视频风格迁移主脚本 check_videos.py D:\\WSY\\VideoGrading\\Exp\\WCT2\\check_videos.py 视频信息检查工具（可删除） 新增目录 目录 用途 inputVideo/ 存放输入视频 outputVideos/ 存放输出视频 原有文件未修改 原项目代码（transfer.py, model.py, utils/）保持不变。\n视频风格迁移运行步骤 步骤1：激活环境 1 conda activate wct2 步骤2：运行脚本 1 cd D:\\WSY\\VideoGrading\\Exp\\WCT2 1 python video_transfer.py --content_video \u0026#34;./inputVideo/input_04.mp4\u0026#34; --style \u0026#34;./examples/style/in58.png\u0026#34; --output \u0026#34;./outputVideos/output.mp4\u0026#34; --image_size 512 --keep_ratio 参数说明 参数 必填 说明 示例 \u0026ndash;content_video ✓ 输入视频路径 ./inputVideo/input_01.mp4 \u0026ndash;style ✓ 风格图片路径 ./examples/style/in14.png \u0026ndash;output 输出视频路径 ./outputVideos/output.mp4 \u0026ndash;image_size 处理分辨率 512 (默认) \u0026ndash;alpha 风格强度 0-1 1.0 (默认) \u0026ndash;keep_ratio 保持原始宽高比 添加此参数启用 \u0026ndash;cpu 使用CPU运行 添加此参数启用 可用风格图 D:\\WSY\\VideoGrading\\Exp\\WCT2\\examples\\style\n├── in00.png ├── in02.jpg ├── in03.jpg ├── in14.png ├── in17.png ├── in34.png ├── in58.png └── in64.png\n处理速度参考 (RTX 3060) 512分辨率：约 2.5 帧/秒 30fps视频每秒内容需 12秒 处理 web app下次启动方式： WCT2/ ├── app.py # Flask 后端 ├── templates/index.html # 前端页面 ├── static/css/style.css # 样式 ├── static/js/app.js # 前端逻辑 └── requirements.txt # 依赖\nconda activate wct2 cd D:\\WSY\\VideoGrading\\Web\\WCT2 python app.py\n然后访问 http://localhost:5000\n","date":"2025-12-14T10:12:30Z","image":"https://Selaphina.github.io/p/wct2%E9%83%A8%E7%BD%B2/CUC%E5%85%91%E6%8D%A2%E7%82%B92_hu_aee3a9923aa263e8.png","permalink":"https://Selaphina.github.io/p/wct2%E9%83%A8%E7%BD%B2/","title":"WCT2部署"},{"content":"1 前期准备工作 1.新建URP 3D项目\n2.点击资产面板的URP资产，此时右侧面板高亮，可以Add Render Objects\n添加Render Objects\n添加3个pass。\nmask改为everything。\nTODO: 有仙人曾言： // 我自己试下来，在角色身上 LowQuality 比 Medium 和 High 好 // Medium 和 High 采样数多，过渡的区间大，在角色身上更容易出现 Perspective aliasing\n等到时候自己验证一下在说 。\nFaceLightmap\nBody_Diffuse\nBody_lightmap\n法线图\nshadow ramp\nFace_Diffuse\nHair_Diffuse\nhair_lightmap\nMetalMap\n贴图设置完毕。\n值得注意的是 杜林的翅膀贴图是和头发放在一起，观察一下贴图纹理的对应。\n2 代码结构 1.整体结构 一开始不太习惯HLSL的代码结构，和CG有一定的区别，注意。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Shader \u0026#34;EXAM1/EXAM_Shader\u0026#34; { Property { …… } Subshader { Pass {……} Pass {……} } } 2.Subshader结构 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 SubShader { HLSLINCLUDE #include \u0026#34;../../ShaderLibrary/…….hlsl\u0026#34; ENDHLSL Tags {……} Pass {……} Pass {……} } Subshader结构 1.HLSLINCLUDE \u0026amp; ENDHLSL\n1 2 3 4 5 6 7 8 9 10 11 SubShader { HLSLINCLUDE //导入库 #include \u0026#34;Packages/com.unity.render-pipelines.universal/ShaderLibrary/Core.hlsl\u0026#34; //默认库 #include \u0026#34;Packages/com.unity.render-pipelines.universal/ShaderLibrary/Lighting.hlsl\u0026#34; //光照库 ENDHLSL Pass { }; } HLSLINCLUDE 和 ENDHLSL 是Unity ShaderLab中的指令，它们之间的代码会被自动包含到该着色器的所有Pass中。\n相当于一个“公共头文件”，写在这里的代码（如变量声明、函数、宏定义）对所有Pass都可见。\n为什么这样设计？\n减少重复：如果不使用HLSLINCLUDE，每个Pass都需要单独声明这些变量和贴图，代码会冗长且难以维护。 保持一致性：所有Pass使用同一套参数和贴图，确保渲染结果统一。 2 HLSLPROGRAM\n在Unity URP Shader中，每个Pass通常需要将顶点和片元着色器代码包裹在HLSLPROGRAM和ENDHLSL块中。\nPass结构 Pass块中，需要在Tags之后添加HLSLPROGRAM：\n1 2 3 4 5 6 7 8 9 10 11 12 Pass { Tags { \u0026#34;LightMode\u0026#34; = \u0026#34;head\u0026#34; } HLSLPROGRAM #pragma vertex vert #pragma fragment frag // 您的a2v和v2f结构体定义 // 您的vert和frag函数定义 ENDHLSL } 3 在 frag 函数内部定义函数（HLSL 不允许）\n在 frag 里写大量函数定义，例如：\n1 2 3 4 5 6 7 8 9 10 11 12 half4 frag (v2f i) : SV_TARGET { float3 shadow_ramp(...) { ... } float3 Spec(...) { ... } float3 Metal(...) { ... } float3 edgeLight(...) { ... } float3 light(...) { ... } float3 Body(...) { ... } float3 Face(...) { ... } ... } 这是绝对错误的\nHLSL / ShaderLab 不支持函数嵌套定义 函数必须定义在全局作用域 DX11 编译器会直接报错 正确写法：\n1 2 3 4 5 6 7 8 9 10 11 12 float3 shadow_ramp(float4 lightmap, float NdotL) { ... } float3 Spec(...) { ... } float3 Metal(...) { ... } float3 edgeLight(...) { ... } float3 light(...) { ... } float3 Body(...) { ... } float3 Face(...) { ... } half4 frag(v2f i) : SV_TARGET { ... } 3 正式开始 正式开始前先把计算需要用到的变量和向量等数据准备好，先来准备面板参数，面板参数如下：\n1 2 3 4 Properties{ } URP的声明参数必须包含在CBUFFER_START(UnityPerMaterial)和CBUFFER_END之间，我们把这些代码和声明贴图的代码都用HLSLINCLUDE和ENDHLSL代码块包起来，之后写的方法也放在这里面，这样后面就不用在每个pass都声明一次了。\n1 2 3 4 5 Subshader { } 1.第一个pass：渲染正面 1 2 3 Pass{ Tags{ \u0026#34;LightMode\u0026#34; = \u0026#34;head\u0026#34; }//渲染标签 } 1）输入结构：a2v定义从应用阶段（CPU）到顶点着色器的顶点数据。 1 2 3 4 5 6 struct a2v{ float4 vertex : POSITION;//顶点坐标 float2 texcoord0 : TEXCOORD0;//纹理坐标UV0 float3 normal : NORMAL;//顶点法线 float4 tangent : TANGENT;//顶点切线 }; 为什么normal是float3而tangent是float4？\n法线（normal）：float3 法线是一个三维向量（x, y, z），表示顶点表面的朝向。 它只需要三个分量就能完整描述方向，因此使用float3足够。 切线（tangent）：float4 切线通常用于法线贴图（normal mapping）计算，需要与法线和副切线（binormal/bitangent）构成切线空间（tangent space）。 前三个分量（x, y, z）表示切线的方向向量。 第四个分量（w）是一个符号值，通常为+1或-1，用于指示副切线的方向。 关键区别：\n法线的方向是绝对的： 法线向量(x, y, z)本身就完整定义了方向 例如，法线(0, 1, 0)明确表示\u0026quot;向上\u0026quot;，没有歧义 切线需要确定副切线方向： 给定法线和切线后，副切线可以通过叉积计算：副切线 = 叉积(法线, 切线) 但叉积有两种可能方向：左手系或右手系 tangent.w（通常±1）就是用来指定这个方向的： 如果tangent.w = 1：副切线 = 叉积(法线, 切线) 如果tangent.w = -1：副切线 = 叉积(切线, 法线) 几何意义： 法线、切线、副切线构成切线空间基 法线是\u0026quot;主方向\u0026quot;，切线和副切线是\u0026quot;辅助方向\u0026quot; 切线的w分量确保整个坐标系的一致性（避免镜像翻转） 2）输出结构：v2f定义在顶点着色器中构建一个【切线空间到世界空间的3x4变换矩阵】，并传递到片元着色器的输出数据。 通常被称为 TBN矩阵（由Tangent、Bitangent、Normal三个向量构成），它的核心作用是搭建一座桥梁，将法线方向从“切线空间”转换到“世界空间”，这是实现法线贴图（Normal Mapping）效果的关键步骤\n1 2 3 4 5 6 7 8 9 10 11 12 struct v2f{ float4 pos : SV_POSITION;//在裁剪空间的顶点位置 float2 uv0 : TEXCOORD0;//uv0，第一套纹理坐标 // 这三行共同构成了一个3x4矩阵。 float4 TtoW0 : TEXCOORD1; //x切线,y副切线,z法线,w顶点 float4 TtoW1 : TEXCOORD2; //x切线,y副切线,z法线,w顶点 float4 TtoW2 : TEXCOORD3; //x切线,y副切线,z法线,w顶点 //x-切线tangent //y-副切线bitangent //z-法线normal //w-顶点 }; 输出结构定义一个4维矩阵存放数据，以充分利用插值寄存器。\na2v和v2f两个结构体中使用了两次TEXCOORD0语义，这是被允许的，因为语义就是会在两个结构体中有着不同的含义，可以理解成代号，但在之后的过程中这两个TEXCOORD0会指代不同的纹理寄存器，所以不会冲突。\nTEXCOORD0 在不同结构体中并不代表一个固定的寄存器，而是一个语义标记，这个语义对应一个四维向量（float4）的存储空间，用于在着色器阶段之间传递数据。\nTEXCOORD1、TEXCOORD2、TEXCOORD3都用于在着色器阶段间传递数据，但它们通过数字编号（1, 2, 3）来代表不同的数据通道，用于区分不同的信息。\n合理规划数据用量：一个 float4可以存储一个四维数据，也可以存储多个低维数据（如两个 float2）。合理打包数据可以节省宝贵的 TEXCOORD通道。\n数量限制：TEXCOORD通道的数量并非无限，存在上限（例如通常最多到 TEXCOORD7），需根据目标平台合理规划。\n在顶点Shader将需要的数据传递给片元Shader，矩阵的xyzw分别存放切线，副切线，法线与顶点。\n3) vert顶点shader：将需要的数据传递给片元Shader，矩阵的xyzw分别存放切线，副切线，法线与顶点。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 v2f vert(a2v v) { v2f o; o.pos = TransformObjectToClip(v.vertex.xyz); o.uv0 = v.texcoord0; float3 nDirWS = TransformObjectToWorldNormal(v.normal); float3 tDirWS = TransformObjectToWorld(v.tangent.xyz); float3 bDirWS = cross(nDirWS,tDirWS) * v.tangent.w; float3 posWS = TransformObjectTOWorld(v.vertex.xyz); o.TtoW0 = float4(nDirWS.x,nDirWS.x,bDirWS.x,posWS.x); o.TtoW1 = float4(nDirWS.y,nDirWS.y,bDirWS.y,posWS.y); o.TtoW2 = float4(nDirWS.z,nDirWS.z,bDirWS.z,posWS.z); return o; } 注意 v.vertex的顶点坐标是4维，因为齐次坐标系多1维（存平移信息）\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 //顶点Shader v2f vert (a2v v) { v2f o; //定义返回值 //MVP变换(模型空间\u0026gt;\u0026gt;世界空间\u0026gt;\u0026gt;视觉空间\u0026gt;\u0026gt;裁剪空间) o.pos = TransformObjectToHClip(v.vertex.xyz); //传递uv0(无变换) o.uv0 = v.texcoord0; // 世界空间法线 float3 nDirWS = TransformObjectToWorldNormal(v.normal); // 世界空间切线 float3 tDirWS = TransformObjectToWorld(v.tangent.xyz); //世界空间副切线 float3 bDirWS = cross(nDirWS, tDirWS) * v.tangent.w; //世界顶点位置 float3 posWS = TransformObjectToWorld(v.vertex.xyz); //构建切线-世界空间变换矩阵 //x切线,y副切线,z法线,w顶点 o.TtoW0 = float4(tDirWS.x, bDirWS.x, nDirWS.x, posWS.x); o.TtoW1 = float4(tDirWS.y, bDirWS.y, nDirWS.y, posWS.y); o.TtoW2 = float4(tDirWS.z, bDirWS.z, nDirWS.z, posWS.z); return o; //返回顶点Shader } 4）【重点】frag 片元着色器 回顾：在pass之前，声明的所有变量\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 SubShader { HLSLINCLUDE //导入库 #include \u0026#34;Packages/com.unity.render-pipelines.universal/ShaderLibrary/Core.hlsl\u0026#34; //默认库 #include \u0026#34;Packages/com.unity.render-pipelines.universal/ShaderLibrary/Lighting.hlsl\u0026#34; //光照库 CBUFFER_START(UnityPerMaterial) //常量缓冲区开头 //声明面板参数 float _genshinShader; //是否是脸部 //diffuse float _fresnel; //边缘光范围 float _edgeLight; //边缘光强度 float _diffuseA; //diffuseA float _Cutoff; //透明阈值 float4 _glow; //自发光强度 float _flicker; //发光闪烁速度 //lightmap/FaceLightmap float _bright; //亮面范围 float _grey; //灰面范围 float _dark; //暗面范围 //normal float _bumpScale; //法线强度 //ramp float _dayAndNight; //是否是白天 float _lightmapA0; //1.0_Ramp条数 float _lightmapA1; //0.7_Ramp条数 float _lightmapA2; //0.5_Ramp条数 float _lightmapA3; //0.3_Ramp条数 float _lightmapA4; //0.0_Ramp条数 //高光 float _gloss; //高光范围 float _glossStrength; //高光强度 float3 _metalMapColor; //金属反射颜色 //描边 float _OutlineWidth; //描边粗细 float _OutlineScale; //描边范围 float _OutlineZOffset; //Outline Z Offset float _Alpha; //Alpha float _AlphaClip; //Alpha Clip float4 _OutlineColor0; //描边颜色1 float4 _OutlineColor1; //描边颜色2 float4 _OutlineColor2; //描边颜色3 float4 _OutlineColor3; //描边颜色4 float4 _OutlineColor4; //描边颜色5 float4 _CustomOutlineCol; //Custom Outline Color CBUFFER_END //常量缓冲区结尾 //------声明贴图------- //1.声明 纹理对象：漫反射纹理 TEXTURE2D(_diffuse); //Diffuse //2.声明 采样器对象：漫反射采样器 SAMPLER(sampler_diffuse); //3.以此类推 TEXTURE2D(_lightmap); //Lightmap/FaceLightmap SAMPLER(sampler_lightmap); TEXTURE2D(_bumpMap); //Normal SAMPLER(sampler_bumpMap); TEXTURE2D(_ramp); //Shadow_Ramp SAMPLER(sampler_ramp); TEXTURE2D(_metalMap); //MetalMap SAMPLER(sampler_metalMap); ENDHLSL 为什么纹理和采样器分开声明？ 纹理和采样器分开声明的方式，是现代图形API的常见做法（如DirectX 11及以上、Vulkan、Metal等）的特性。以实现更灵活的纹理采样和更好的性能。\n作用：复用采样器。\n这是最直接的好处。在同一个Shader中，如果多张纹理需要使用相同的过滤模式（如Linear）和包裹模式（如Repeat），那么只需声明一个具有相应模式的采样器，所有纹理都可以共享它。\n片元着色器的主要任务是为模型表面的每一个像素计算最终颜色。它的工作流程可以概括为以下几步：\n纹理采样：从不同的纹理中读取信息。\n向量准备：计算光照和视角所需的方向向量。\n光照计算：根据不同的渲染路径（身体或脸部）计算颜色。\n后期处理：处理自发光或透明度裁剪。\n输出：返回最终的像素颜色。\nfrag 片元着色器 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 half4 frag (v2f i) : SV_TARGET { //采样贴图 float3 baseColor = SAMPLE_TEXTURE2D(_diffuse, sampler_diffuse, i.uv0).rgb; //diffuseRGB通道 float diffuseA = SAMPLE_TEXTURE2D(_diffuse,sampler_diffuse, i.uv0).a; //diffuseA通道 float4 lightmap = SAMPLE_TEXTURE2D(_lightmap, sampler_lightmap, i.uv0).rgba; //lightmap //法线贴图 float3 nDirTS = UnpackNormal(SAMPLE_TEXTURE2D(_bumpMap, sampler_bumpMap, i.uv0)).rgb; //切线空间法线(采样法线贴图并解码) nDirTS.xy *= _bumpScale; //法线强度 nDirTS.z = sqrt(1.0 - saturate(dot(nDirTS.xy, nDirTS.xy))); //计算法线z分量 //saturate的作用是把输入值钳制在（0.0, 1.0）之间. //准备向量 float3 posWS = float3(i.TtoW0.w, i.TtoW1.w, i.TtoW2.w); //世界空间顶点 //切线空间法线转世界空间法线 float3 nDirWS = normalize(half3(dot(i.TtoW0.xyz, nDirTS), dot(i.TtoW1.xyz, nDirTS), dot(i.TtoW2.xyz, nDirTS))); Light mlight = GetMainLight(); //光源 float3 lDirWS= normalize(mlight.direction); //世界光源方向(平行光) float3 vDirWS = normalize(_WorldSpaceCameraPos.xyz - posWS.xyz); //世界观察方向 float3 nDirVS = normalize(mul((float3x3)UNITY_MATRIX_V, nDirWS)); //世界空间法线转观察空间法线 float3 hDirWS = normalize(vDirWS + lDirWS) ; //半角方向 //向量点乘 float NdotL = dot(nDirWS, lDirWS); //兰伯特 float NdotH = dot(nDirWS, hDirWS); //Blinn-Phong float NdotV = dot(nDirWS, vDirWS); //菲涅尔 float3 col = float3(0.0, 0.0, 0.0); //主体渲染 if(_genshinShader == 0.0){ //身体 col = Body(NdotL, NdotH, NdotV, lightmap, baseColor, nDirVS); }else if(_genshinShader == 1.0){ //脸部 col = Face(lDirWS, baseColor, i.uv0); } //计算diffuse.a if(_diffuseA == 2){ //自发光 float3 diffA = light(col, diffuseA); col = col + diffA; }else if(_diffuseA == 1){ //裁剪 diffuseA = smoothstep(0.05, 0.7, diffuseA); //去除噪点 clip(diffuseA - _Cutoff); } return half4(col, 1.0); //输出 } 其中：\n1 Saturate Saturate是钳制函数，作用是将输入值限制在0.0~1.0之间。\n如果输入值 小于0，则返回 0。\n如果输入值 大于1，则返回 1。\n如果输入值 在0和1之间，则原样返回。\n法线的z值nDirTS.z = sqrt (1.0 - saturate( dot ( nDirTS.xy , nDirTS.xy ) ) );等价于z = sqrt(1 - (x² + y²))\n数学上，平方和 x² + y²有可能略微超过1（例如由于纹理采样的精度问题）。如果不对其进行钳制，1.0 - (x² + y²)可能会得到一个负数，而对负数进行 sqrt开方运算会产生无效的结果（在Shader中通常会导致不可预知的渲染错误，如出现黑色斑点或NaN）。使用 saturate确保了开方运算的内容永远是一个非负数，从而避免了这种错误。\n2 TBN矩阵 1 2 //切线空间法线转世界空间法线 float3 nDirWS = normalize(half3(dot(i.TtoW0.xyz, nDirTS), dot(i.TtoW1.xyz, nDirTS), dot(i.TtoW2.xyz, nDirTS))); $$ 世界空间法线 = TBN矩阵 × 切线空间法线 $$代码中混合使用了 float和 half精度。对于颜色等不需要高精度的数据，使用 half可以优化性能。\n在着色器中，颜色分量（R, G, B, A）通常被规范在 [0, 1]的范围内。这个范围完全在 fixed类型的表示能力之内.\n人眼的感知局限：人眼对颜色的细微变化并不像对亮度变化那样敏感。微小的颜色量化误差（例如，0.5和 0.501的红色）在绝大多数情况下是难以察觉的.\n哪些数据需要高精度\n与颜色相反，以下类型的数据通常要求使用 float高精度：\n空间坐标：尤其是世界空间坐标和纹理坐标。它们的数值范围很大且变化非常细微，高精度是确保物体位置、轮廓和纹理映射准确无误的基础。 复杂数学运算：在进行三角函数计算（如sin, cos）、幂运算（pow）或复杂的插值时，必须使用 float来保证中间过程和最终结果的准确性，避免误差累积导致画面瑕疵。 需要高动态范围（HDR）的颜色：虽然普通颜色用half足矣，但HDR颜色的值会远超 [0, 1]的范围，因此需要 half或 float来存储和处理。 3 切线空间法线贴图 颜色特征:\n整体偏蓝紫色\n核心优势:\n旋转不变性、可复用性高\n适用场景:\n通用性强，适用于可变形、动画模型\n4 三个光照模型： 1 2 3 4 //向量点乘 float NdotL = dot(nDirWS, lDirWS); //兰伯特 float NdotH = dot(nDirWS, hDirWS); //Blinn-Phong float NdotV = dot(nDirWS, vDirWS); //菲涅尔 在计算前，我们看一下最终的混合分别需要什么：漫反射(半Lambet) + 高光(BlinnPhong) + 金属(MatCap) + 边缘光(菲涅尔) + 自发光 **+ 后处理(Bloom、ToneMapping、ColorAdjustments)。**那我们就按这个顺序来一一实现一下。\n菲涅尔效应（Fresnel Effect） 是一个核心物理规律，用于描述光线在物体表面的反射强度随着视角角度变化的现象。\n兰伯特（Lambert）模型公式 理想的漫反射：\n$$ I _{diffuse} ​\n=K _d ​\n⋅I _l ​\n⋅max(0,N⋅L) $$\n半兰伯特（Half-Lambert）改进\n基础兰伯特模型的一个问题是，当法线与光线方向垂直时，计算结果会直接变为0，导致背光面一片死黑。半兰伯特模型通过一个简单的缩放和平移变换来缓解这个问题\n这个技巧将点乘的结果从区间 [−1,1]映射到 [0,1]，使得背光区域也能保留一定的细节，常用于卡通渲染等风格化效果中\n$$ I _{half-lambert} ​\n=K _d ​\n⋅I _l ​\n⋅(0.5⋅(N⋅L)+0.5) $$\n布林-冯（Blinn-Phong）模型公式 布林-冯模型是一个完整的光照模型，它结合了环境光、漫反射和镜面高光三项。其总光照强度为\n$$ I _{total} ​\n=I _{ambient} ​\n+I _{diffuse} ​\n+I _{specular} ​ $$ **环境光（Ambient）** $$ I _{ambient} ​\n=K a ​\n⋅I a ​ $$ Ka：材质的环境光反射系数\n漫反射（Diffuse） $$ I _{diffuse} ​\n=K _d ​\n⋅I _l ​\n⋅max(0,N⋅L) $$\n镜面高光（Specular）·布林-冯的核心\n$$ I specular ​\n=K _s ​\n⋅I _l ​\n⋅(max(0,N⋅H)) ^{shininess} $$ H：半角向量，通过将光线方向 L和视线方向 V相加后归一化得到：H=∣L+V∣L+V。\n**shininess：**高光指数，这是一个非常重要的参数。值越大，高光点越小、越锐利，表示表面越光滑；值越小，高光范围越大、越柔和，表示表面越粗糙。\nKs：材质的镜面反射系数，决定了高光的强度和颜色\n5)漫反射： shadow_ramp（lightmap，NdotL） 我们看一下最终的混合分别需要什么：漫反射(半Lambet) + 高光(BlinnPhong) + 金属(MatCap) + 边缘光(菲涅尔) + 自发光 + 后处理(Bloom、ToneMapping、ColorAdjustments)。\n原神角色的漫反射，最重要的是ramp的部分，先看一下原神的ramp图。\n共2张shadowmap，一张给body，一张给hair\n原神的ramp一共十条颜色，上面五条代表白天，下面五条代表晚上\n怎样采样这张ramp呢？\n+0.05：防止采样到色块边缘。\n1 2 3 4 //lightmap/FaceLightmap float _bright; //亮面范围 float _grey; //灰面范围 float _dark; //暗面范围 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 //完整的shadow_ramp float3 shadow_ramp(float4 lightmap, float NdotL){ lightmap.g = smoothstep(0.2, 0.5, lightmap.g); //lightmap.g float halfLambert = smoothstep(0.0, _grey, NdotL + _dark) * lightmap.g; //半Lambert float brightMask = step(_bright, halfLambert); //亮面 //判断白天与夜晚 float rampSampling = 0.0; if(_dayAndNight == 0){rampSampling = 0.5;} //计算ramp采样条数 float ramp0 = _lightmapA0 * -0.1 + 1.05 - rampSampling; //0.95 float ramp1 = _lightmapA1 * -0.1 + 1.05 - rampSampling; //0.65 float ramp2 = _lightmapA2 * -0.1 + 1.05 - rampSampling; //0.75 float ramp3 = _lightmapA3 * -0.1 + 1.05 - rampSampling; //0.55 float ramp4 = _lightmapA4 * -0.1 + 1.05 - rampSampling; //0.45 //分离lightmap.a各材质 float lightmapA2 = step(0.25, lightmap.a); //0.3 float lightmapA3 = step(0.45, lightmap.a); //0.5 float lightmapA4 = step(0.65, lightmap.a); //0.7 float lightmapA5 = step(0.95, lightmap.a); //1.0 //重组lightmap.a float rampV = ramp0; //0.0 rampV = lerp(rampV, ramp1, lightmapA2); //0.3 rampV = lerp(rampV, ramp2, lightmapA3); //0.5 rampV = lerp(rampV, ramp3, lightmapA4); //0.7 rampV = lerp(rampV, ramp4, lightmapA5); //1.0 //采样ramp float3 ramp = SAMPLE_TEXTURE2D(_ramp, sampler_ramp, float2(halfLambert, rampV)); float3 shadowRamp = lerp(ramp, halfLambert, brightMask); //遮罩亮面 return shadowRamp; //输出结果 } 5.1 Lightmap.g lightmap.g该图的g通道存储了模型的AO，仅看lightmap.g结果如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 //这里仅做测试代码 //ramp float3 shadow_ramp(float4 lightmap, float NdotL){ float3 ramp = lerp(0, 1, lightmap.g); return ramp; } //身体//身体 float3 Body(){ float3 ramp = shadow_ramp(lightmap, NdotL); //ramp …… float3 body = ramp; return body; //输出结果 } 很显然，仅lightmap.g的AO边缘锯齿明显，smoothstep后的则平滑。\n5.2 halfLambert采样ramp（U轴） 1 2 lightmap.g = smoothstep(0.2, 0.5, lightmap.g); //lightmap.g，0.2-0.5是我截取lightmap的rgb值估计的一个范围 float halfLambert = smoothstep(0.0, _grey, NdotL + _dark) * lightmap.g; //半Lambert变体 NdotL是顶点法向量与光照方向点积的结果，传统范围是 [-1, 1].\n传统半Lambert: halfLambert = 0.5（NdotL）+0.5\n把NdotL的范围压缩到（0，1）内。这样原本位于（-1，0）的暗部也会提亮。\n半兰伯特模型变体 1 halfLambert (0.0, _grey, NdotL + _dark) smoothstep(0.0,_grey, \u0026hellip;)：这个函数将 (NdotL + _dark)的值映射为： 所有小于 0.0的值被压成0（最暗）， 所有大于 _grey的值被提成1（最亮）， 介于 0.0和 _grey之间的值则产生平滑的过渡。 grey参数控制了从暗部过渡到亮部的区域宽度。 NdotL + _dark：这里的 _dark是一个偏移量（对应传统半Lambert的0.5），可以理解为暗部阈值。增加 _dark会整体提亮模型，因为更多的值会超过 _grey阈值；减小它则会变暗。 5.3 rampV采样lightmap（V轴） 1 2 3 //判断白天与夜晚 float rampSampling = 0.0; if(_dayAndNight == 0){rampSampling = 0.5;} 然后开放5个参数在面板上，方便美术调节。并把面板上1-5的采样条数转换成ramp图的取值范围。如果是白天，则是采样到0.55-0.95，而晚上则是采样到0.05-0.45.\n1 2 3 4 5 6 //计算ramp采样条数 float ramp0 = _lightmapA0 * -0.1 + 1.05 - rampSampling; //0.95 | 0.45 float ramp1 = _lightmapA1 * -0.1 + 1.05 - rampSampling; //0.65 | 0.35 float ramp2 = _lightmapA2 * -0.1 + 1.05 - rampSampling; //0.75 | 0.25 float ramp3 = _lightmapA3 * -0.1 + 1.05 - rampSampling; //0.55 | 0.15 float ramp4 = _lightmapA4 * -0.1 + 1.05 - rampSampling; //0.45 | 0.05 然后用step将lightmap.a的各个材质分离出来。\n1 2 3 4 5 //分离lightmap.a各材质 float lightmapA2 = step(0.25, lightmap.a); //0.3 float lightmapA3 = step(0.45, lightmap.a); //0.5 float lightmapA4 = step(0.65, lightmap.a); //0.7 float lightmapA5 = step(0.95, lightmap.a); //1.0 step(a, b)的含义\nb\u0026lt;a b\u0026gt;=a 0 1 所以 lightmapA2/3/4/5的值可以看作一个开关：要么是1，要么是0.\n1 2 3 4 5 6 //重组lightmap.a float rampV = ramp0; rampV = lerp(rampV, ramp1, lightmapA2); //0.3 rampV = lerp(rampV, ramp2, lightmapA3); //0.5 rampV = lerp(rampV, ramp3, lightmapA4); //0.7 rampV = lerp(rampV, ramp4, lightmapA5); //1.0 lerp(a, b, t)\nt = 0 → = a\nt = 1 → = b\n这整个流程就是一个条件覆盖链： 如果区域 ≥ 0.25 → rampV换成 ramp1 如果区域 ≥ 0.45 → rampV再换成 ramp2 如果区域 ≥ 0.65 → rampV再换成 ramp3 如果区域 ≥ 0.95 → rampV最终换成 ramp4\n5.4 U（兰伯特）+V（rampV）采样shadowramp图 1 2 3 //采样ramp float3 ramp = SAMPLE_TEXTURE2D(_ramp, sampler_ramp, float2(halfLambert, rampV)); return ramp; 可以得到：\n将亮面遮罩出来，用step来处理halfLambert分出亮面和暗面。(用smoothstep会更平滑) 1 float brightMask = step(_bright, halfLambert); //亮面 _bright开放变量给inspector手动设置，一般0.90-0.99,对 step (bright, halfLambert)来说\nhalfLambert brightMask 说明 \u0026lt; _bright 0 暗面 ≥ _bright 1 亮面 用lerp遮罩ramp和halfLambert。 1 2 float3 shadowRamp = lerp(ramp, halfLambert, brightMask); //遮罩亮面 return shadowRamp; //输出结果 brightMask 使用结果 0（暗面） ramp 1（亮面） halfLambert 其实就是二分亮（固有色）-暗面。\nramp的采样就结束了，最后将lerp的结果乘以baseColor就可以了.\n1 2 3 4 float3 Body(float NdotL, float NdotH, float NdotV, float4 lightmap, float3 baseColor, float3 nDirVS){ float3 ramp = shadow_ramp(lightmap, NdotL) ; //ramp float3 diffsue = baseColor * ramp ; //漫反射 } 6）高光 高光的计算相对简单，高光使用Blinn-Phong光照模型：\n1 2 3 4 5 6 7 8 9 10 11 12 float3 Spec(float NdotL, float NdotH, float3 nDirVS, float4 lightmap, float3 baseColor) { float blinnPhong = pow(max(0.0, NdotH), _gloss); //Blinn-Phong float3 specular = blinnPhong * lightmap.r * _glossStrength; //高光强度 specular = specular * lightmap.b; //混合高光细节 specular = baseColor * specular; //叠加固有色 lightmap.g = smoothstep(0.2, 0.3, lightmap.g); //lightmap.g float halfLambert = smoothstep(0.0, _grey, NdotL + _dark) * lightmap.g; //半Lambert float brightMask = step(_bright, halfLambert); //亮面 specular = specular * brightMask; //遮罩暗面 return specular; //输出结果 } lightmap的r通道存放了模型的高光强度（值越白(1.0)的区域，高光越强；），b通道存放了模型的高光细节（形状）。\n1）首先，通过经典的Blinn-Phong模型计算出一个基础的高光强度 blinnPhong，\n1 float blinnPhong = pow(max(0.0, NdotH), _gloss); 2）用Lightmap的R通道 lightmap.r高光强度 * 面板开放变量的 _glossStrength高光强度来调制这个高光。\n1 float3 specular = blinnPhong * lightmap.r * _glossStrength; 3）最后再乘以lightmap.b，得到高光形状。\n1 specular = specular * lightmap.b; //混合高光细节 原神中角色处于暗部的部分是没有高光的，所以我们还要step一个halfLambert来遮罩亮暗部分。halfLambert的处理方式和漫反射的一样。 1 2 3 4 5 lightmap.g = smoothstep(0.2, 0.3, lightmap.g); //lightmap.g，计算环境遮蔽AO float halfLambert = smoothstep(0.0, _grey, NdotL + _dark) * lightmap.g; //半Lambert float brightMask = step(_bright, halfLambert); //亮面 specular = specular * brightMask; //遮罩暗面 return specular; //输出结果 在亮部区域：半兰伯特的计算结果本身较亮，乘以 lightmap.g后，开阔区域（AO值为白色）保持明亮，而褶皱缝隙处（AO值为黑色或深色）则会变暗，从而自然地增添了细节。\n在暗部区域：半兰伯特的计算结果本身较暗，再乘以 lightmap.g的深色值，会使这些区域（如腋下、衣褶）变得更暗，强化了闭塞阴影，让角色不会显得轻飘，而是牢牢地“锚定”在场景中。\n7）金属 金属部分用的是MatCap采样方法。\n1 2 3 4 5 6 7 8 9 //金属 float3 Metal(float3 nDirVS, float4 lightmap, float3 baseColor){ float metalMask = 1 - step(lightmap.r, 0.9); //金属遮罩，即step(,0.9,lightmap.r)，一般不推荐取反的写法，这里标记出来纯属为了之后读到类似代码能有反应的能力。 //采样metalMap float3 metalMap = SAMPLE_TEXTURE2D(_metalMap, sampler_metalMap, nDirVS.rg * 0.5 + 0.5).r; metalMap = lerp(_metalMapColor, baseColor, metalMap); //金属反射颜色 metalMap = lerp(0.0, metalMap, metalMask); //遮罩非金属区域 return metalMap; //输出结果 } 1 float3 metalMap = SAMPLE_TEXTURE2D(_metalMap, sampler_metalMap, nDirVS.rg * 0.5 + 0.5).r; 为什么用nDirVS.rg * 0.5 + 0.5 采样，而不是i.uv0？ _metalMap 在这里的角色是 MatCap / Reflection LUT， 而不是一张“表面贴图”。\ni.uv0的适用场景\nalbedo normal roughness AO 手绘纹理 *一般是随着建模过程中，由建模师/画师绘制好的贴图。\nnDirVS.rg * 0.5 + 0.5 这是基于法线方向的环境反射 MatCap 技法。\n一张图片只有：\n横向（U） 纵向（V） 为什么选 .rg作为UV？\nx、y 刚好就是“屏幕平面方向”\nx：左右 y：上下 而z的含义： z（b）—— 朝不朝你\nz = +1：正对摄像机 z = 0：侧着 z = -1：背对摄像机 nDirVS → UV → MatCap\n1 2 3 4 //采样metalMap float3 metalMap = SAMPLE_TEXTURE2D(_metalMap, sampler_metalMap, nDirVS.rg * 0.5 + 0.5).r; metalMap = lerp(_metalMapColor, baseColor, metalMap); //金属反射颜色 metalMap = lerp(0.0, metalMap, metalMask); //遮罩非金属区域 为什么metalMap要只取.r的单通道？ 因为这张 metalMap 在这里的是“权重 / 强度 ”，而不是“颜色”， 所以只需要一个标量通道就够了。\n注意： 👉 你并没有直接把 metalMap 当颜色输出 👉 而是马上拿它去做 lerp\n1 metalMap = lerp(_metalMapColor, baseColor, metalMap); 也就是说：\n_metalMapColor：颜色阈值1 baseColor：颜色阈值2 metalMap：混合权重 混合权重在图形学里永远是“单通道”的。 8）菲涅尔（边缘光） 原神中用的是屏幕深度边缘光，这里用菲涅耳实现的效果。代码非常简单，就是简单的菲涅尔用step卡出硬边，再乘baseColor即可。\n1 2 3 4 5 6 //边缘光 float3 edgeLight(float NdotV, float3 baseColor){ float3 fresnel = pow(1 - NdotV, _fresnel); //菲涅尔范围 fresnel = step(0.5, fresnel) * _edgeLight * baseColor; //边缘光强度 return fresnel; //输出结果 } 问题：\n模型内部转折也会亮 鼻梁、衣服褶皱会出边 基于屏幕深度空间的边缘光 1 2 3 float2 screenUV = i.screenPos.xy / i.screenPos.w; // 获取屏幕UV float rawDepth = SAMPLE_DEPTH_TEXTURE(_CameraDepthTexture, sampler_CameraDepthTexture, screenUV); float sceneDepth = Linear01Depth(rawDepth, _ZBufferParams); // 转换为线性01深度 9）自发光 1 2 3 4 5 6 //自发光 float3 light(float3 baseColor, float diffsueA){ diffsueA = smoothstep(0.0, 1.0, diffsueA); //去除噪点 float3 glow = lerp(0.0, baseColor * ((sin(_Time.w * _flicker) * 0.5 + 0.5) * _glow), diffsueA); //自发光 return glow; //输出结果 } diffuse的a通道存放的是自发光遮罩(也有可能是透明遮罩)。这个遮罩有很多奇怪的噪点，smoothstep一下把噪点去除。\n10）最终混合diffsue + metal + specular + fresnel 1 2 3 4 5 6 7 8 9 10 11 12 //身体 float3 Body(float NdotL, float NdotH, float NdotV, float4 lightmap, float3 baseColor, float3 nDirVS){ float3 ramp = shadow_ramp(lightmap, NdotL) ; //ramp float3 specular = Spec(NdotL, NdotH, nDirVS, lightmap, baseColor); //高光 float3 metal = Metal(nDirVS, lightmap, baseColor); //金属 float3 diffsue = baseColor * ramp ; //漫反射 diffsue = diffsue * step(lightmap.r, 0.9); //遮罩金属区域 float3 fresnel = edgeLight(NdotV, baseColor); //边缘光 //混合最终结果 float3 body = diffsue + metal + specular + fresnel; return body; //输出结果 } 11）脸部 原神用的SDF的方法来区分脸部明暗面，这是脸部的SDF图：\n(2 封私信 / 36 条消息) 神作面部阴影渲染还原 - 知乎\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 //脸部 float3 Face(float3 lDirWS, float3 baseColor, float2 uv, float2 screenUV){ lDirWS = -lDirWS; //采样贴图 float SDF = SAMPLE_TEXTURE2D(_lightmap, sampler_lightmap, uv).r; //采样SDF float SDF2 = SAMPLE_TEXTURE2D(_lightmap, sampler_lightmap, float2(1-uv.x, uv.y)).r; //翻转x轴采样SDF //计算向量 float3 up = float3(0,1,0); //上方向 float3 front = unity_ObjectToWorld._13_23_33; //角色前朝向 float3 left = cross(front, up); //左侧朝向 float3 right = -cross(front, up); //右侧朝向 //点乘向量 float frontL = dot(normalize(front.xz), normalize(lDirWS.xz)); //前点乘光 float leftL = dot(normalize(left.xz), normalize(lDirWS.xz)); //左点乘光 float rightL = dot(normalize(right.xz), normalize(lDirWS.xz)); //右点乘光 //计算阴影 float lightAttenuation = (frontL \u0026gt; 0) * min((SDF \u0026gt; leftL), 1 - (SDF2 \u0026lt; rightL)); //判断白天与夜晚 float rampSampling = 0.0; if(_dayAndNight == 0){rampSampling = 0.5;} //计算V轴 float rampV = _lightmapA4 * -0.1 + 1.05 - rampSampling; //0.85 //采样ramp float3 rampColor = SAMPLE_TEXTURE2D(_ramp, sampler_ramp, float2(lightAttenuation, rampV)); //混合baseColor float3 face = lerp(baseColor * rampColor, baseColor, lightAttenuation); return face; //输出结果 } 我发现直接把SDF反向得到的另半边的SDF的结果是错误的，所以翻转x轴再采样了一遍SDF图。\n用if来判断:\n1 2 3 4 5 6 7 float3 col = float3(0.0, 0.0, 0.0); //主体渲染 if(_genshinShader == 0.0){ //身体 col = Body(NdotL, NdotH, NdotV, lightmap, baseColor, nDirVS,screenUV); }else if(_genshinShader == 1.0){ //脸部 col = Face(lDirWS, baseColor, i.uv0,screenUV); } 12) 添加自发光 diffuse的a通道，可能是自发光，也有可能是透明蒙版，我们需要开放一个参数给美术选择，当是自发光的时候，就把输出结果加上自发光效果，为透明蒙版的时候就按透明蒙版裁剪掉透明的像素。\n1 2 3 4 5 6 7 8 //计算diffuse.a if(_diffuseA == 2){ //自发光 float3 diffA = light(col, diffuseA); col = col + diffA; }else if(_diffuseA == 1){ //裁剪 diffuseA = smoothstep(0.05, 0.7, diffuseA); //去除噪点 clip(diffuseA - _Cutoff); } 4 描边 轮廓线实现方案：背面膨胀法。很常见。\n在顶点着色器中，将模型的顶点沿着其法线方向向外轻微扩张。由于这个Pass只渲染模型的背面（通过Cull Front设置），扩张后的背面就会形成一个包裹在模型外的“壳”。\n1）（debug）查看模型中存储的平滑法线。\n美术人员可能会在UV2中存预烘焙的平滑法线，用于解决低多边形模型在轮廓线生成时的锯齿问题。我们可以假设模型里有这个平滑法线信息，输出来看看咸淡。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 struct a2v { …… float2 packSmoothNormal : TEXCOORD2; }; struct v2f { …… float3 normalWS : TEXCOORD2; }; v2f BackFaceOutlineVertex(a2v input) { v2f o; …… float3 smoothTS = UnpackNormalOctQuadEncode(input.packSmoothNormal); o.normalWS = smoothTS; …… return o; } half4 frag(v2f i, FRONT_FACE_TYPE isFrontFace : FRONT_FACE_SEMANTIC) : SV_Target { float3 normalData = i.normalWS; // 使用解码函数后的平滑法线 float3 debugColor = (normalData + 1.0) * 0.5; return half4(debugColor, 1.0); // Alpha通道设为1，不透明 } 杜林模型在前发/眼睛/口腔内外部/翅膀边缘有预烘焙的平滑法线信息。这些地方的法线会特殊处理。这里也查看了一下模型的顶点颜色——顶点色是单一的，这里暂且认为顶点色不存放额外信息。\n后记：其实在虚幻里很明显可以看出口眼鼻有平滑法线。\n正式开工：\n2）剔除表面\n1 2 3 4 5 6 Pass { Tags { \u0026#34;LightMode\u0026#34; = \u0026#34;outline\u0026#34; } Cull Front ZWrite On shader_feature_local：本地着色器变体（Shader Variants）。表示这些特性 只在当前 Pass 生效，不会全局生效\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 HLSLPROGRAM #pragma vertex BackFaceOutlineVertex #pragma fragment BackFaceOutlineFragment //是否开启自定义描边颜色。 #pragma shader_feature_local _OUTLINE_CUSTOM_COLOR_ON //描边根据 切线方向 计算。 #pragma shader_feature_local _OUTLINENORMALCHANNEL_TANGENT //描边根据 第二 UV 通道（平滑法线，模型中的嘴/眼/翅膀部分） 计算。 #pragma shader_feature_local _OUTLINENORMALCHANNEL_UV2 #include \u0026#34;Packages/com.unity.render-pipelines.universal/ShaderLibrary/Core.hlsl\u0026#34; #include \u0026#34;Packages/com.unity.render-pipelines.universal/ShaderLibrary/Lighting.hlsl\u0026#34; TEXTURE2D(_ilmTex); SAMPLER(sampler_ilmTex); 定义顶点输入和输出结构体\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 struct a2v { float4 positionOS : POSITION; float4 color : COLOR; float3 normalOS : NORMAL; float4 tangentOS : TANGENT; float2 uv1 : TEXCOORD0; float2 uv2 : TEXCOORD1; float2 packSmoothNormal : TEXCOORD2; }; struct v2f { float4 positionCS : SV_POSITION; float2 uv : TEXCOORD0; float4 color : COLOR; float3 positionWS : TEXCOORD1; float3 normalWS : TEXCOORD2; }; 3）通过 materialID函数，将ILM贴图的a通道的值（0到1之间）映射为不同的材质ID【0到4】，然后根据这个ID从预设的颜色数组中选择对应的轮廓颜色。这使得模型的不同区域可以根据这张贴图的定义，呈现出5种不同颜色的轮廓线。\n1 2 3 4 5 6 7 8 float materialID(float mask) { if (mask \u0026lt; 0.2) return 0; if (mask \u0026lt; 0.4) return 1; if (mask \u0026lt; 0.6) return 2; if (mask \u0026lt; 0.8) return 3; return 4; } 4）看顶点着色器 BackFaceOutlineVertex 中：\n1）获取用于顶点扩张的世界空间法线方向：通过着色器变体（#pragma shader_feature_local）提供了三种法线来源选择，以应对不同模型质量和效果。\nOUTLINENORMALCHANNEL_运行时只启用其中一个宏，避免 if-else 运行开销。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 float3 GetSmoothNormalWS(a2v input) { //描边根据默认法线通道 float3 smoothNormalOS = input.normalOS; //描边根据 切线方向 计算。 #if defined(_OUTLINENORMALCHANNEL_TANGENT) smoothNormalOS = input.tangentOS.xyz; //描边根据 UV2 通道（平滑法线，模型中的嘴/眼/翅膀部分） 计算。 #elif defined(_OUTLINENORMALCHANNEL_UV2) //TNB矩阵：切线空间转换到模型空间变换矩阵 float3 n = normalize(input.normalOS); float3 t = normalize(input.tangentOS.xyz); float3 b = cross(n, t) * input.tangentOS.w; //解码切线空间法线 float3 smoothTS = UnpackNormalOctQuadEncode(input.packSmoothNormal); smoothNormalOS = mul(smoothTS, float3x3(t, b, n)); smoothNormalOS = smoothTS; #endif return TransformObjectToWorldNormal(smoothNormalOS); } input.packSmoothNormal是存储在UV2中的美术人员预先烘焙的平滑法线。\nUnpackNormalOctQuadEncode是一个解码函数，将压缩存储的二维数据还原为三维法线向量（通常范围是[-1, 1]）。这种编码方式可以高效地在纹理通道中存储三维方向信息\n输出：smoothTS是在切线空间中的平滑法线向量。\n描边宽度：\n基于距离的动态轮廓宽度，解决轮廓线“近粗远细”或“远距离过粗”的问题。\n1 2 3 4 5 6 float GetOutlineWidth(float viewZ) { float fovFactor = 2.414 / UNITY_MATRIX_P[1].y;// 根据视野(FOV)进行校正 float z = abs(viewZ * fovFactor); return 0.01 * _OutlineWidth * _OutlineScale * saturate(1.0 / z); } 轮廓位置：这是顶点扩张的核心函数，负责将顶点位置偏移到轮廓位置。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 float4 GetOutlinePosition(VertexPositionInputs posInput, float3 normalWS, float alpha) { float width = GetOutlineWidth(posInput.positionVS.z) * alpha; // 将世界法线转换到视角空间，并忽略Z分量，确保扩张在屏幕平面进行 float3 normalVS = TransformWorldToViewNormal(normalWS); normalVS = normalize(float3(normalVS.xy, 0)); float3 posVS = posInput.positionVS; posVS += width * normalVS; //处理深度冲突(Z-fighting)：将顶点轻微推向相机 posVS += 0.01 * _OutlineZOffset * normalize(posVS); //将视角空间位置转换回裁剪空间 return TransformWViewToHClip(posVS); } 顶点着色器：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 v2f BackFaceOutlineVertex(a2v input) { v2f o; VertexPositionInputs posInput = GetVertexPositionInputs(input.positionOS.xyz); float3 smoothNormalWS = GetSmoothNormalWS(input); o.positionCS = GetOutlinePosition(posInput, smoothNormalWS, input.color.a); o.uv = input.uv1; o.color = input.color; o.positionWS = TransformObjectToWorld(input.positionOS.xyz); o.normalWS = smoothNormalWS; return o; } 片元着色器 (BackFaceOutlineFragment)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 half4 BackFaceOutlineFragment(v2f i, FRONT_FACE_TYPE isFrontFace : FRONT_FACE_SEMANTIC) : SV_Target { // 1. 采样纹理，获取材质ID掩码 half mask = SAMPLE_TEXTURE2D(_ilmTex, sampler_ilmTex, i.uv).a; float id = materialID(mask); // 将掩码值映射为0-4的整数ID // 2. 根据ID从预设颜色数组中选取轮廓色 int idx = (int)clamp(id, 0.0, 4.0); float3 color = outlineColors[idx]; // 3. 检查是否启用自定义轮廓色覆盖 #if defined(_OUTLINE_CUSTOM_COLOR_ON) color = _CustomOutlineCol.rgb; #endif clip(_Alpha - _AlphaClip); return half4(color, 1); } 5 投影 使用\u0026quot;ShadowCaster\u0026quot;Pass来产生投影。\n1 UsePass \u0026#34;Universal Render Pipeline/Lit/ShadowCaster\u0026#34; 6 后处理 后处理我使用了三个，分别是Bloom(泛光)、ToneMapping(色调映射)、ColorAdjustments(颜色调整)。\n我使用的是URP自带的后处理体积，好处是不用写代码了，和UE的后处理盒子类似，坏处是URP的后处理是全屏效果的，实际后处理的效果应该只影响角色。那样就要用C#脚本传递来制作后处理，用RendererFeatures的LayerMask来控制影响的对象图层，具体实现原理可以参考这篇文章。\n附录： 一些常见问题：\n报错：INVALID UTF8 STRING\nVisual Studio 设置默认编码格式为 UTF-8 或 GB2312-80 Visual Studio 设置默认编码格式为 UTF-8 或 GB2312-80 与文件没有高级保存选项怎么显示_visual studio 不使用简体中文gb2312编码加载文件-CSDN博客\n怎么去除边缘锯齿化 环境遮蔽AO lightmap.g通道存放的是环境遮蔽信息。它是一张由美术师预先绘制或软件生成的灰度图，用于标识模型表面哪些区域不容易接收到间接光照。 例如，衣服的褶皱深处、手臂与身体之间的缝隙、颈部下方等，这些地方即使在白天，也会比开阔的表面更暗。lightmap.g会在这些地方显示为黑色或深灰色。\nsmoothstep 三次平滑函数 1 smoothstep(edge0, edge1, x) 基本形式 Smoothstep₁(t) = t²(3 - 2t)。它用于在两个值之间生成一条平滑的S形过渡曲线。\n工作原理：\nSmoothstep(edge0, edge1, x)在三个区间内的返回值规则是\n1 2 3 4 5 x \u0026lt;= edge0 时，返回 0 x \u0026gt;= edge1 时，返回 1 edge0 \u0026lt; x \u0026lt; edge1 时，中间区域，返回在 0 和 1 之间使用埃尔米特插值得到的平滑过渡。 其中：中间区域通过三次多项式 t²(3 - 2t)产生平滑过渡。这种S形曲线在起点和终点的斜率接近零，过渡非常自然。\nsmoothstep-三次平滑函数 | Desmos\n这里可以拉参数看变化来直观感受这个三次平滑函数。\n举例计算 lightmap.g = 0.25 时的输出 我们来具体计算 smoothstep(0.2, 0.3, 0.25)的值。计算过程完全遵循其数学定义：\n缩放与钳位：首先将 x映射到 0 到 1 的区间。\n(0.25 - 0.2) / (0.3 - 0.2) = 0.05 / 0.1 = 0.5\n三次多项式插值：然后对结果进行平滑插值。\n0.5 * 0.5 * (3 - 2 * 0.5) = 0.25 * (3 - 1) = 0.25 * 2 = 0.5\n所以，当 lightmap.g的值为 0.25 时，smoothstep(0.2, 0.3, lightmap.g)的输出是 0.5。\n","date":"2025-12-11T20:32:30Z","image":"https://Selaphina.github.io/p/%E4%BB%BF%E5%8E%9F%E7%A5%9E%E6%B8%B2%E6%9F%93/CUC%E5%85%91%E6%8D%A2%E7%82%B92_hu_aee3a9923aa263e8.png","permalink":"https://Selaphina.github.io/p/%E4%BB%BF%E5%8E%9F%E7%A5%9E%E6%B8%B2%E6%9F%93/","title":"仿原神渲染"},{"content":"1 关于Tex Mapping 1.1 albedo和diffuse的区别与联系 Albedo：反照率\ndiffuse：漫反射\n它们都用于定义物体表面的基础颜色。下面是它们的核心区别。\n特性维度 Albedo (反照率) 贴图 Diffuse (漫反射) 贴图 核心定义 表面本身不包含任何光照信息，只包含物体固有的颜色和反射特性。 表面包含简单光影信息：物体固有色、简单的光照阴影、环境遮蔽等 所属工作流 基于物理的渲染（PBR）工作流 传统/非基于物理的渲染（Non-PBR）工作流 设计目的 将材质与光照解耦，确保在不同的光照环境下物理正确。 对某光照环境的【预计算】，提前将阴影等光影信息“烘焙”进去。 金属表现 金属区域通常为深色甚至纯黑，因为纯金属几乎无漫反射 不严格区分金属与非金属的漫反射表现 视觉对比 色调通常更平缓，对比度较低 因包含光影，通常对比度更明显 常见引擎 Unity (称 Albedo), Unreal Engine (称 Base Color) 旧版引擎或传统Lambert着色器 总而言之，它们的区别实际上标志着渲染技术的进步：\n传统的/非真实感渲染：\n通过将光影信息 “烘培” 进颜色贴图的Diffuse方式，纯粹使用贴图而不考虑光照效果来达到物体阴影表达的目的。\n基于PBR的渲染：\n从传统的Diffuse方式，演进到基于物理的、纯粹的材质属性（Albedo）与复杂动态光照计算分离的PBR方式，一般需要与金属度/粗糙度，高光/光泽度等贴图配合使用。\n追问：如何将传统的Diffuse贴图转换为PBR工作流的Albedo贴图？ 核心手动处理思路\n手动调整主要依赖于像Photoshop这样的图像处理软件，其核心步骤包括：\n去除光照信息：仔细识别并减弱或移除Diffuse贴图中的阴影、高光和污迹。可以使用色阶、曲线等工具调整整体对比度，或用克隆图章、修复画笔工具处理掉明显的光影过渡。 校准基础颜色：确保颜色值符合PBR的物理规律。对于非金属（绝缘体），其反照率颜色通常不应过亮（sRGB值一般不建议高于240）或过暗。金属部分在Albedo贴图中应呈现为深色。 分离与重构：有时需要将Diffuse贴图中的某些信息分离出来，交给PBR工作流中的其他贴图。例如，将粗糙的划痕细节从颜色信息中分离，其视觉效果可能通过提高粗糙度贴图的相应区域值来表现，而非在Albedo中保留深色划痕 1.2 什么是Metallic Map？ 在 Metallic 工作流中：\nMetallic = 0 非金属 F0 ≈ 0.04 有漫反射 Metallic = 1 金属 F0 = Albedo 几乎无漫反射 为什么通常建议Metallic为 0 或 1 ？\n在物理世界中，物质通常被分为两类：\n电介质（非金属，Metallic = 0）： 如木头、塑料、皮毛、玻璃、石头。它们具有弱反射，且反射光保留光线本身的颜色。 导体（金属，Metallic = 1）： 如金、银、铝、铜。它们具有强反射，且反射光的颜色由材质表面（Base Color）决定。 由于自然界中很少有物质处于“半金属”状态，因此在 PBR 模型中，中间值（如 0.5）往往会产生在物理上不真实的效果，看起来既不像塑料也不像金属。\n什么时候会用到中间值（0 到 1 之间）？\n虽然物理上不常见，但在以下特定场景中会使用中间值：\n过渡区域（混合材质）： 当一张贴图覆盖了金属和非金属的边缘时，为了抗锯齿或平滑过渡，边缘像素可能是中间值。 氧化或生锈： 金属表面生锈（氧化层是非金属）或被灰尘、油漆覆盖时，可以使用中间值来模拟这种物理性质的退化。 特定复合材质： 某些半导体或极其特殊的合成材料。 使用 Metallic Map\n使用贴图：\n如果一个物体既有金属部分又有塑料部分，你应该使用一张 Metallic Map（金属度贴图），用纯黑（0）表示非金属，纯白（1）表示金属。\n1.3 MatCap 是什么 MatCap（Material Capture）是一种基于视角的材质采样技术，本质是：\n用一张 2D 纹理，直接查表“相机空间法线 → 光照结果”，从而在无需实时光照计算的情况下，模拟复杂材质的明暗、金属感与高光。\n一句话总结：\nMatCap = 把“光照”和“材质响应”预烘焙进一张球面纹理，用法线方向索引它。\nMatCap 形式 通常是一张 正方形 2D 纹理\n内容看起来像一个被照亮的球\nMatcap使用的是一个【视空间（View Space/ Camera Space）法线】\n法线会随相机旋转 光照效果始终“贴着相机” 不受世界光源影响 核心数学关系 N_view = normalize( view-space normal ) N_view = (nx, ny, nz) 将法线的 x, y 分量 映射到 2D 纹理坐标：\n1 2 u = nx * 0.5 + 0.5 v = ny * 0.5 + 0.5 例：视空间法线（nDirVS）采样一张金属 MatCap 贴图，并用 lightmap.r 作为“是否为金属”的遮罩，最终生成一个“假金属反射颜色”。\n1 2 3 4 5 6 7 8 9 //用 lightmap.r 作为“是否为金属”的遮罩 float metalMask = step(0.9, lightmap.r); // Matcap采样（金属反射） float3 metalMap = SAMPLE_TEXTURE2D( _metalMap, sampler_metalMap, nDirVS.rg * 0.5 + 0.5 ).r; // 其中，UV的计算部分：nDirVS.rg * 0.5 + 0.5 1.3 菲涅尔（Fresnel）——边缘光 菲涅尔效应（Fresnel Effect） 是一个核心物理规律，用于描述光线在物体表面的反射强度随着视角角度变化的现象。\n在一些风格化渲染样例中，通常把【边缘光】直接起名为Fresnel。\n完整的菲涅尔方程依赖折射率（IOR），计算复杂。\n实时渲染通用近似：Schlick Fresnel——游戏引擎 / 实时 PBR 的标准写法： $$ F(θ)=F0​+(1−F0​)⋅(1−cosθ)5 $$ 其中：\nθ：视线与法线夹角\nF0：正视角下的反射率 $$ cos⁡θ=N⋅V\\cos\\theta = N \\cdot Vcosθ=N⋅V $$ F0 是什么？为什么它很重要\n对非金属（Dielectric）\nF0 ≈ 0.02 – 0.08 常用默认值：0.04 对金属（Metal）\nF0 由金属本身颜色决定\nF0 = Albedo\n金属 F0 近似颜色 金 (1.0, 0.71, 0.29) 铜 (0.95, 0.64, 0.54) 铝 (0.91, 0.92, 0.92) 1.4 什么是light map？ Light Map 是把“光照计算的结果”烘焙成一张或多张贴图，在运行时直接查表使用。\n它通常包含：\n直接光照（Direct Lighting） 间接光照 / 全局光照（Indirect Lighting / GI） 阴影（Shadow） 光照颜色与强度信息 用 存储空间换取实时性能。\nlight map怎么制作？ {bilibili}\n在编辑器或离线工具中：\n固定光源（Directional / Point / Spot） 固定静态物体（Static Geometry） 进行高质量光照计算（Ray / Path Tracing） 将结果写入 UV2 对应的纹理（Light Map） 采样（Runtime Sampling）\n在 Shader 中：\n第二套 UV（Lightmap UV / UV2）\n1 2 float3 bakedLight = SAMPLE_LIGHTMAP(uv2); finalColor = albedo * bakedLight; 运行时不再做光照计算，仅采样贴图。\n2 常见内置着色器 什么是URP和HDRP？ 在Unity现在的生态里，URP 和 HDRP 是基于 SRP (Scriptable Render Pipeline，可编程渲染管线) 的两套核心解决方案。\nURP 是一条极其高效、灵活的流水线，旨在保证画得快，同时质量不错。 灵活多变（非强写实）： URP 并不强求“照片级写实”，它更擅长处理**风格化（Stylized）**的画面。比如卡通渲染（Toon Shading）、二次元风格、低多边形（Low Poly）风格。\n光照相对简化： 它的默认光照模型主要是为了性能妥协的。它通常使用单次Pass的前向渲染（Forward Rendering），对光源数量有限制（虽然后来引入了Forward+技术支持更多光源，但本质还是为了轻量）。\n应用场景 (Scenarios)\n移动端游戏（手游）： 这是URP的主战场。比如《王者荣耀》、《原神》（虽然原神魔改了管线，但底子更接近URP的思路）这类需要覆盖从低端机到高端机海量设备的游戏。 VR / AR / MR： 像Meta Quest这样的VR一体机，硬件性能相当于手机，必须用URP才能跑得动高帧率。 独立游戏 \u0026amp; 风格化游戏： 如果你的游戏像《纪念碑谷》或者任天堂的《塞尔达：旷野之息》那种画风，URP是首选。 任天堂Switch游戏： 硬件性能受限，URP非常合适。 HDRP 是一条顶级精密的流水线，旨在画得极度逼真，不惜工本。 照片级写实（Photorealistic）： HDRP 的目标就是模拟真实世界。它遵循严格的 PBR（基于物理的渲染）法则。\n高级光影与材质：\n光照： 默认支持体积光（Volumetric Lighting，那种空气中的丁达尔效应）、物理单位的光照强度（勒克斯、流明）。 材质： 支持次表面散射（SSS，比如皮肤通透的效果）、清漆（Clear Coat，比如车漆）、各向异性（头发、拉丝金属）。 厚重与电影感： 默认开启大量高质量的后期处理，如高质量的景深、动态模糊、环境光遮蔽（GTAO/RTAO），画面看起来很有“3A大作”的质感。\n应用场景\n3A主机/PC游戏： 目标平台是 PS5, Xbox Series X, 高端PC。比如《使命召唤》级别的画质，或者充满赛博朋克霓虹灯的场景。 建筑可视化 \u0026amp; 汽车展示： 房地产漫游、汽车配置器。这些场景需要极度真实的光影反射，甚至需要用到**光线追踪（Ray Tracing）**技术，只有HDRP原生支持得好。 影视动画制作： 现在很多动画片用Unity制作，为了接近离线渲染器（如Arnold, V-Ray）的效果，必须用HDRP。 数字孪生： 模拟真实的工厂、城市光照。 为什么 PBR 离不开 HDR？ 核心原因：为了正确的“对比度”和“能量感”。\n举个经典的**“太阳照在镜子上”**的例子：\n情况 A：PBR + LDR (普通模式) 输入： 太阳亮度被锁死在 1.0（因为 LDR 存不下更大的数）。 PBR 计算： 镜子反射了 90% 的光。于是反射光亮度 = 1.0 * 0.9 = 0.9。 结果： 镜子里的太阳是一个灰白色的圆点，甚至不如一张白纸（1.0）亮。 观感： 假！ 现实中看镜子里的太阳会亮瞎眼，但在这种模式下，它看起来像贴了一张贴纸。 情况 B：PBR + HDR (正确模式) 输入： 太阳亮度设定为真实的 65000.0 (HDR 数值)。 PBR 计算： 镜子反射了 90% 的光。于是反射光亮度 = 65000.0 * 0.9 = 58500.0。 结果： 这个像素点的亮度是 58500！这远远超过了显示器的上限（1.0）。 后期处理（Bloom）： 引擎发现这个点亮得离谱，于是会给它周围产生**辉光（Bloom）**效果，模拟人眼被强光刺激的感觉。 Bloom的计算过程：在 Unity 的 URP 或 HDRP 中，Bloom 是作为 Post-Processing（后期处理）的一部分来计算的。流程大概是这样：\n提取亮部 (Thresholding)： 引擎先扫描整张画面，问：“哪些像素特别亮？”（通常由 Threshold 阈值参数控制，比如亮度超过 1.0 的）。 模糊处理 (Blurring)： 把这些提取出来的亮斑，进行高斯模糊，让它们从“一个点”变成“一团雾”。 叠加 (Combining)： 把这团模糊的光雾，叠加回原始的画面上。 结果： 原本亮的地方，现在周围多了一圈柔和的光。\n观感： 真！ 那个点亮得刺眼，周围有一圈光晕，看起来非常有能量感。 结论： PBR 的公式是用来处理光照的，如果光照的数据（HDR）本身被“削顶”了（变成 LDR），PBR 算出来的结果也是错的。HDR 给 PBR 提供了真实的亮度阶梯。\nstandard（标准着色器） 详见unity shader入门精要18\n**含义：**最常见最通用的基于物理渲染（PBR）着色器。它模拟真实世界的光照行为，遵循能量守恒原则。\n核心特点:\n支持PBR工作流。\n追问：\n什么是能量守恒？\n一个简单的道理：一个表面反射出去的光线能量，绝对不会超过它接收到的光线能量 。\n在渲染中，这意味着一个物体不会变得“不自然地亮”，从而保证了材质在不同光照环境下都能呈现出真实、一致的外观。\n一束光打到表面后，其能量主要被分为两部分：\n漫反射：一部分光会穿透进物体内部，经过散射后，再部分地重新射出表面，形成我们看到的物体固有色（Albedo颜色）。 镜面反射：另一部分光会在物体表面直接被反射出去，形成高光（Specular）。 能量守恒的关键在于： 漫反射和镜面反射的关系是：此消彼长。\n​\t-金属度高，表面光滑的物体：镜面反射的能量多，漫反射的能量少。\n​\t-金属度低，表面磨砂的物体：镜面反射的能量少，漫反射的能量高。\n​\t-完美的镜面：非常光滑的金属，几乎所有的光被反射，几乎没有漫反射。\n简单来说，标准着色器的能量守恒原则就是确保材质不超支它获得的光线能量。这是其能够模拟真实世界材质，在不同光照条件下保持视觉正确性的基石\nTerrain（地形着色器） 含义：专门为渲染大面积地形而优化的着色器，支持多纹理混合和地形细节增强。\n核心特点：\n通常支持多层混合（如草地、泥土、岩石、雪地的平滑过渡）。可能包含视差映射、细节纹理（Detail Map） 等，近距离保持细节。针对地形网格进行了性能优化（如LOD、纹理流送）。\n典型用途：开放世界地形、自然景观、大型地表。\nToon（卡通着色器） 含义：也称为“风格化”或“非真实感渲染（NPR）”着色器，用于模拟卡通、动漫、手绘风格。\n核心特点：\n色阶化（Color Ramping）：将连续的光照过渡转换为有限的几个色块，形成卡通明暗。描边（Outline）：常通过背面膨胀、后处理等技术添加轮廓线。高光和阴影往往是艺术化控制，而非物理计算。\n典型用途：\n卡通风格游戏、动漫风格角色、低多边形艺术风格场景。\nUnlit（无光照着色器） 含义：最简单的着色器，完全不进行任何光照计算，直接输出纹理或颜色。\n核心特点：\n不受任何光源影响，颜色完全由纹理/顶点颜色决定。\n性能开销极低，渲染速度最快。\n常用于不需要光影反应的物体。\n典型用途：\nUI元素、天空盒、发光体（如自发光广告牌）。\n特效粒子（部分）、简化的低性能需求场景。\nLit着色器（Lit Shader） **含义：**在Unity的渲染管线中，Lit着色器（Lit Shader）特指那些基于物理的渲染（PBR）原理、能够模拟复杂光线与材质交互的着色器。\n物理基础：其核心是微表面理论和能量守恒定律。这意味着光线照射到表面时，反射光的总能量不会超过入射光能量（除非表面自发光），从而保证渲染的物理正确性\n两种工作流：Lit着色器通常支持两种工作流程以适应不同的制作习惯。\n金属/粗糙度工作流：这是最主流的工作流。它使用Base Map定义基础颜色和金属反射率，Metallic Map区分金属与非金属区域，Roughness Map控制表面的粗糙程度。 高光/光泽度工作流：它使用Diffuse Map、Specular Map定义高光颜色和强度，Glossiness Map控制光滑度。此工作流更灵活但更易违反能量守恒，且资源开销稍大。 高级效果：在高清渲染管线中，Lit着色器还能实现次表面散射（模拟光线穿透半透明材质如皮肤、玉石）、各向异性（模拟拉丝金属等方向性高光）等高级视觉效果\n什么是Tone Mapping（色调映射） 这是连接 HDR 和 显示器的桥梁。\n问题： 你的屏幕（手机/显示器）通常只能显示 0-1 的亮度。如果你直接把算出来的 58500 亮度输出给屏幕，屏幕只能把它当成 1（白），结果就是画面一片惨白（过曝），没有任何细节。 解决： Tone Mapping 是一种后期处理技术。它的作用是把 HDR 那个巨大的范围（0 到 10000），用一种优雅的曲线，“压”回显示器能显示的范围（0 到 1）。 它能保留暗部的细节，同时让亮部（太阳）虽然在屏幕上只显示为白，但保留了亮部的层次感，配合 Bloom 让人觉得“它很亮”。 ACES 在设置 Tone Mapping 时，你会经常看到 ACES 这个词。\nACES 是电影工业（奥斯卡级别）的标准色彩编码系统。 在 Unity 中选择 ACES Tone Mapping，能让你的 PBR + HDR 画面呈现出类似电影胶片的质感：对比度略高，色彩过度非常自然，高光处不会死白，而是会稍微偏向某种颜色（色偏），非常好看。 MSAA/FXAA 是什么? 这两个概念都属于 抗锯齿（Anti-Aliasing，简称 AA） 技术。\n游戏里会有“锯齿”？\n因为屏幕是由无数个方形的像素点组成的（像十字绣或乐高拼图）。当你试图用这些方块去拼一条斜线或者圆弧时，边缘一定会出现阶梯状的缺口，这就是锯齿（Aliasing）。\nMSAA 和 FXAA 就是用来“磨平”这些棱角的两种完全不同的流派。\nMSAA (Multi-Sample Anti-Aliasing) —— 多重采样抗锯齿 它是怎么工作的？\nMSAA 是一种传统的、暴力的方法，它主要发生在显卡光栅化（Rasterization）的阶段。\n原理： 比如你开 4x MSAA。对于物体的边缘像素，显卡不会只算一次颜色，而是会在这个像素内部再取 4 个子采样点。如果这4个点有的在三角形内，有的在三角形外，它就把这4个点的颜色进行混合（平均化）。 结果： 原本生硬的黑白交界（要么是物体，要么是背景），变成了一个过渡色。边缘瞬间变滑顺了。 ✅ 优点\n画面清晰锐利： 它只处理模型的边缘几何体，完全不影响物体内部的贴图纹理。所以画面非常干净，不会变糊。 静态动态都好： 不管相机怎么动，边缘都很稳。 ❌ 缺点\n性能开销大： 显存占用高，计算量大。特别是 8x MSAA，对显卡是巨大的考验。 挑剔渲染管线： MSAA 是 前向渲染 (Forward Rendering) 的好基友（URP 默认就是前向），但在 延迟渲染 (Deferred Rendering) 下非常难实现（HDRP 默认是延迟）。 管不了Alpha Test： 对于像草丛、树叶、铁丝网这种用透明贴图做的模型（Alpha Cutout），MSAA 往往无能为力，边缘依然会有锯齿。 FXAA (Fast Approximate Anti-Aliasing) —— 快速近似抗锯齿 核心关键词： 后期处理、极快、万金油、由于“糊”而闻名。\n⚙️ 它是怎么工作的？\nFXAA 不关心场景里有几个三角形，它是一种 后期处理 (Post-Processing) 技术。\n原理： 等显卡把整张图都画好了（此时图上已经有锯齿了），FXAA 像 PhotoShop 的滤镜一样，扫描整张图。它通过算法寻找“哪里看起来像锯齿”（比如对比度剧烈的边缘），然后直接对那些像素进行模糊处理。 通俗比喻： 就像你用美颜相机磨皮，不管脸上是痘痘还是本来就有的痣，统统抹平。 ✅ 优点\n极度便宜： 几乎不怎么吃显卡性能。几十年前的老爷机都能跑。 啥都能抗： 只要是屏幕上显示的锯齿，不管是模型边缘、还是 MSAA 搞不定的树叶、阴影边缘，它都能给你抹平。 兼容性无敌： 它是后期特效，所以不在乎你是 URP 还是 HDRP，是前向还是延迟，通吃。 ❌ 缺点\n画面变糊： 这是最大的槽点。因为它不仅模糊了锯齿，也可能把纹理细节（比如衣服的织物纹理、远处文字的清晰度）给误伤了。整个画面会有一种蒙了一层凡士林的感觉。 闪烁： 在画面高速运动时，FXAA 有时处理不过来，边缘可能会出现闪烁。 直观对比： 特性 MSAA (多重采样) FXAA (快速近似) 画质清晰度 🌟🌟🌟🌟🌟 (极高，纹理清晰) 🌟🌟🌟 (一般，整体偏软/糊) 抗锯齿能力 几何边缘完美，但对树叶/镂空无效 全屏都能抗，但精度不高 性能消耗 🔴 高 (吃显存和带宽) 🟢 极低 (几乎忽略不计) Unity URP ✅ 完美支持 (如果是Forward渲染) ✅ 支持 (作为后期特效) Unity HDRP ❌ 默认不支持 (除非切换渲染模式) ✅ 支持 适用场景 VR游戏 (必须清晰)、风格化游戏、低多边形 性能吃紧的低端机、场景极其复杂的游戏 TAA（Temporal Anti-Aliasing） 虽然 MSAA 和 FXAA 还在用，但 3A 大作（HDRP 项目）的主流霸主其实是 TAA (Temporal Anti-Aliasing，时间性抗锯齿)。\nTAA 的逻辑： 它不仅看当前这一帧，还会借用上一帧的画面信息来修补当前帧的锯齿。 地位： 它是目前画质和性能平衡得最好的方案。 副作用： 会有“鬼影”（Ghosting），就是物体移动时后面拖着残影，但现在的引擎优化得已经很好了。 给小白的建议：\n做手游/VR (URP)： 优先开 MSAA 4x。因为手机屏幕小但像素密，MSAA 效果最好且现在手机跑得动。 如果手机发烫太厉害，降到 MSAA 2x 或关掉。 尽量别在手机上用 FXAA，本来屏幕就小，一糊就没法看了。 做PC大作 (HDRP)： 首选 TAA。 如果是配置极低的“老爷机模式”，选 FXAA。 HDRP 里基本忘掉 MSAA 吧。 3 环境光遮蔽（AO） 环境光遮蔽贴图：模拟物体缝隙和角落因不易接收光线而产生的阴影，增强立体感.\n","date":"2025-12-11T10:32:30Z","image":"https://Selaphina.github.io/p/%E6%9D%90%E8%B4%A8%E7%BE%8E%E6%9C%AF-%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5%E6%A2%B3%E7%90%86%E7%AC%94%E8%AE%B0/CUC%E5%85%91%E6%8D%A2%E7%82%B92_hu_aee3a9923aa263e8.png","permalink":"https://Selaphina.github.io/p/%E6%9D%90%E8%B4%A8%E7%BE%8E%E6%9C%AF-%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5%E6%A2%B3%E7%90%86%E7%AC%94%E8%AE%B0/","title":"材质美术-基础概念梳理笔记"},{"content":"第16章 Unity中的渲染优化技术 正在阅读本书的读者，有可能是移动游戏的开发者。和PC相比，移动设备上的GPU有着完全不同的架构设计，它能使用的带宽、功能和其他资源都非常有限。这要求我们需要时刻把优化谨记在心，才可以避免等到项目完成时才发现游戏根本无法在移动设备上流畅运行的结果。\n在本章，我们将会阐述一些Unity中常见的优化技术。这些优化技术都是和渲染相关的，例如，使用批处理、LOD(LevelofDetail)技术等。在本章最后的扩展阅读部分，我们给出一些非常有价值的参考资料,在那里读者可以学习到更多真实项目中的优化技术。\n在开始学习之前，我们希望读者能够理解，游戏优化不仅是程序员的工作，更需要美工人员在游戏的美术上进行一定的权衡，例如，避免使用全屏的屏幕特效，避免使用计算复杂的shader，减少透明混合造成的overdraw等。也就是说，这是由程序员和美工人员等各个部分人员共同参与的工作。\n16.1 移动平台的特点 和PC平台相比，移动平台上的GPU架构有很大的不同。由于处理资源等条件的限制，移动设备上的GPU架构专注于尽可能使用更小的带宽和功能，也由此带来了许多和PC平台完全不同的现象。\n例如，为了尽可能移除那些隐藏的表面，减少overdraw(即一个像素被绘制多次)，PowerVR芯片(通常用于iOS设备和某些Android设备)使用了基于瓦片的延迟渲染(Tiled-based Deferred Rendering,TBDR)架构,把所有的渲染图像装入一个个瓦片(tile)中,再由硬件找到可见的片元，而只有这些可见片元才会执行片元着色器。另一些基于瓦片的GPU架构，如Adreno(高通的芯片)和Mali(ARM的芯片)则会使用Early-Z或相似的技术进行一个低精度的的深度检测,来剔除那些不需要渲染的片元。还有一些GPU，如Tegra(英伟达的芯片)，则使用了传统的架构设计，因此在这些设备上，overdraw更可能造成性能的瓶颈。\n由于这些芯片架构造成的不同，一些游戏往往需要针对不同的芯片发布不同的版本，以便对每个芯片进行更有针对性的优化。尤其是在Android平台上，不同设备使用的硬件，如图形芯片、屏幕分辨率等，大相径庭，这对图形优化提出了更高的挑战。相比与Android平台，iOS平台的硬件条件则相对统一。读者可以在Unity手册的 iOS硬件指南(http://docs.unity3d.com/Manual/iphone-Hardware.html)中找到相关的资料。\n16.2 影响性能的因素 首先,在学习如何优化之前,我们得了解影响游戏性能的因素有哪些,才能对症下药。对于一个游戏来说，它主要需要使用两种计算资源:CPU和GPU。它们会互相合作，来让我们的游戏可以在预期的帧率和分辨率下工作。其中，CPU主要负责保证帧率，GPU主要负责分辨率相关的一些处理。\n据此，我们可以把造成游戏性能瓶颈的主要原因分成以下几个方面。\n(1) CPU 过多的draw call 复杂的脚本或者物理模拟 (2) GPU 顶点处理 过多的顶点 过多的逐顶点计算 片元处理 过多的片元(既可能是由于分辨率造成的，也可能是由于overdraw造成的) 过多的逐片元计算 (3) 带宽 使用了尺寸很大且未压缩的纹理 分辨率过高的帧缓存 对于CPU来说，限制它的主要是每一帧中draw call的数目。我们曾在2.2节和2.4.3节中介绍过drawcall的相关概念和原理。简单来说，就是CPU在每次通知GPU进行渲染之前，都需要提前准备好顶点数据(如位置、法线、颜色、纹理坐标等)，然后调用一系列API把它们放到GPU可以访问到的指定位置，最后，调用一个绘制命令，来告诉GPU，“嘿，我把东西都准备好了，你赶紧出来干活(渲染)吧!”。而调用绘制命令的时候，就会产生一个draw call。过多的draw call会造成CPU的性能瓶颈，这是因为每次调用draw call时，CPU往往都需要改变很多渲染状态的设置，而这些操作是非常耗时的。如果一帧中需要的drawcall数目过多的话，就会导致CPU把大部分时间都花费在提交drawcall的工作上面了。当然，其他原因也可能造成CPU瓶颈，例如物理、布料模拟、蒙皮、粒子模拟等，这些都是计算量很大的操作，但由于本书主要讨论Shader方面的相关技术，因此，这些内容不在本书的讨论范围内。\n而对于GPU来说，它负责整个渲染流水线。它从处理CPU传递过来的模型数据开始，进行顶点着色器、片元着色器等一系列工作，最后输出屏幕上的每个像素。因此，GPU的性能瓶颈和需要处理的顶点数目、屏幕分辨率、显存等因素有关。而相关的优化策略可以从减少处理的数据规模(包括顶点数目和片元数目)、减少运算复杂度等方面入手。\n在了解了上面基本的内容后，本章后续章节会涉及的优化技术有：\n(1) CPU优化 使用批处理技术减少draw call数目 (2) GPU优化 减少需要处理的顶点数目 优化几何体 使用模型的LOD(Level of Detail)技术 使用遮挡剔除(Occlusion Culling)技术 减少需要处理的片元数目 控制绘制顺序 警惕透明物体 减少实时光照 减少计算复杂度 使用 Shader的 LOD(Level of Detail)技术 代码方面的优化 (3) 节省内存带宽 减少纹理大小 利用分辨率缩放 在开始优化之前，我们首先需要知道是哪个步骤造成了性能瓶颈。而这可以利用 Unity提供的一些渲染分析工具来实现。\n16.3 Unity中的渲染分析工具 Unity内置了一些工具，来帮助我们方便地查看和渲染相关的各个统计数据。这些数据可以帮助我们分析游戏渲染性能,从而更有针对性地进行优化。在 Unity 5中,这些工具包括了渲染统计窗口(Rendering Statistics Window)、性能分析器(Profiler),以及帧调试器(Frame Debugger)。需要注意的是，在不同的目标平台上，这些工具中显示的数据也会发生变化。\n16.3.1 认识Unity5的渲染统计窗口 Unity5提供了一个全新的窗口，即渲染统计窗口(Rendering Statistics Window)来显示当前游戏的各个渲染统计变量,我们可以通过在 Game视图右上方的菜单中单击 Stats按钮来打开它,如图 16.1所示。从图 16.1中可以看出,渲染统计窗口主要包含了 3个方面的信息:音频(Audio)、图像(Graphics)和网络(Network)。我们这里只关注第二个方面，即图像相关的渲染统计结果。\n渲染统计窗口中显示了很多重要的渲染数据，例如FPS、批处理数目、顶点和三角网格的数目等。表 16.1列出了渲染统计窗口中显示的各个信息。\n表16.1 渲染统计窗口信息\n信息名称 描述 每帧的时间和FPS 在Graphic的右侧显示,给出了处理和渲染一帧所需的时间,以及FPS数目 Batches 一帧中需要进行的批处理数目 Saved by batching 合并的批处理数目,这个数字表明了批处理为我们节省了多少draw call Tris和 Verts 需要绘制的三角面片和顶点数目 Screen 屏幕的大小,以及它占用的内存大小 SetPass 渲染使用的 Pass的数目,每个 Pass都需要 Unity的 runtime来绑定一个新的 Shader,这可能造成CPU的瓶颈 Visible Skinned Meshes 渲染的蒙皮网格的数目 Animations 播放的动画数目 Unity 5的渲染统计窗口相较于之前版本中的有了一些变化,最明显的区别之一就是去掉了draw call数目的显示，而添加了批处理数目的显示。 Batches和 Saved by batching更容易让开发者理解批处理的优化结果。当然,如果我们想要查看draw call的数目等其他更加详细的数据,可以通过Unity编辑器的性能分析器来查看。\n16.3.2 性能分析器的渲染区域 我们可以通过单击Window-\u0026gt;Profiler来打开Unity的性能分析器(Profiler)。性能分析器中的渲染区域(Rendering Area)提供了更多关于渲染的统计信息，图16.2给出了对图16.1中场景的渲染分析结果。\n性能分析器显示了绝大部分在渲染统计窗口中提供的信息，例如，绿线显示了批处理数目、蓝线显示了Pass数目等，同时还给出了许多其他非常有用的信息，例如，draw call数目、动态批处理/静态批处理的数目、渲染纹理的数目和内存占用等。\n结合渲染统计窗口和性能分析器，我们可以查看与渲染相关的绝大多数重要的数据。一个值得注意的现象是，性能分析器给出的drawcall数目和批处理数目、Pass数目并不相等，并且看起来好像要大于我们估算的数目,这是因为Unity在背后需要进行很多工作,例如,初始化各个缓存、为阴影更新深度纹理和阴影映射纹理等,因此需要花费比“预期”更多的draw call。一个好消息是，Unity5引入了一个新的工具来帮助我们查看每一个drawcall的工作，这个工具就是帧调试器。\n16.3.3 再谈帧调试器 我们已经在之前的章节中多次看到帧调试器(Frame Debugger)的应用，例如5.5.3节中解释了如何使用帧调试器来对 Shader进行调试。我们可以通过 Window-\u0026gt; Frame Debugger来打开它。在这个窗口中,我们可以清楚地看到每一个 draw call的工作和结果,如图 16.3所示。\n帧调试器的调试面板上显示了渲染这一帧所需要的所有的渲染事件,在本例中,事件数目为14,而其中包含了 10个 draw call事件(其他渲染事件多为清空缓存等)。通过单击面板上的每个事件，我们可以在Game视图查看该事件的绘制结果，同时渲染统计面板上的数据也会显示成截止到当前事件为止的各个渲染统计数据。以本例为例(场景如图16.1所示)，要渲染一帧共需要花费10个draw call，其中4个draw call用于更新深度纹理(对应UpdateDepthTexture)，4个draw call用于渲染平行光的阴影映射纹理，1个draw call用于绘制动态批处理后的3个立方体模型，1个draw call用于绘制球体。\n在Unity的渲染统计窗口、分析器和帧调试器这3个利器的帮助下，我们可以获得很多有用的优化信息。但是，很多诸如渲染时间这样的数据是基于当前的开发平台得到的，而非真机上的结果。事实上，Unity正在和硬件生产商合作，来首先让使用英伟达图睿(Tegra)的设备可以出现在Unity的性能分析器中。我们有理由相信，在后续的Unity版本中，直接在Unity中对移动设备进行性能分析不再是梦想。然而，在这个梦想实现之前，我们仍然需要一些外部的性能分析工具的帮助。\n16.3.4 其他性能分析工具 对于移动平台上的游戏来说，我们更希望得到在真机上运行游戏时的性能数据。这时，Unity目前提供的各个工具可能就不再能满足我们的需求了。\n对于Android平台来说，高通的Adreno分析工具可以对不同的测试机进行详细的性能分析。英伟达提供了NVPerfHUD工具来帮助我们得到几乎所有需要的性能分析数据，例如，每个draw call的GPU时间，每个shader花费的cycle数目等。\n对于iOS平台来说，Unity内置的分析器可以得到整个场景花费的GPU时间。PowerVRAM的 PVRUniSCo shader分析器也可以给出一个大致的性能评估。Xcode中的 OpenGL ES Driver Instruments可以给出一些宏观上的性能信息，例如，设备利用率、渲染器利用率等。但相对于Android平台，对iOS的性能分析更加困难(工具较少)。而且PowerVR芯片采用了基于瓦片的延迟渲染器，因此，想要得到每个drawcall花费的GPU时间是几乎不可能的。这时，一些宏观上的统计数据可能更有参考价值。\n一些其他的性能分析工具可以在Unity的官方手册(http://docs.unity3d.com/Manual/MobileProfiling.html)中找到。当找到了性能瓶颈后，我们就可以针对这些方面进行特定的优化。\n16.4 减少 draw call数目 读者最常看到的优化技术大概就是批处理(batching)了。批处理的实现原理就是为了减少每一帧需要的draw call数目。为了把一个对象渲染到屏幕上，CPU需要检查哪些光源影响了该物体，绑定shader并设置它的参数，再把渲染命令发送给GPU。当场景中包含了大量对象时，这些操作就会非常耗时。一个极端的例子是，如果我们需要渲染一千个三角形，把它们按一千个单独的网格进行渲染所花费的时间要远远大于渲染一个包含了一千个三角形的网格。在这两种情况下，GPU的性能消耗其实并没有多大的区别，但CPU的draw call数目就会成为性能瓶颈。因此，批处理的思想很简单，就是在每次面对drawcall时尽可能多地处理多个物体。我们已经在2.2节和2.4.3节中详细地讲述了drawcall和批处理之间的联系，本节旨在介绍如何在Unity中利用批处理技术来优化渲染。\n那么，什么样的物体可以一起处理呢?答案就是使用同一个材质的物体。这是因为，对于使用同一个材质的物体，它们之间的不同仅仅在于顶点数据的差别。我们可以把这些顶点数据合并在一起，再一起发送给GPU，就可以完成一次批处理。\nUnity中支持两种批处理方式:一种是动态批处理，另一种是静态批处理。对于动态批处理来说，优点是一切处理都是Unity自动完成的，不需要我们自己做任何操作，而且物体是可以移动的，但缺点是，限制很多，可能一不小心就会破坏了这种机制，导致Unity无法动态批处理一些使用了相同材质的物体。而对于静态批处理来说，它的优点是自由度很高，限制很少;但缺点是可能会占用更多的内存，而且经过静态批处理后的所有物体都不可以再移动了(即便在脚本中尝试改变物体的位置也是无效的)。\n16.4.1 动态批处理 如果场景中有一些模型共享了同一个材质并满足一些条件，Unity就可以自动把它们进行批处理，从而只需要花费一个drawcall就可以渲染所有的模型。动态批处理的基本原理是，每一帧把可以进行批处理的模型网格进行合并，再把合并后模型数据传递给GPU，然后使用同一个材质对其渲染。除了实现方便，动态批处理的另一个好处是，经过批处理的物体仍然可以移动，这是由于在处理每帧时Unity都会重新合并一次网格。\n虽然Unity的动态批处理不需要我们进行任何额外工作，但只有满足条件的模型和材质才可以被动态批处理。需要注意的是，随着Unity版本的变化，这些条件也有一些改变。在本节中，我们给出一些主要的条件限制。\n能够进行动态批处理的网格的顶点属性规模要小于900。例如，如果shader中需要使用顶点位置、法线和纹理坐标这3个顶点属性，那么要想让模型能够被动态批处理，它的顶点数目不能超过300。需要注意的是，这个数字在未来有可能会发生变化，因此不要依赖这个数据。 一般来说，所有对象都需要使用同一个缩放尺度(可以是(1,1,1)、(1,2,3)、(1.5,1.4,1.3)等，但必须都一样)。一个例外情况是，如果所有的物体都使用了不同的非统一缩放，那么它们也是可以被动态批处理的。但在Unity5中，这种对模型缩放的限制已经不存在了。 使用光照纹理(lightmap)的物体需要小心处理。这些物体需要额外的渲染参数，例如，在光照纹理上的索引、偏移量和缩放信息等。因此，为了让这些物体可以被动态批处理，我们需要保证它们指向光照纹理中的同一个位置。 多Pass的 shader会中断批处理。在前向渲染中，我们有时需要使用额外的Pass来为模型添加更多的光照效果，但这样一来模型就不会被动态批处理了。 在本书资源的 Scene_16_3_1场景中，我们给出了这样一个场景。场景中包含了3个立方体，它们使用同一个材质，同时还包含了一个使用其他材质的球体。场景中还包含了一个平行光，但我们关闭了它的阴影效果，以避免阴影计算对批处理数目的影响。这样一个场景的渲染统计数据如图16.4所示。\n从图16.4中可以看出，要渲染这样一个包含了4个物体的场景共需要两个批处理。其中，一个批处理用于绘制经过动态批处理合并后的3个立方体网格，另一个批处理用于绘制球体。我们可以从Save by batching看出批处理帮我们节省了两个draw call。\n现在，我们再向场景中添加一个点光源，并调整它的位置使它可以照亮场景中的4个物体。由于场景中的物体都使用了多个Pass的shader，因此，点光源会对它们产生光照影响。图16.5给出了添加点光源后的渲染统计数据。\n从图 16.5中可以看出,渲染一帧所需的批处理数目增大到了 8,而 Save by batching的数目也变成了 0。这是因为,使用了多个 Pass的 shader在需要应用多个光照的情况下,破坏了动态批处理的机制，导致Unity不能对这些物体进行动态批处理。而由于平行光和点光源需要对4个物体分别产生影响,因此,需要24个批处理操作。需要注意的是,只有物体在点光源的影响范围内，Unity才会调用额外的Pass来处理它。因此，如果场景中点光源距离物体很远，那么它们仍然会被动态批处理的。\n动态批处理的限制条件比较多，例如很多时候，我们的模型数据往往会超过900的顶点属性限制。这种时候依赖动态批处理来减少 draw call显然已经不能够满足我们的需求了。这时，我们可以使用 Unity的静态批处理技术。\n16.4.2 静态批处理 Unity提供了另一种批处理方式，即静态批处理。相比于动态批处理来说，静态批处理适用于任何大小的几何模型。它的实现原理是,只在运行开始阶段,把需要进行静态批处理的模型合并到一个新的网格结构中,这意味着这些模型不可以在运行时刻被移动。但由于它只需要进行一次合并操作，因此，比动态批处理更加高效。静态批处理的另一个缺点在于，它往往需要占用更多的内存来存储合并后的几何结构。这是因为,如果在静态批处理前一些物体共享了相同的网格,那么在内存中每一个物体都会对应一个该网格的复制品，即一个网格会变成多个网格再发送给GPU。如果这类使用同一网格的对象很多，那么这就会成为一个性能瓶颈了。例如，如果在一个使用了1000个相同树模型的森林中使用静态批处理，那么，就会多使用1000倍的内存，这会造成严重的内存影响。这种时候，解决方法要么忍受这种牺牲内存换取性能的方法，要么不要使用静态批处理，而使用动态批处理技术(但要小心控制模型的顶点属性数目),或者自己编写批处理的方法。\n在本书资源的 Scene_16_3_2场景中,我们给出了一个测试静态批处理的场景。场景中包含了3个Teapot模型，它们使用同一个材质，同时还包含了一个使用不同材质的立方体。场景中还包含了一个平行光,但我们关闭了它的阴影效果,以避免阴影计算对批处理数目的影响。在运行前,这样一个场景的渲染统计数据如图 16.6所示。\n从图16.6中可以看出，尽管3个Teapot模型使用了相同的材质，但它们仍然没有被动态批处理。这是因为,Teapot模型包含的顶点数目是393,而它们使用的 shader中需要使用4个顶点属性(顶点位置、法线方向、切线方向和纹理坐标)，超过了动态批处理中限定的900限制。此时，要想减少 draw call就需要使用静态批处理。\n静态批处理的实现非常简单，只需要把物体面板上的 Static复选框勾选上即可(实际上我们只需要勾选Batching Static即可)，如图16.7所示。\n这时，我们再观察渲染统计窗口中的批处理数目，还是没有变化。但是不要急，运行程序后，变化就出现了，如图 16.8所示。\n从图16.2中可以看出，现在的批处理数目变成了2，而Save by batching数目也显示为2。此时，如果我们在运行时查看每个模型使用的网格，会发现它们都变成了一个名为 Combined Mesh(roo:scene)的东西，如图16.9所示。这个网格是Unity合并了所有被标识为“Static”的物体的结果，在我们的例子里，就是3个Teapot和一个立方体。读者可能会有一个疑问，这4个对象明明不是都使用了一个材质，为什么可以合并成一个呢?如果你仔细观看图16.9的结果，会发现在图16.9的右下方标明了“4 submeshes”，也就是说，这个合并后的网格其实包含了4个子网格，即场景中的4个对象。对于合并后的网格，Unity会判断其中使用同一个材质的子网格，然后对它们进行批处理。\n在内部实现上，Unity首先把这些静态物体变换到世界空间下，然后为它们构建一个更大的顶点和索引缓存。对于使用了同一材质的物体，Unity只需要调用一个draw call就可以绘制全部物体。而对于使用了不同材质的物体，静态批处理同样可以提升渲染性能。尽管这些物体仍然需要调用多个draw call，但静态批处理可以减少这些draw call之间的状态切换，而这些切换往往是费时的操作。从合并后的网格结构中我们还可以发现，尽管3个Teapot对象使用了同一个网格，但合并后却变成了3个独立网格。而且，我们可以从Unity的分析器中观察到在应用静态批处理前后VBO total的变化,从图 16.10所示中可以看出,VBO(Vertex Buffer Object,顶点缓冲对象)的数目变大了。这正是因为静态批处理会占用更多内存的缘故,正如本节一开头所讲,静态批处理需要占用更多的内存来存储合并后的几何结构,如果一些物体共享了相同的网格,那么在内存中每一个物体都会对应一个该网格的复制品。\n如果场景中包含了除了平行光以外的其他光源,并且在 shader中定义了额外的 Pass来处理它们，这些额外的Pass部分是不会被批处理的。图16.11显示了在场景中添加了一个会影响4个物体的点光源之后，渲染统计窗口的数据变化。\n但是,处理平行光的 Base Pass部分仍然会被静态批处理,因此,我们仍然可以节省两个 draw call。\n16.4.3 共享材质 从之前的内容可以看出，无论是动态批处理还是静态批处理，都要求模型之间需要共享同一个材质。但不同的模型之间总会需要有不同的渲染属性，例如，使用不同的纹理、颜色等。这时，我们需要一些策略来尽可能地合并材质。\n如果两个材质之间只有使用的纹理不同,我们可以把这些纹理合并到一张更大的纹理中,这张更大的纹理被称为是一张图集(atlas)。一旦使用了同一张纹理,我们就可以使用同一个材质,再使用不同的采样坐标对纹理采样即可。\n但有时,除了纹理不同外,不同的物体在材质上还有一些微小的参数变化,例如,颜色不同、某些浮点属性不同。但是，不管是动态批处理还是静态批处理，它们的前提都是要使用同一个材质。是同一个，而不是使用了同一种 Shader的材质，也就是说它们指向的材质必须是同一个实体。这意味着,只要我们调整了参数,就会影响到所有使用这个材质的对象。那么想要微小的调整怎么办呢?一种常用的方法就是使用网格的顶点数据(最常见的就是顶点颜色数据)来存储这些参数。\n前面说过，经过批处理后的物体会被处理成更大的VBO发送给GPU，VBO中的数据可以作为输入传递给顶点着色器，因此，我们可以巧妙地对VBO中的数据进行控制，从而达到不同效果的目的。一个例子是，森林场景中所有的树使用了同一种材质，我们希望它们可以通过批处理来减少 draw call,但不同树的颜色可能不同。这时,我们可以利用网格的顶点的颜色数据来调整。\n需要注意的是,如果我们需要在脚本中访问共享材质,应该使用Renderer.sharedMaterial来保证修改的是和其他物体共享的材质，但这意味着修改会应用到所有使用该材质的物体上。另一个类似的API是Renderer.material,如果使用Renderer.material来修改材质,Unity会创建一个该材质的复制品，从而破坏批处理在该物体上的应用，这可能并不是我们希望看到的。\n16.4.4 批处理的注意事项 在选择使用动态批处理还是静态批处理时,我们有一些小小的建设。\n尽可能选择静态批处理,但得时刻小心对内存的消耗,并且记住经过静态批处理的物体不可以再被移动。 如果无法进行静态批处理，而要使用动态批处理的话，那么请小心上面提到的各种条件限制。例如，尽可能让这样的物体少并且尽可能让这些物体包含少量的顶点属性和顶点数目。 对于游戏中的小道具，例如可以捡拾的金币等，可以使用动态批处理。 对于包含动画的这类物体，我们无法全部使用静态批处理，但其中如果有不动的部分，可以把这部分标识成“Static”。 除了上述提示外，在使用批处理时还有一些需要注意的地方。由于批处理需要把多个模型变换到世界空间下再合并它们，因此，如果shader中存在一些基于模型空间下的坐标的运算，那么\n16.5 减少需要处理的顶点数目 尽管drawcall是一个重要的性能指标，但顶点数目同样有可能成为GPU的性能瓶颈。在本节中，我们将给出3个常用的顶点优化策略。\n16.5.1 优化几何体 3D游戏制作通常都是由模型制作开始的。而在建模时，有一条规则我们需要记住:尽可能减少模型中三角面片的数目，一些对于模型没有影响、或是肉眼非常难察觉到区别的顶点都要尽可能去掉。为了尽可能减少模型中的顶点数目，美工人员往往需要优化网格结构。在很多三维建模软件中，都有相应的优化选项，可以自动优化网格结构。\n在Unity的渲染统计窗口中，我们可以查看到渲染当前帧需要的三角面片数目和顶点数目。需要注意的是，Unity中显示的数目往往要多于建模软件里显示的顶点数，通常Unity中显示的数目要大很多。谁才是对的呢?其实，这是因为在不同的角度上计算的，都有各自的道理，但我们真正应该关心的是Unity里显示的数目。\n我们在这里简单解释一下造成这种不同的原因。三维软件更多地是站在我们人类的角度理解顶点的，即组成几何体的每一个点就是一个单独的点。而Unity是站在GPU的角度上去计算顶点数的。在GPU看来，有时需要把一个顶点拆分成两个或更多的顶点。这种将顶点一分为多的原因主要有两个:一个是为了分离纹理坐标(uv splits)，另一个是为了产生平滑的边界(smoothing splits)。它们的本质，其实都是因为对于GPU来说，顶点的每一个属性和顶点之间必须是一对的关系。而分离纹理坐标，是因为建模时一个顶点的纹理坐标有多个。例如，对于一个立方体，它的6个面之间虽然使用了一些相同的顶点，但在不同面上，同一个顶点的纹理坐标可能并不相同。对于GPU来说，这是不可理解的，因此，它必须把这个顶点拆分成多个具有不同纹理坐标的顶点。而平滑边界也是类似的，不同的是，此时一个顶点可能会对应多个法线信息或切线信息。这通常是因为我们要决定一个边是一条硬边(hardedge)还是一条平滑边(smooth edge)。\n对于GPU来说，它本质上只关心有多少个顶点。因此，尽可能减少顶点的数目其实才是我们真正需要关心的事情。因此，最后一条几何体优化建议就是:移除不必要的硬边以及纹理衔接，避免边界平滑和纹理分离。\n16.5.2 模型的LOD技术 另一个减少顶点数目的方法是使用LOD(LevelofDetail)技术。这种技术的原理是，当一个物体离摄像机很远时，模型上的很多细节是无法被察觉到的。因此，LOD允许当对象逐渐远离摄像机时，减少模型上的面片数量，从而提高性能。\n在Unity中，我们可以使用LODGroup组件来为一个物体构建一个LOD。我们需要为同一一个对象准备多个包含不同细节程序的模型,然后把它们赋给LOD Group组件中的不同等级,Unity就会自动判断当前位置上需要使用哪个等级的模型。\n16.5.3 遮挡剔除技术 我们最后要介绍的顶点优化策略就是遮挡剔除(Occlusion culling)技术。遮挡剔除可以用来消除那些在其他物件后面看不到的物件，这意味着资源不会浪费在计算那些看不到的顶点上，进而提升性能。\n我们需要把遮挡剔除和摄像机的视锥体剔除(FrustumCulling)区分开来。视锥体剔除只会剔除掉那些不在摄像机的视野范围内的对象，但不会判断视野中是否有物体被其他物体挡住。而遮挡剔除会使用一个虚拟的摄像机来遍历场景，从而构建一个潜在可见的对象集合的层级结构。在运行时刻，每个摄像机将会使用这个数据来识别哪些物体是可见的，而哪些被其他物体挡住不可见。使用遮挡剔除技术，不仅可以减少处理的顶点数目，还可以减少overdraw，提高游戏性能。\n要在Unity中使用遮挡剔除技术，我们需要进行一系列额外的处理工作。具体步骤可以参见Unity手册的相关内容(http://docs.unity3d.com/Manual/OcclusionCulling.html)，本书不再赘述。\n模型的LOD技术和遮挡剔除技术可以同时减少CPU和GPU的负荷。CPU可以提交更少的draw call，而GPU需要处理的顶点和片元数目也减少了。\n16.6 减少需要处理的片元数目 另一个造成GPU瓶颈的是需要处理过多的片元。这部分优化的重点在于减少overdraw。简单来说，overdraw指的就是同一个像素被绘制了多次。\nUnity还提供了查看 overdraw的视图，我们可以在Scene视图左上方的下拉菜单中选中Overdraw即可。实际上，这里的视图只是提供了查看物体相互遮挡的层数，并不是真正的最终屏幕绘制的overdraw。也就是说，可以理解为它显示的是，如果没有使用任何深度测试和其他优化策略时的overdraw。这种视图是通过把所有对象都渲染成一个透明的轮廓，通过查看透明颜色的累计程度，来判断物体之间的遮挡。当然，我们可以使用一些措施来防止这种最坏情况的出现。\n16.6.1 控制绘制顺序 为了最大限度地避免overdraw，一个重要的优化策略就是控制绘制顺序。由于深度测试的存在，如果我们可以保证物体都是从前往后绘制的，那么就可以很大程度上减少overdraw。这是因为，在后面绘制的物体由于无法通过深度测试，因此，就不会再进行后面的渲染处理。\n在Unity中，那些渲染队列数目小于2500(如\u0026quot;Background\u0026quot;\u0026ldquo;Geometry\u0026quot;和\u0026quot;AlphaTest\u0026rdquo;)的对象都被认为是不透明(opaque)的物体，这些物体总体上是从前往后绘制的，而使用其他的队列(如\u0026quot;Transparent\u0026quot;\u0026ldquo;Overlay\u0026quot;等)的物体，则是从后往前绘制的。这意味着，我们可以尽可能地把物体的队列设置为不透明物体的渲染队列，而尽量避免使用半透明队列。\n而且，我们还可以充分利用Unity的渲染队列来控制绘制顺序。例如，在第一人称射击游戏中，对于游戏中的主要人物角色来说，他们使用的shader往往比较复杂，但是，由于他们通常会挡住屏幕的很大一部分区域，因此我们可以先绘制它们(使用更小的渲染队列)。而对于一些敌方角色，它们通常会出现在各种掩体后面，因此，我们可以在所有常规的不透明物体后面渲染它们(使用更大的渲染队列)。而对于天空盒子来说,它几乎覆盖了所有的像素,而且我们知道它永远会出现在所有物体的后面，因此，它的队列可以设置为\u0026quot;Geometry+1\u0026rdquo;。这样，就可以保证不会因为它而造成overdraw。\n这些排序的思想往往可以节省掉很多渲染时间。\n16.6.2 时刻警惕透明物体 对于半透明对象来说，由于它们没有开启深度写入，因此，如果要得到正确的渲染效果，就必须从后往前渲染。这意味着，半透明物体几乎一定会造成overdraw。如果我们不注意这一点，在一些机器上可能会造成严重的性能下降。例如，对于GUI对象来说，它们大多被设置成了半透明，如果屏幕中GUI占据的比例太多，而主摄像机又没有进行调整而是投影整个屏幕，那么GUI就会造成大量overdraw。\n因此，如果场景中包含了大面积的半透明对象，或者有很多层相互覆盖的半透明对象(即便它们每个的面积可能都不大)，或者是透明的粒子效果，在移动设备上也会造成大量的overdraw。这是应该尽量避免的。\n对于上述GUI的这种情况，我们可以尽量减少窗口中GUI所占的面积。如果实在无能为力，我们可以把GUI的绘制和三维场景的绘制交给不同的摄像机，而其中负责三维场景的摄像机的视角范围尽量不要和GUI的相互重叠。当然，这样会对游戏的美观度产生一定影响，因此，我们可以在代码中对机器的性能进行判断，例如，首先关闭一些耗费性能的功能，如果发现这个机器表现非常良好，再尝试开启一些特效功能。\n在移动平台上，透明度测试也会影响游戏性能。虽然透明度测试没有关闭深度测试，但由于它的实现使用了discard或clip操作，而这些操作会导致一些硬件的优化策略失效。例如，我们之前讲过PowerVR使用的基于瓦片的延迟渲染技术，为了减少overdraw它会在调用片元着色器前就判断哪些瓦片被真正渲染的。但是，由于透明度测试在片元着色器中使用了discard函数改变了片元是否会被渲染的结果，因此，GPU就无法使用上述的优化策略了。也就是说，只要在执行了所有的片元着色器后，GPU才知道哪些片元会被真正渲染到屏幕上，这样，原先那些可以减少overdraw的优化就都无效了。这种时候，使用透明度混合的性能往往比使用透明度测试更好。\n16.6.3 减少实时光照和阴影 实时光照对于移动平台是一种非常昂贵的操作。如果场景中包含了过多的点光源，并且使用了多个Pass的Shader，那么很有可能会造成性能下降。例如，一个场景里如果包含了3个逐像素的点光源，而且使用了逐像素的 Shader，那么很有可能将 draw call数目(CPU的瓶颈)提高3倍，同时也会增加overdraw(GPU的瓶颈)。这是因为，对于逐像素的光源来说，被这些光源照亮的物体需要被再渲染一次。更糟糕的是，无论是静态批处理还是动态批处理，对于这种额外的处理逐像素光源的Pass都无法进行批处理，也就是说，它们会中断批处理。\n当然，游戏场景还是需要光照才能得到出色的画面效果。我们看到很多成功的移动平台的游戏，它们的画面效果看起来好像包含了很多光源，但其实这都是骗人的。这些游戏往往使用了烘焙技术，把光照提前烘焙到一张光照纹理(lightmap)中，然后在运行时刻只需要根据纹理采样得到光照结果即可。另一个模拟光源的方法是使用GodRay。场景中很多小型光源的效果都是靠这种方法模拟的。它们一般并不是真的光源，很多情况是通过透明纹理模拟得到的。更多信息可以参见本章的扩展阅读部分。在移动平台上，一个物体使用的逐像素光源数目应该小于1(不包括平行光)。如果一定要使用更多的实时光，可以选择用逐顶点光照来代替。\n在游戏《ShadowGun》中，游戏角色看起来使用了非常复杂高级的光照计算，但这实际上是优化后的结果。开发者们把复杂的光照计算存储到一张查找纹理(lookup texture，也被称为查找表，lookup table，LUT)中。然后在运行时刻，我们只需要使用光源方向、视角方向、法线方向等参数，对LUT采样得到光照结果即可。使用这样的查找纹理，不仅可以让我们使用更出色的光照模型，例如，更加复杂的BRDF模型，还可以利用查找纹理的大小来进一步优化性能，例如，主要角色可以使用更大分辨率的LUT，而一些NPC就使用较小的LUT。《ShadowGun》的开发者开发了一个LUT烘焙工具，来帮助美工人员快速调整光照模型，并把结果存储到LUT中。\n实时阴影同样是一个非常消耗性能的效果。不仅是CPU需要提交更多的draw call，GPU也需要进行更多的处理。因此，我们应该尽量减少实时阴影，例如，使用烘焙把静态物体的阴影信息存储到光照纹理中，而只对场景中的动态物体使用适当的实时阴影。\n16.7 节省带宽 大量使用未经压缩的纹理以及使用过大的分辨率都会造成由于带宽而引发的性能瓶颈。\n16.7.1 减少纹理大小 之前提到过，使用纹理图集可以帮助我们减少drawcall的数目，而这些纹理的大小同样是个需要考虑的问题。需要注意的是，所有纹理的长宽比最好是正方形，而且长宽值最好是2的整数幂。这是因为有很多优化策略只有在这种时候才可以发挥最大效用。在Unity5中，即便我们导入的纹理长宽值并不是2的整数幂，Unity也会自动把长宽转换到离它最近的2的整数幂值。但我们仍然应该在制作美术资源时就把这条规则谨记在心，防止由于放缩而造成不好的影响。\n除此之外，我们还应该尽可能使用多级渐远纹理技术(mipmapping)和纹理压缩。在Unity中，我们可以通过纹理导入面板来查看纹理的各个导入属性。通过把纹理类型设置为Advanced，就可以自定义许多选项，例如，是否生成多级渐远纹理(mipmaps)，如图16.12所示。当勾选了Generate Mip Maps选项后,Unity就会为同一张纹理创建出很多不同大小的小纹理,构成一个纹理金字塔。而在游戏运行中就可以根据距离物体的远近，来动态选择使用哪一个纹理。这是因为，在距离物体很远的时候，就算我们使用了非常精细的纹理，但肉眼也是分辨不出来的。这种时候，我们完全可以使用更小、更模糊的纹理来代替,这可以让GPU使用分辨率更小的纹理,大量节省访问的像素数目。在某些设备上，关闭多级渐远纹理往往会造成严重的性能问题。因此，除非我们确定该纹理不会发生缩放，例如GUI和2D游戏中使用的纹理等,都应该为纹理生成相应的多级渐远纹理。\n纹理压缩同样可以节省带宽。但对于像Android这样的平台,有很多不同架构的GPU,纹理压缩就变得有点复杂,因为不同的GPU架构有它自己的纹理压缩格式,例如,PowerVRAM的PVRTC格式、Tegra的DXT格式、Adreno的ATC格式。所幸的是，Unity可以根据不同的设备选择不同的压缩格式，而我们只需要把纹理压缩格式设置为自动压缩即可。但是，GUI类型的纹理同样是个例外，一些时候由于对画质的要求，我们不希望对这些纹理进行压缩。\n16.7.2 利用分辨率缩放 过高的屏幕分辨率也是造成性能下降的原因之一,尤其是对于很多低端手机,除了分辨率高其他硬件条件并不尽如人意,而这恰恰是游戏性能的两个瓶颈:过大的屏幕分辨率和糟糕的GPU。因此,我们可能需要对于特定机器进行分辨率的放缩。当然,这样可能会造成游戏效果的下降,但性能和画面之间永远是个需要权衡的话题。\n在 Unity中设置屏幕分辨率可以直接调用 Screen.SetResolution。实际使用中可能会遇到一些情况,雨松 MOMO有一篇文章(http://www.xuanyusong.com/archives/3205)详细讲解了如何使用这种技术,读者可参考。\n16.8 减少计算复杂度 计算复杂度同样会影响游戏的渲染性能。在本节中，我们会介绍两个方面的技术来减少计算复杂度。\n16.8.1 Shader的 LOD技术 和 16.5.2节提到的模型的 LOD技术类似， Shader的 LOD技术可以控制使用的 Shader等级。它的原理是，只有Shader的LOD值小于某个设定的值，这个Shader才会被使用，而使用了那些超过设定值的Shader的物体将不会被渲染。\n我们通常会在SubShader中使用类似下面的语句来指明该shader的LOD值:\n","date":"2024-08-05T12:10:30Z","image":"https://Selaphina.github.io/p/16-unity%E4%B8%AD%E7%9A%84%E6%B8%B2%E6%9F%93%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF/cover1_hu_5e2014920a4d8f6.png","permalink":"https://Selaphina.github.io/p/16-unity%E4%B8%AD%E7%9A%84%E6%B8%B2%E6%9F%93%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF/","title":"16 Unity中的渲染优化技术"},{"content":"方法1：使用conda（推荐） 1) 创建一个新环境并安装Python 3.10.16 1 conda create -n py310 python=3.10.16 2）激活新环境 1 conda activate py310 3）验证版本 1 python --version 1 pip install ipywidgets pyyaml VoxAI 纯web的线上应用，没有本地软件，也没有找到可调用的API库。应该是闭源的商业化服务。\nhttps://blog.voxelai.ai/\n生成的red lawn mower（红色割草机）质量很好，而且是vox和minecraft风格的。免费额度有限，生成大量模型需要付费订阅。只能导出vox和minecraft的格式。需要二次转格式。\n闭源且无api库，不能本地部署/集成。\nShap-e openai的开源项目，可以本地部署，但是质量差，达不到商业水准\nGitHub - openai/shap-e: Generate 3D objects conditioned on text or images\n输出主流格式：obj/ply\n文本prompt生成的效果蛮差，图片to 3D必须是白底的图片才行，比较弱。\n本地部署生成的是无贴图无材质的obj网格。\nMake‑A‑Shape\n","date":"2025-11-14T10:12:30Z","image":"https://Selaphina.github.io/p/python%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/CUC%E5%85%91%E6%8D%A2%E7%82%B92_hu_aee3a9923aa263e8.png","permalink":"https://Selaphina.github.io/p/python%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/","title":"python虚拟环境"},{"content":"1.插件：UnrealToUnity 在日常创作中，经常碰到一些好的UE美术资源，可以使用这个插件把它转换到Unity当中。\n百度网盘链接\nUnity：2019以上的版本\nUE：版本根据自己的安装插件对应的版本自行选择，我选择的是5.3.2\n步骤：\n解压对应版本的UE插件： 新建一个UE工程。 找到工程目录，新建一个Plugins文件夹，如下：\n把解压后的插件文件夹整个复制到Plugins文件夹下。 重启UE，插件会在菜单栏上显示，可直接点击使用。 把需要转成unity资产的ue美术素材uasset拖到世界中，即可。\n默认设置即可导出。可以选择取消勾选shader等，因为会与unity本体的shader有冲突。\n事实上，导入unity后大概率会进入报错的安全模式，按照console窗口的报错一步步注释掉UE转生过来的代码即可，很快就能清理完毕。另外，还是强调，尽量把ue带来的shader删除干净，在unity里重新写一遍shader。\n2.简单水墨渲染的初步实现 2.1 轮廓线的渲染 《unity shader入门精要 14章》已经详细介绍轮廓线的渲染方法。\n过程式集合轮廓线渲染方法： 1）单独一个pass将模型沿法线扩张一点，渲染成轮廓线的颜色，\n2）然后再用一个pass正常内部渲染着色，遮住前面的部分。留下显示出来的部分就是轮廓线了。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 Properties { [Header(OutLine)] // Stroke Color 描边颜色 _StrokeColor (\u0026#34;Stroke Color\u0026#34;, Color) = (0,0,0,1) // Noise Map 噪声图 _OutlineNoise (\u0026#34;Outline Noise Map\u0026#34;, 2D) = \u0026#34;white\u0026#34; {} // First Outline Width _Outline (\u0026#34;Outline Width\u0026#34;, Range(0, 1)) = 0.1 // Second Outline Width _OutsideNoiseWidth (\u0026#34;Outside Noise Width\u0026#34;, Range(1, 2)) = 1.3 _MaxOutlineZOffset (\u0026#34;Max Outline Z Offset\u0026#34;, Range(0,1)) = 0.5 } SubShader { Tags { \u0026#34;RenderType\u0026#34;=\u0026#34;Opaque\u0026#34; \u0026#34;Queue\u0026#34;=\u0026#34;Geometry\u0026#34;} // the first outline pass Pass { // 主要在vertex shader内进行计算 省略部分基本参数设置 v2f vert (a2v v) { // fetch Perlin noise map here to map the vertex // add some bias by the normal direction float4 burn = tex2Dlod(_OutlineNoise, v.vertex); v2f o = (v2f)0; float3 scaledir = mul((float3x3)UNITY_MATRIX_MV, normalize(v.normal.xyz)); scaledir += 0.5; scaledir.z = 0.01; scaledir = normalize(scaledir); // camera space float4 position_cs = mul(UNITY_MATRIX_MV, v.vertex); position_cs /= position_cs.w; float3 viewDir = normalize(position_cs.xyz); float3 offset_pos_cs = position_cs.xyz + viewDir * _MaxOutlineZOffset; // y = cos（fov/2） float linewidth = -position_cs.z / (unity_CameraProjection[1].y); linewidth = sqrt(linewidth); position_cs.xy = offset_pos_cs.xy + scaledir.xy * linewidth * burn.x * _Outline ; position_cs.z = offset_pos_cs.z; o.pos = mul(UNITY_MATRIX_P, position_cs); return o; } // fragment shader只是输出了一个颜色 不赘述 } } 水墨画风格渲染，做了一个最简单的noise干扰，在这里使用noise纹理图片（_OutlineNoise）进行采样，这样又个好处就是随机出来的轮廓不会随着视角的改变而改变。\n其中，\n1 2 // y = cos（fov/2） float linewidth = -position_cs.z / (unity_CameraProjection[1].y); 增加了一个linewidth的操作，因为unity_CameraProjection[1].y = cos（fov/2）。\n**目的：**保证轮廓线随着FOV的变换也是成一定比例，近大远小，而不是远近一样粗细。\n最后一个小trick是，再增加了一个pass进行完全相同的操作，只是宽度再稍微增加一点，然后在fragment shader里根据noise再进行一下剔除。这也是在属性里面，之前没有用到的_OutsideNoiseWidth，来控制第二个pass的轮廓线的宽度，理论上它要大于1，比第一个pass稍微宽一些。简要的代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 // 在vertex shader内 只需要稍微改变一点 position_cs.xy = offset_pos_cs.xy + scaledir.xy * linewidth * burn.y * _Outline * _OutsideNoiseWidth ; // 在fragment shader内 也稍微根据noise突变做了下剔除 fixed4 frag(v2f i) : SV_Target { //clip randome outline here fixed4 c = _StrokeColor; fixed3 burn = tex2D(_OutlineNoise, i.uv).rgb; if (burn.x \u0026gt; 0.5) discard; return c; } 这样可以形成毛笔笔触边缘毛边的效果：\n2.2 内部着色 而内部着色的基本思想和unity卡通渲染的一致，使用最基本的光照方程，再映射到一张ramp图上进行采样，最后形成的就是阶梯状的颜色过渡。在这里基础物体用的ramp图如下：\n天空盒用的ramp图进行了反转，否则黑天和灰色墨迹很丑：\n将纹理笔触作为一个noise贴图，扰动uv的值之后再进行一次高斯模糊。在这里是用了一张笔触纹理和一个noise贴图混合的一起扰动uv。\n内部着色步骤：\n先计算半郎伯特漫反射系数;\n用笔触纹理+noise纹理稍微扰动一下。\n在采样ramp纹理的时候进行高斯模糊。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 Shader \u0026#34;ChinesePainting/MountainShader\u0026#34; { Properties { [Header(OutLine)] //...省略上述已介绍过的 [Header(Interior)] _Ramp (\u0026#34;Ramp Texture\u0026#34;, 2D) = \u0026#34;white\u0026#34; {} // Stroke Map _StrokeTex (\u0026#34;Stroke Tex\u0026#34;, 2D) = \u0026#34;white\u0026#34; {} _InteriorNoise (\u0026#34;Interior Noise Map\u0026#34;, 2D) = \u0026#34;white\u0026#34; {} // Interior Noise Level _InteriorNoiseLevel (\u0026#34;Interior Noise Level\u0026#34;, Range(0, 1)) = 0.15 // Guassian Blur radius (\u0026#34;Guassian Blur Radius\u0026#34;, Range(0,60)) = 30 resolution (\u0026#34;Resolution\u0026#34;, float) = 800 hstep(\u0026#34;HorizontalStep\u0026#34;, Range(0,1)) = 0.5 vstep(\u0026#34;VerticalStep\u0026#34;, Range(0,1)) = 0.5 } SubShader { Tags { \u0026#34;RenderType\u0026#34;=\u0026#34;Opaque\u0026#34; \u0026#34;Queue\u0026#34;=\u0026#34;Geometry\u0026#34;} // the first outline pass // 省略 // the second outline pass for random part, a little bit wider than last one // 省略 // the interior pass Pass { // 之前的vertex shader部分没有特殊操作 省略 float4 frag(v2f i) : SV_Target { fixed3 worldNormal = normalize(i.worldNormal); fixed3 worldLightDir = normalize(UnityWorldSpaceLightDir(i.worldPos)); // Noise // For the bias of the coordiante float4 burn = tex2D(_InteriorNoise, i.uv); //a little bit disturbance fixed diff = dot(worldNormal, worldLightDir); diff = (diff * 0.5 + 0.5); float2 k = tex2D(_StrokeTex, i.uv).xy; float2 cuv = float2(diff, diff) + k * burn.xy * _InteriorNoiseLevel; // This iniminate the bias of the uv movement if (cuv.x \u0026gt; 0.95) { cuv.x = 0.95; cuv.y = 1; } if (cuv.y \u0026gt; 0.95) { cuv.x = 0.95; cuv.y = 1; } cuv = clamp(cuv, 0, 1); // Guassian Blur float4 sum = float4(0.0, 0.0, 0.0, 0.0); float2 tc = cuv; // blur radius in pixels float blur = radius/resolution/4; sum += tex2D(_Ramp, float2(tc.x - 4.0*blur*hstep, tc.y - 4.0*blur*vstep)) * 0.0162162162; sum += tex2D(_Ramp, float2(tc.x - 3.0*blur*hstep, tc.y - 3.0*blur*vstep)) * 0.0540540541; sum += tex2D(_Ramp, float2(tc.x - 2.0*blur*hstep, tc.y - 2.0*blur*vstep)) * 0.1216216216; sum += tex2D(_Ramp, float2(tc.x - 1.0*blur*hstep, tc.y - 1.0*blur*vstep)) * 0.1945945946; sum += tex2D(_Ramp, float2(tc.x, tc.y)) * 0.2270270270; sum += tex2D(_Ramp, float2(tc.x + 1.0*blur*hstep, tc.y + 1.0*blur*vstep)) * 0.1945945946; sum += tex2D(_Ramp, float2(tc.x + 2.0*blur*hstep, tc.y + 2.0*blur*vstep)) * 0.1216216216; sum += tex2D(_Ramp, float2(tc.x + 3.0*blur*hstep, tc.y + 3.0*blur*vstep)) * 0.0540540541; sum += tex2D(_Ramp, float2(tc.x + 4.0*blur*hstep, tc.y + 4.0*blur*vstep)) * 0.0162162162; return float4(sum.rgb, 1.0); } ENDCG } } FallBack \u0026#34;Diffuse\u0026#34; } 目前是这样的效果：\n","date":"2025-11-11T10:12:30Z","image":"https://Selaphina.github.io/p/%E6%AF%95%E4%B8%9A%E8%AE%BE%E8%AE%A1-%E6%B0%B4%E5%A2%A8%E9%A3%8E%E6%A0%BC%E5%8C%96%E5%9C%BA%E6%99%AF%E6%B8%B2%E6%9F%93%E5%85%B6%E4%B8%80/cover1_hu_89d58f86b094ccaa.png","permalink":"https://Selaphina.github.io/p/%E6%AF%95%E4%B8%9A%E8%AE%BE%E8%AE%A1-%E6%B0%B4%E5%A2%A8%E9%A3%8E%E6%A0%BC%E5%8C%96%E5%9C%BA%E6%99%AF%E6%B8%B2%E6%9F%93%E5%85%B6%E4%B8%80/","title":"毕业设计 | 水墨风格化场景渲染（其一）"},{"content":"第18章 基于物理的渲染 在之前的章节中，我们学习了Lambert光照模型、Phong光照模型和Blinn-Phong光照模型。但这些光照模型的缺点在于，它们都是经验模型。如果我们需要渲染更高质量的画面，这些经验模型就显得不再能满足我们的要求了。\n近年来，基于物理的渲染技术（Physically Based Shading，PBS）被逐渐应用于实时渲染中。总体来说，PBS是为了对光和材质之间的行为进行更加真实的建模。PBS早已被广泛应用到电影行业中，但游戏中的PBS是近年来才逐渐流行起来的。Unity最早在2012年的《蝴蝶效应》（英文名：Butterfly Effect）的demo中大量使用了PBS，并在Unity5中正式将PBR引入到引擎渲染中。Unity5引入了一个名为Standard Shader的可在不同材质之间通用的着色器，而该着色器就是使用了基于物理的光照模型。需要注意的是，PBS并不意味着渲染出来的画面一定是像照片一样真实的，例如，Pixar和Disney尽管长期使用PBS渲染电影画面，但它们得到的风格是非常有特色的艺术风格。相信很多读者或多或少看到过使用PBS渲染出来的画面是多么的酷炫，并很想了解这背后的技术原理。如果你是一个程序员，可能有很大的冲动想要自己实现一个PBS渲染框架，但往往走到后面会发现有很多看不懂的名词以及一大堆与之相关的论文：如果你是一个美工人员，你可能会找到很多关于如何制作PBS中使用的纹理教程，但你大概也了解，想要使用PBS实现出色的渲染效果，并不是纹理+一个shader这么简单的问题。\n现在，我们有一个好消息和一个坏消息要告诉大家。先说好消息，Unity5引入的基于物理的渲染不需要我们过多地了解PBS是如何实现的，就能利用各种内置工具来实现一个不错的渲染效果。然而坏消息是，我们很难通过短短几段文字来非常详细地告诉读者这些渲染到底是如何实现的，因为这其中需要牵扯许多复杂的光照模型，如果要完全理解每一种模型的话，大概还要讲很多论文和其他参考文献。不过还有一个好消息是，我们相信读者在学完本章后可以了解一些PBS的原理，如果你对PBS有着浓厚的兴趣，想要尝试自己构建一个PBS的渲染框架，可以在本章的扩展阅读部分找到许多非常有价值的参考资料。\n在本章中，我们首先会讲解PBS的基本原理，让读者了解它们与我们之前所学的渲染方式到底有哪些不同。尽管本书的定位并不是“教你如何使用Unity”，但我们决定花一点时间来告诉读者Unity5引入的Standard Shader是如何工作的，以及如何在Unity5中使用它和其他工具来搭建一个场景，我们希望通过这些内容来让读者明白PBS中的一些关键因素。尽管PBS在手机上的应用并不十分广泛，但我们相信这是未来的发展趋势，希望本章可以为读者打开PBS的大门。\n18.1 PBS的理论和数学基础 在学习如何实现PBS之前，我们非常有必要来了解基于物理的渲染所基于的理论和数学基础。我们不会过多地牵扯一些论文资料，但如果在阅读过程中读者发现无法理解一些光照模型的实现原理，这可能意味着你需要阅读更多的参考文献。本节主要参考了Naty Hoffman在SIGGRAPH2013上做的名为Background: Physics and Math of Shading的演讲[1]。\n18.1.1 光是什么 尽管我们之前一直讲光照模型，但要问读者，光到底是什么，可能没有多少人可以解释清楚。在物理学中，光是一种电磁波。首先，光由太阳或其他光源中被发射出来，然后与场景中的对象相交，一些光线被吸收（absorption），而另一些则被散射（scattering），最后光线被一个感应器（例如我们的眼睛）吸收成像。\n通过上面的过程，我们知道材质和光线相交会发生两种物理现象：散射和吸收（其实还有自发光现象）。光线会被吸收是由于光被转化成了其他能量，但吸收并不会改变光的传播方向。相反的，散射则不会改变光的能量，但会改变它的传播方向。在光的传播过程中，影响光的一个重要的特性是材质的折射率（refractive index）。我们知道，在均匀的介质中，光是沿直线传播的。但如果光在传播时介质的折射率发生了变化，光的传播方向就会发生变化。特别是，如果折射率是突变的，就会发生光的散射现象。\n实际上，在现实生活中，光和物体之间的交互过程是非常复杂的，大多数情况下并不存在一种可分析的解决方法。但为了在渲染中对光照进行建模，我们往往只考虑一种特殊情况，即只考虑两个介质的边界是无限大并且是光学平滑（optically flat）的。尽管真实物体的表面并不是无限延伸的，也不是绝对光滑的，但和光的波长相比，它们的大小可以被近似认为是无限大以及光学平滑的。在这样的前提下，光在不同介质的边界会被分割成两个方向：反射方向和折射方向。而有多少百分比的光会被反射（另一部分就是被折射了）则是由菲涅耳等式（Fresnel equations）来描述的，如图18.1所示。\n但是，这些与光线的交界处真的是像镜子一样平坦吗？尽管在上面我们已经说过，相对于光的波长来说，它们的确可以被认为是光学平坦的。但是，如果想象我们有一个高倍放大镜，去放大这些被照亮的物体表面，就会发现有很多之前肉眼不可见的凹凸不平的平面。在这种情况下，物体的表面和光照发生的各种行为，更像是一系列微小的光学平滑平面和光交互的结果，其中每个小平面会把光分割成不同的方向。\n这种建立在微表面的模型更容易解释为什么有些物体看起来粗糙，而有些看起来就平滑，如图18.2所示。想象我们用一个放大镜去观察一个光滑物体的表面，尽管它的表面仍然由许多凹凸不平的微表面构成，但这些微表面的法线方向变化角度小，因此，由这些表面反射的光线方向变化也比较小，如图18.2左图所示，这使得物体的高光反射更加清晰。而图18.2右图所示的粗糙表面则相反，由此得到的高光反射效果更模糊。\n在下面的内容中，我们并没有讨论那些被微表面折射的光。这些光被折射到物体的内部，一部分被介质吸收，一部分又被散射到外部。金属材质具有很高的吸收系数，因此，所有被折射的光往往会被立刻吸收，被金属内部的自由电子转化成其他形式的能量。而非金属材质则会同时表现出吸收和散射两种现象，这些被散射出去的光又被称为次表面散射光（subsurface-scattered light）。在图18.3中，我们给出了一条由微表面折射的光的传播路径（实际反射面可看到监线）。\n现在，我们把放大镜从物体表面拿开，继续从渲染的层级大小上考虑光与表面一点的交互行为。那么，由微表面反射的光可以被认为是该点上一些方向变化不大的反射光，如图18.4中的黄线所示。而折射光线（蓝线）则需要更多的考虑。那些次表面散射光会从不同于入射点的位置从物体内部再次射出，如图18.4左图所示。而这些离入射点的距离值和像素大小之间的关系会产生两种建模结果。如果像素要大于这些散射距离的话，意味着这些次表面散射产生的距离可以被忽略，那我们的渲染就可以在局部进行，如图18.4右图所示。如果像素要小于这些散射距离，我们就不可以选择忽略它们了，要实现更真实的次表面散射效果，我们需要使用特殊的渲染模型，也就是所谓的次表面散射渲染技术。\n我们下面的内容均建立在不考虑次表面散射的距离，而完全使用局部着色渲染的前提下。\n18.1.2 双向反射分布函数（BRDF） 在了解了上面的理论基础后，我们现在来学习如何用数学表达式来表示上面的光照模型。这意味着，我们要对光这个看似抽象的概念进行量化。\n我们可以用辐射率（radiance）来量化光。辐射率是单位面积、单位方向上光源的辐射通量，通常用L来表示，被认为是对单一光线的亮度和颜色评估。在渲染中，我们通常会基于表面的入射光线的入射辐射率L来计算出射辐射率Lo，这个过程也往往被称为是着色（shading）过程。而要得到出射辐射率Lo，我们需要知道物体表面一点是如何和光进行交互的。而这个过程就可以使用BRDF（Bidirectional Reflectance Distribution Function，中文名称为双向反射分布函数）来定量分析。大多数情况下，BRDF可以用f(I,V)来表示，其中I为入射方向，V为观察方向（双向的含义）。这种情况下，绕着表面法线旋转入射方向或观察方向并不会影响BRDF的结果，这种BRDF被称为是各项同性（isotropic）的BRDF。与之对应的则是各向异性（anisotropic）的BRDF。\n那么，BRDF到底表示的含义是什么呢？BRDF有两种理解方式——第一种理解是，当给定入射角度后，BRDF可以给出所有出射方向上的反射和散射光线的相对分布情况；第二种理解是，当给定观察方向（即出射方向）后，BRDF可以给出从所有入射方向到该出射方向的光线分布。一个更直观的理解是，当一束光线沿着入射方向I到达表面某点时，f(I,V)表示了有多少部分的能量被反射到了观察方向V上。\n据此，我们给出基于物理渲染的技术中，第一个重要的等式——反射等式（reflection equation）：\n反射等式实际上是渲染方程的一个特殊情况，但它是基于物理基础的。尽管上面的式子看起来有些复杂，但很好理解，即给定观察视角V，该方向上的出射辐射率Lo(V)等于所有入射方向的辐射率积分乘以它的BRDF值f(I,V)，再乘以一个余弦值（n·I）。如果积分的概念对某些读者来说难以理解，我们使用更简单的方式来理解。想象我们现在要计算表面上某点的出射辐射率，我们知道该点的观察方向，该点的出射辐射率是由从许多不同方向的入射辐射率叠加后的结果。其中，BRDF表示了不同方向的入射光在该观察方向上的权重分布。我们把这些不同方向的光辐射率（Li(I)部分）乘以观察方向上所占的权重（f(I,V)部分），再乘以它们在该表面的投影结果（(n·I)部分），最后再把这些值加起来（即做积分）就是最后的出射辐射率。\n在游戏渲染中，我们通常和一些精确光源（punctual light sources）打交道的，而不是计算所有入射光线在半球面上的积分。精确光源指的是那些方向确定、大小为无限小的光源，例如，常见的点光源、聚光灯等。我们使用I来表示它的方向，使用C_light表示它的颜色。使用精确光源的最大的好处在于，我们可以大大简化上面的反射等式。这里省略推导过程（有兴趣的读者可以阅读参考文献[1]），直接给出结论，即对于一个精确光源，我们可以使用下面的等式来计算它在某个观察方向V上的出射辐射率：\n和之前使用积分形式的原始反射等式相比，上面的式子使用一个特定的BRDF值来代替积分操作，这大大简化了计算。如果场景中包含了多个精确光源，我们可以把它们分别代入上面的式子进行计算，然后把它们的结果相加即可。\n下面，我们来看一下反射等式中的重要组成部分——BRDF是如何得到的。可以看出，BRDF决定了着色过程是否是基于物理的。这可以由BRDF是否满足两个特性来判断：它是否满足交换律（reciprocity）和能量守恒（energy conservation）。\n交换律要求当交换I和V的值后，BRDF的值不变，即： $$ f(I,V) = f(V,I) $$ 而能量守恒则要求表面反射的能量不能超过入射的光能，即：\n基于这些理论，BRDF可以用于描述两种不同的物理现象：表面反射和次表面散射。针对每种现象，BRDF通常会包含一个单独的部分来描述它们——用于描述表面反射的部分被称为高光反射项（specular term），以及用于描述次表面散射的漫反射项（diffuse term），如图18.5所示。\n18.1.3 漫反射项 我们之前所学习的Lambert模型就是最简单、也是应用最广泛的漫反射BRDF。准确的Lambertian BRDF的表示为：\n其中，C_diff表示漫反射光线所占的比例，它也通常被称为是漫反射颜色（diffuse color）。与我们之前讲过的Lambert光照模型不太一样的是，上面的式子实际上是一个定值，我们常见到的余弦（即(n·I)）因子部分实际是反射等式的一部分，而不是BRDF的部分。上面的式子之所以要除以π，是因为我们假设漫反射在所有方向上的强度都是相同的，而BRDF要求在半球内的积分值为1。因此，给定入射方向I的光源在表面某点的出射漫反射辐射率为：\nLambert模型虽然简单，但很多基于物理的渲染选择使用了更复杂的漫反射项来模拟次表面散射的结果。例如，在Disney使用的BRDF[2]中，它的漫反射项为：\n其中，F_D90 = 0.5 + 2 × roughness × (h·I)²\n在Disney的实现中，baseColor是表面颜色，通常由纹理采样得到，roughness是表面的粗糙度。上面的漫反射项既考虑了在掠射角（glancing angles）漫反射项的能量变化，还考虑了表面的粗糙度对漫反射的影响。而上面的式子也正是Unity5内部使用的漫反射项。\n18.1.4 高光反射项 在现实生活中，几乎所有的物体都或多或少有高光反射现象。John Hable在他的文章中就强调了Everything is Shiny。但在许多传统的shader中，很多材质只考虑了漫反射效果，而并没有添加高光反射，这使得渲染出来的画面并不那么真实可信。在基于物理的渲染中，BRDF中的高光反射项大多数都是建立在微面元理论（microfacet theory）的假设上的。微面元理论认为，物体表面实际是由许多人眼看不到的微面元组成的，虽然物体表面并不是光学平滑的，但这些微面元可以被认为是光学平滑的，也就是说它们具有完美的高光反射。当光线和物体表面一点相交时，实际上是和一系列微面元交互的结果。正如我们在18.1.1节中看到的，当光和这些微面元相交时，光线会被分割成两个方向——反射方向和折射方向。这里我们只需要考虑被反射的光线，而折射光线已经在之前的漫反射项中考虑过了。当然，微面元理论也仅仅是真实世界的散射的一种近似理论，它也有自身的缺陷，仍然有一些材质是无法使用微面元理论来描述的。\n假设表面法线为n，这些微面元的法线m并不都等于n，因此，不同的微面元会把同一入射方向的光线反射到不同的方向上。而当我们计算BRDF时，入射方向I和观察方向V都会被给定，这意味着只有一部分微面元反射的光线才会进入到我们的眼睛中，这部分微面元会恰好把光线反射到方向V上，即它们的法线m等于I和V的一半，也就是我们一直看到的半角度矢量h（half-angle vector，也被称为half vector），如图18.6（a）所示。\n然而，这些m=h的微面元反射也并不会全部添加到BRDF的计算中。这是因为，它们其中一部分会在入射方向I上被其他微面元挡住（shadowing），如图18.6（b）所示，或是在它们的反射方向V上被其他微面元挡住了（masking），如图18.6（c）所示。微面元理论认为，所有这些被遮挡住的微面元不会添加到高光反射项的计算中（实际上它们中的一些由于多次反射仍然会被我们看到，但这不在微面元理论的考虑范围内）。\n基于微面元理论的这些假设，BRDF的高光反射项可以用下面的形式来表示：\n这就是著名的Torrance-Sparrow微面元模型[5]。上面的式子看起来难以理解，实际上其中的各个项对应了我们之前讲到的不同现象。D(h)是微面元的法线分布函数（normal distribution function，NDF），它用于计算有多少比例的微面元的法线满足m=h，只有这部分微面元才会把光线从I方向反射到V上。G(I,V,h)是阴影-遮掩函数（shadowing-masking function），它用于计算那些满足m=h的微面元中有多少会由于遮挡而不会被人眼看到，因此它给出了活跃的微面元（active microfacets）所占的浓度，只有活跃的微面元才会成功地把光线反射到观察方向上。F(I,h)则是这些活跃微面元的菲涅耳反射（Fresnel reflectance）函数，它可以告诉我们每个活跃的微面元会把多少入射光线反射到观察方向上，即表示了反射光线占入射光线的比率。事实上，现实生活中几乎所有的物体都会表现出菲涅耳现象，读者可以在一篇很有意思的文章Everything has Fresnel中看到一些这样的例子。最后，分母4(n·I)(n·V)是用于校正从微面元的局部空间到整体宏观表面数量差异的校正因子。\n这些不同的部分又可以衍生出很多不同的BRDF实现。例如，我们之前学习的Blinn-Phong模型就是一种非常简单的模型，它使用的法线分布函数D(h)为：\n但实际上Blinn-Phong模型并不能真实地反映很多真实世界中物体的微面元法线反射分布，因此，很多更加复杂的分布函数被提了出来，例如GGX[3]、Beckmann[4]等。同样，阴影-遮掩函数G(I,V,h)也有很多相关工作被提了出来，例如Smith模型[6]。这些数学模型都是为了更加接近使用光学测量仪器测量出来的真实物体的反射分布数据。\n尽管存在很多基于物理的BRDF模型，但在真实的电影或游戏制作中，我们希望在直观性和物理可信度之间找到一个平衡点，使得实现的BRDF既可以让美工人员直观地调节各个参数，而又有一定的物理可信度。当然，有时候为了满足直观性我们不得不牺牲一定的物理特性，得到的BRDF可能不是严格基于物理原理的。\n在下面的内容中，我们给出Unity5使用的实现。\n18.1.5 Unity中的PBS实现 在之前的内容中，我们提到了Unity5的PBS实际上是受Disney的BRDF[2]的启发。这种BRDF最大的好处之一就是很直观，只需要提供一个万能的shader就可以让美工人员通过调整少量参数来渲染绝大部分常见的材质。我们可以在Unity内置的UnityStandardBRDF.cginc文件中找到它的实现。\n总体来说，Unity5一共实现了两种PBS模型。一种是基于GGX模型的，另一种则是基于归一化的Blinn-Phong模型的。这两种模型使用了不同的公式来计算高光反射项中的法线分布函数D(h)和阴影-遮掩函数G(I,V,h)。在默认情况下，Unity5使用基于归一化后的Blinn-Phong模型来实现基于物理的渲染（尽管很多引擎选择使用GGX模型）。\n如前面所讲，Unity使用的BRDF中的漫反射项使用的公式如下：\n其中，F_D90 = 0.5 + 2 × roughness × (h·I)²\n下面我们给出基于GGX模型的高光反射项公式。对于基于归一化的Blinn-Phong模型的高光反射公式，读者可以从UnityStandardBRDF.cginc文件找到它们的实现。\nUnity对高光反射项中的法线分布函数D(h)采用了GGX模型的一种实现：\n其中，α = roughness²\n阴影-遮掩函数G(I,V,h)则使用了一种由GGX衍生出的Smith-Schlick模型： $$ G(I,V,h) = 1 / (((n·I) × (1 - k) + k) × ((n·V) × (1 - k) + k)) $$ 其中，k = roughness² / 2\n而菲涅耳反射F(I,h)则使用了图形学中经常使用的Schlick菲涅耳近似等式：\n其中F0表示高光反射系数，在Unity中往往指的就是高光反射颜色。\n上面的公式对于某些读者来说可能晦涩难懂，实际上，这些数学大多来源于对真实世界中各种物体的BRDF的分析，再使用不同的数学模型进行逼近。如果读者想要深入了解基于物理的渲染的数学原理和应用的话，可以参见本章的扩展阅读部分。\n幸运的是，在Unity中我们不需要自己在shader中实现上面的公式，Unity已经为我们提供了现成的基于物理着色的shader，也就是Standard Shader。\n18.2 Unity5的Standard Shader 当我们在Unity5中新创建一个模型或是新创建一个材质时，其默认使用的着色器都是一个名为Standard的着色器。这个Standard Shader就使用了我们之前所讲的基于物理的渲染。\nUnity支持两种流行的基于物理的工作流程：金属工作流（Metallic workflow）和高光反射工作流（Specular workflow）。其中，金属工作流是默认的工作流程，对应的Shader为Standard Shader。而如果想要使用高光反射工作流，就需要在材质的Shader下拉框中选择Standard（Specular setup）。需要注意的是，通常来讲，使用不同的工作流可以实现相同的效果，只是它们使用的参数不同而已。金属工作流也不意味着它只能模拟金属类型的材质，金属工作流的名字来源于它定义了材质表面的金属值（是金属类型的还是非金属类型的）。高光反射工作流的名字来源于它可以直接指定表面的高光反射颜色（有很强的高光反射还是很弱的高光反射）等，而在金属工作流中这个颜色需要由漫反射颜色和金属值衍生而来。在实际的游戏制作过程中，我们可以选择自己更偏好的工作流来制作场景，这更多的是个人喜好的问题。当然也可以同时混用两种工作流。\n在下面的内容中，我们用Standard Shader来统称Standard和Standard（Specular setup）着色器。Unity提供的Standard Shader允许让我们只使用这一种shader来为场景中所有的物体进行着色，而不需要考虑它们是否是金属材质还是塑料材质等，从而大大减少我们不断调整材质参数所花费的时间。\n18.2.1 它们是如何实现的 Standard和Standard（Specular setup）的Shader源代码可以在Unity内置的builtin_shaders-5.x/DefaultResourcesExtra文件夹中找到，这些shader依赖于builtin_shaders-5.x/CGIncludes文件夹中定义的一些头文件。这些相关的头文件的名称大多类似于UnityStandardXXX.cginc，其中定义了和PBS相关的各个函数、结构体和宏等。表18.1列出了这些头文件的名称以及它们的主要用处。\n表18.1\n文件 描述 UnityPBSLighting.cginc 定义了表面着色器使用的标准光照函数和相关的结构体等，如LightingStandardSpecular函数和SurfaceOutputStandardSpecular结构体 UnityStandardCore.cginc 定义了Standard和Standard（Specular setup）Shader使用的顶点/片元着色器和相关的结构体、辅助函数等，如vertForwardBase、fragForwardBase、MetallicSetup、SpecularSetup函数和VertexOutputForwardBase、FragmentCommonData结构体 UnityStandardBRDF.cginc 实现了Unity中基于物理的渲染技术，定义了BRDF1_Unity_PBS、BRDF2_Unity_PBS和BRDF3_Unity_PBS等函数，来实现不同平台下的BRDF UnityStandardInput.cginc 声明了Standard Shader使用的相关输入，包括shader使用的属性和顶点着色器的输入结构体VertexInput，并定义了基于这些输入的辅助函数，如TexCoords、Albedo、Occlusion、SpecularGloss等函数 UnityStandardUtils.cginc Standard Shader使用的一些辅助函数，将来可能会移到UnityCG.cginc文件中 UnityStandardConfig.cginc 对Standard Shader的相关配置，例如默认情况下关闭简化版的PBS实现（将UNITY_STANDARD_SIMPLE设为0），以及使用基于归一化的Blinn-Phong模型而非GGX模型来实现BRDF（将UNITY_BRDF_GGX设为0） UnityStandardMeta.cginc 定义了Standard Shader中“LightMode”为“Meta”的Pass（用于提取光照纹理和全局光照的相关信息）使用的顶点/片元着色器，以及它们使用的输入/输出结构体 UnityStandardShadow.cginc 定义了Standard Shader中“LightMode”为“ShadowCaster”的Pass（用于投射阴影）使用的顶点/片元着色器，以及它们使用的输入/输出结构体 UnityGlobalIllumination.cginc 定义了和全局光照相关的函数，如UnityGlobalIllumination函数 我们可以打开Standard.shader和StandardSpecular.shader文件来分析Unity是如何实现基于物理的渲染的。总体来讲，这两个shader的代码基本相同——它们都定义了两个SubShader，第一个SubShader使用的计算更加复杂，主要针对非移动平台（通过#pragma exclude_renderers gles来排除GLES平台），并定义了前向渲染路径和延迟渲染路径使用的Pass，以及用于投射阴影和提取元数据的Pass；第二个SubShader定义了4个Pass，其中两个Pass用于前向渲染路径，一个Pass用于投射阴影，另一个Pass用于提取元数据，该SubShader主要针对移动平台。\nStandard.shader和StandardSpecular.shader最大的不同之处在于，它们在设置BRDF的输入时使用了不同的函数来设置各个参数——基于金属工作流的Standard Shader使用MetallicSetup函数来设置各个参数，基于高光反射工作流的Standard（Specular setup）Shader使用SpecularSetup函数来设置。MetallicSetup和SpecularSetup函数均在UnityStandardCore.cginc文件中被定义。图18.7给出了Standard Shader中用于前向渲染路径的典型实现，这是由对内置文件的分析所得。\n从图18.7中可以看出，两个Pass的代码大体相同，只是ForwardBase Pass进行了更多的光照计算，例如，计算全局光照、自发光等效果，这些计算只需要在物体的整个渲染过程中计算一次即可，因此不需要在ForwardAdd Pass中再计算一次，这与我们之前学习前向渲染时的经验一致。\n18.2.2 如何使用Standard Shader 我们之前提到，Unity5的Standard Shader适用于各种材质的物体，但是，我们应该如何使用Standard Shader来得到不同的材质效果呢？\n我们首先来回答一个问题，为什么不同的材质看起来是如此不同呢？这需要回顾我们在18.1节讲到的内容。我们知道，材质和光的交互可以分成漫反射和高光反射两个部分，其中漫反射对应了次表面散射的结果，而高光反射则对应了表面反射的结果。通过对金属材质和非金属材质的分析，我们可以得到它们的漫反射和高光反射的一些特点。\n金属材质\n几乎没有漫反射，因为所有被吸收的光都会被自由电子立刻转化为其他形式的能量 有非常强烈的高光反射 高光反射通常是有颜色的，例如金子的反光颜色为黄色 非金属材质\n大多数角度高光反射的强度比较弱，但在掠射角时高光反射强度反而会增强，即菲涅耳现象 高光反射的颜色比较单一 漫反射的颜色多种多样 但真实的材质大多混合了上面的这些特性，Unity提供的工作流就是为了更加方便地让我们针对以上特性来调整材质效果。在Unity官方提供的示例项目ShaderCalibrationScene（https://www.assetstore.unity3d.com/en/#!/content/25422）中，Unity提供了两个非常有参考价值的校准表格，如图18.8所示，它们分别对应了金属工作流和高光反射工作流使用的参考属性值，来方便我们针对不同类型的材质来调整参数。读者也可以在本书资源的Assets/Textures/Chapter18/Charts文件夹找到这两张校准表格。\n我们以图18.8的左图，即金属工作流使用的校准表格为例，来解释如何使用这张校准表格来指导我们调整材质。在本书资源的场景文件Scene_18_2中，我们提供了一个简单的场景来展示不同材质的结果。图18.9显示了场景结果以及物体使用的材质。需要注意的是，读者需要在Edit→Project Settings→Player→Other Settings→Color Space中选择Linear才可以得到和图18.9中相同的效果，这是因为基于物理的渲染需要使用线性空间（详见18.3.4节）来进行相关计算。\n在金属工作流中，材质面板中的Albedo定义了物体的整体颜色，它通常就是我们视觉上认为的物体颜色。从亮度来看，非金属材质的亮度范围通常在50243之间，而金属材质的亮度一般在186255之间。Unity给的校准表格（见图18.8中的左图）中还给出了一些非金属材质和金属材质使用的示例Albedo属性值，我们可以直接使用这些示例值来作为材质属性。当然，也可以直接使用一张纹理作为材质的Albedo值。在我们的例子中，我们把金属材质（图18.9中的左边的球体）的Albedo设为银灰色，而把塑料材质（图18.9中的右边的球体）的设为蓝绿色。材质面板中的下一个属性是Metallic，它定义了该物体表面看起来是否更像金属或非金属。同样，我们也可以使用一张纹理来采样得到表面的Metallic值，此时该纹理中的R通道值将对应了Metallic值。在我们的例子中，我们把金属材质的Metallic值设为1，表明该物体几乎完全是一个金属材质，同时把塑料材质的Metallic值设为0，表明该物体几乎没有任何金属特性。最后一个重要的材质属性是Smoothness，它是上一个属性Metallic的附属值，定义了从视觉上来看该表面的光滑程度。如果我们在设置Metallic属性时使用的是一张纹理，那么这张纹理的A通道就对应了表面的Smoothness值（此时纹理的GB通道则被忽略）。在我们的例子中，我们把金属材质的Smoothness值设置为相对较大的0.7，表明该金属表面比较光滑，而把塑料材质的Smoothness值设为0.4，表明该塑料表面比较粗糙。\n高光反射工作流使用的面板和上述金属工作流使用的基本相同，只是使用了不同含义的Albedo属性，并使用Specular代替了上述的Metallic属性。在高光反射工作流中，材质的Albedo属性定义了表面的漫反射强度。对于非金属材质，它的值通常仍然是视觉上认为的物体颜色，但对于金属材质，Albedo的值通常非常接近黑色（还记得吗，金属材质几乎不存在次表面散射的现象）。高光反射工作流的Specular属性则定义了表面的高光反射强度。非金属材质通常使用一个灰度值范围在0～55的深灰色来作为Specular值，表明非金属材质的高光反射较弱。金属材质则通常会使用视觉上认为的该金属的颜色作为它的Specular值。Specular属性同样也有一个子属性Smoothness，它定义了从视觉上来看该表面的光滑程度。和上面的金属工作流类似，如果使用了一张纹理来为Specular属性赋值，那么纹理的RGB通道对应了Specular属性值，A通道对应了Smoothness属性值。\n上述材质属性都属于材质面板中的Main Maps部分，除了上述提到的属性外，Main Maps还包含了其他材质属性，例如，切线空间下的法线纹理、遮挡纹理、自发光纹理等。Main Maps部分的下面还有一个Secondary Maps的属性部分，这个部分的属性是用来定义额外的细节信息，这些细节通常会直接绘制在Main Maps的上面，来为材质提供更多的微表面或细节表现。\n除了上述属性，我们还可以为Standard Shader选择它使用的渲染模式，即材质面板上的Render Mode选项。Standard Shader支持4种渲染模式，分别是Opaque、Cutout、Fade和Transparent。其中，Opaque用于渲染最常见的不透明物体，这也是默认的渲染模式。对于像玻璃这样的材质，我们可以选择Transparent模式，在这个渲染模式下，Albedo属性的A通道用于控制材质的透明度。而在Cutout渲染模式下，Albedo属性中纹理的A通道会成为一个掩码纹理，而它的子属性Alpha Cutoff将是透明度测试时使用的阈值。Fade模式和Transparent模式是类似的，不同的是：在Transparent模式下，当材质的透明值不断降低时，它的反射仍然能被保留，而在Fade模式下，该材质的所有渲染效果都会逐渐从屏幕上淡出。\n需要注意的是，尽管Standard Shader的材质面板有许多可供调节的属性，但我们不用担心由于没有使用一些属性而会对性能有所影响。Unity在背后已经进行了高度优化，在我们生成可执行程序时，Unity会检查哪些属性没有被使用到，同时也会针对目标平台进行相应的优化。\n从上面的内容可以看出，要想得到可信度更高的渲染结果，我们需要对不同材质使用合适的属性值，尤其是一些重要的属性值，例如Albedo、Metallic和Specular。当然，想要让整个场景的渲染结果令人满意，尤其包含了复杂光照的场景，仅仅有这些使用了PBS的材质是不够的，我们需要使用Unity提供的其他一些重要的技术，例如HDR格式的Skybox、全局光照、反射探针、光照探针、HDR和屏幕后处理等。\n18.3 一个更加复杂的例子 在本章最后，我们将以一个更加复杂的、基于物理渲染的场景结束，该场景对应了本书资源中的Scene_18_3。本场景使用的元素大多来源于Unity官方的示例项目Viking Village（https://www.assetstore.unity3d.com/jp/#!/content/29140），读者可以下载完整的项目来更加深入地学习Unity中的PBS。\n图18.10展示了在不同光照条件下本例实现的效果。需要注意的是，读者需要在Edit→Project Settings→Player→Color Space中选择Linear才可以得到和图18.9中相同的效果，这是因为基于物理的渲染需要使用线性空间（详见18.3.4节）来进行相关计算。\n那么，基于物理的Standard Shader是如何与其他Unity功能相互配合得到这样的场景呢？\n18.3.1 设置光照环境 我们首先需要为场景设置光照环境。在默认情况下，Unity5中一个新创建的场景会包含一个默认的Skybox。在本例中，我们使用一个自定义的Skybox来代替默认值。做法是，打开Window→Lighting，在Scene标签页下把本例使用的SunsetSkyboxHDR拖曳到Skybox选项中，如图18.11所示。\n本例中的Skybox使用了一个HDR格式的Cubemap，这与我们之前在10.1节中制作Skybox时使用的纹理不同。这需要解释HDR（High Dynamic Range）的相关知识，我们将在18.4.3节更加详细地介绍HDR的原理和应用。但在这里，我们只需要知道，使用HDR格式的Skybox可以让场景中物体的反射更加真实，有利于我们得到更加可信的光照效果。\n我们还可以设置场景使用的环境光照，这些环境光照可以对场景中所有的物体表面产生影响，在图18.11所示的设置面板中，我们可以选择环境光照的来源（Ambient Source选项），是来自于场景使用的Skybox，还是使用渐变值，亦或是某个固定的颜色。我们还可以设置环境光照的强度（Ambient Intensity参数），如果想要场景中的所有物体不接受任何环境光照，可以把该值设为0。在使用了Standard Shader的前提下，如果我们关闭场景中所有的光源，并把环境光照的强度设为0，场景中的物体仍然可以接受一些光照，如图18.12中的左图所示。\n那么，这些光照是从哪里来的呢？答案就是反射。默认的反射源（Reflection Source选项）是场景使用的Skybox。如果我们不想让场景中的物体接受任何默认的反射光照，可以把反射源设置为自定义（即Custom），并把自定义的Cubemap保留为空即可（另一种方式是直接把场景使用的Skybox设为空），如图18.12右图所示。但为了得到更加逼真的渲染结果，我们通常是不会这样做的。在渲染实现上，即便场景中没有任何光源，Unity在内部仍然会调用ForwardBase Pass（假设使用的是前向渲染路径的话），并使用反射的光照信息来填充光源信息，再进行基于物理的渲染计算。读者可以通过帧调试器（Frame Debugger）来查看渲染过程。需要注意的是，这里设置的反射源是默认的反射源，如果我们在场景中添加了其他反射探针（Reflection Probes，见18.3.2节），物体可能会使用其他反射源。当默认反射源是Skybox时，Unity会由场景使用的Skybox生成一个Cubemap，我们可以通过Resolution选项来控制它每个面的分辨率。\n除了Standard Shader外，Unity还引入了一个重要的流水线——实时全局光照（Global Illumination，GI）流水线。使用GI，场景中的物体不仅可以受直接光照的影响，还可以接受间接光照的影响。直接光照指的是那些直接把光照射到物体表面的光源，在本书之前的章节中，我们使用的都是直接光照来渲染场景中的物体。但在现实生活中，物体还会受到间接光照的影响。例如，想象一个红色墙壁旁边放置了一个球体，尽管墙壁本身不发光，但球体靠近墙的一面仍会有少许的红色，这是由于红色墙壁把一些间接光照投射到了球体上。在Unity中，间接光照指的就是那些被场景中其他物体反弹的光，这些间接光照会受反弹光的表面的颜色影响（例如之前例子中的红色的墙壁），这些表面会在反弹光线时把自身表面的颜色添加到反射光的计算中。在Unity5中，我们可以使用这些直接光照和间接光照来创建更加真实的视觉效果。\n下面，我们首先设置场景使用的直接光照——一个平行光。在PBR（Physically Based Rendering）中，想要让渲染效果更加真实可信，我们需要保证平行光的方向和Skybox中的太阳或其他光源的位置一致，使得物体产生的光照信息可以与Skybox互相吻合。有时，我们可能会使用一张耀斑纹理（Flare Texture）来模拟太阳等光源，此时我们同样需要确保平行光的方向与耀斑纹理的位置一致。与之类似的还有平行光的颜色，我们应该尽量让平行光的颜色和场景环境相匹配。例如，在图18.10的左图中，场景的光照环境为日落时分，因此平行光的颜色为浅黄色，如图18.13所示，而在图18.10的右图中，场景的光照环境更接近傍晚，此时平行光的颜色为淡蓝色。我们还在Skybox的材质面板上调整天空的旋转角度及曝光度，来调整场景的背景。\n在平行光面板的烘焙选项（即Baking）中，我们选择了Realtime模式，这意味着，场景中受平行光影响的所有物体都会进行实时的光照计算，当光源或场景中其他物体的位置、旋转角度等发生变化时，场景中的光照结果也会随之变化。然而，实时光照往往需要较大的性能消耗，对于移动平台这样资源比较短缺的平台，我们可以选择Baked模式，此时，Unity会把该光源的光照效果烘焙到一张光照纹理（lightmap）中，这样我们就不用实时为物体计算复杂的光照，而只需要通过纹理采样来得到光照结果。选择烘焙模式的缺点在于，如果场景中的物体发生了移动，但是它的阴影等光照效果并不会发生变化。烘焙选项中的Mixed模式则允许我们混合使用实时模式和烘焙模式，它会把场景中的静态物体（即那些被标识为Static的物体）的光照烘焙到光照纹理中，但仍然会对动态物体产生实时光照。\nUnity5引入了实时间接光照的功能，在这个系统下，场景中的直接光照会在场景中各个物体之间来回反射，产生间接光照。正如我们之前讲到的，间接光照可以让那些没有直接被光源照亮的物体同样可以接受到一定的光照信息，这些光照是由它周围的物体反射到它的表面上的。当一条光线从光源被发射出来后，它会与场景中的一些物体相交，第一个和光线相交的物体受到的光照即为直接光照。当得到直接光照在该物体上的光照结果后，该物体还会继续反射该光线，从而对其他物体产生间接光照。此后与该光线相交的物体，就会受到间接光照的影响，同时它们也会继续反射。当经过多次反射后，该光线最后完全消失。这些间接光照的强度是由GI系统计算得到的默认亮度值。图18.13所示的光源面板中的Bounce Intensity参数可以让我们调节这些间接光照的强度。当我们把它设为0时，意味着一条光线仅会和一个物体相交，不再被继续反射，也就是说，场景中的物体只会受到直接光照的影响。图18.14显示了Bounce Intensity分别为0和8时，场景的渲染结果，注意其中阴影部分的细节。\n除了上述调整单个光源的间接光照强度，我们也可以对整个场景的间接光照强度进行调整。这是按照图18.11所示的光照面板来实现的。在光照面板的Scene标签页下，我们可以调整General GI参数块中的Bounce Boost参数来控制场景中反射的间接光照的强度，它会和单个光源的Bounce Intensity参数一起来控制间接光照的反射强度。除此之外，把Indirect Intensity参数调大同样可以增大间接光照的强度。需要注意的是，间接光照还有可能来自一些自发光的物体。\n18.3.2 放置反射探针 回忆我们在10.1节中讲到的环境映射，在实时渲染中，我们经常会使用Cubemap来模拟物体的反射效果。例如，在赛车游戏中，我们需要对车身或车窗使用反射映射的技术来模拟它们的反光材质。然而，如果我们永远使用同一个Cubemap，那么，当赛车周围的场景发生较大变化时，就很容易出现“穿帮镜头”，因为车身或车窗的环境反射并没有随环境变化而发生变化。一种解决办法是可以在脚本中控制何时生成从当前位置观察到的Cubemap，而Unity5为我们提供了一种更加方便的途径，即使用反射探针（Reflection Probes）。反射探针的工作原理和光照探针（Light Probes）类似，它允许我们在场景中的特定位置上对整个场景的环境反射进行采样，并把采样结果存储在每个探针上。当游戏中包含反射效果的物体从这些探针附近经过时，Unity会把从这些邻近探针存储的反射结果传递给物体使用的反射纹理。如果物体周围存在多个反射探针，Unity还会在这些反射结果之间进行插值，来得到平滑渐变的反射效果。实际上，Unity会在场景中放置一个默认的反射探针，这个反射探针存储了对场景使用的Skybox的反射结果，来作为场景的环境光照（见18.3.1节）。如果我们需要让场景中的物体包含额外的反射效果，就需要放置更多的反射探针。\n反射探针同样有3种类型：Baked，这种类型的反射探针是通过提前烘焙来得到该位置使用的Cubemap的，在游戏运行时反射探针中存储的Cubemap并不会发生变化。需要注意的是，这种类型的反射探针在烘焙的同样只会处理那些静态物体（即那些被标识为ReflectionProbeStatic的物体）；Realtime，这种类型则会实时更新当前的Cubemap，并且不受静态物体还是动态物体的影响。当然，这种类型的反射探针需要花费更多的处理时间，因此，在使用时应当非常小心它的性能。幸运的是，Unity允许我们从脚本中通过触发来精确控制反射探针的更新；最后一种类型是Custom，这种类型的探针既可以让我们从编辑器中烘焙它，也可以让我们使用一个自定义的Cubemap来作为反射映射，但自定义的Cubemap不会被实时更新。\n我们在本节使用的场景中放置了3个反射探针，它们的类型都是Baked（前提是我们把场景中的物体标识成了Static）。使用反射探针前后的对比效果如图18.15所示。\n需要注意的是，在放置反射探针时，我们选取的位置并不是任意的。通常来说，反射探针应该被放置在那些具有明显反射现象的物体的旁边，或是一些墙角等容易发生遮挡的物体周围。在本例使用的场景中，木屋内的盾牌具有比较明显的反射效果，而盾牌本身又被木屋遮挡，因此，其中一个反射探针的位置就在盾牌附近。当我们放置好探针后，我们还需要为它们定义每个探针的影响区域，当反射物体进入这个区域后，反射探针就会对物体的反射产生影响。通常情况下，反射探针的影响区域之间往往会有所重叠，例如，本例中盾牌附近的反射探针和另外两个（一个在木屋前方，一个在木屋后方）的影响区域都有所重叠。此时，Unity会计算反射物体的包围盒与这些重叠区域的交叉部分，并据此来选择使用的反射映射。如果当前的目标平台使用的是SM3.0及以上的话，Unity还可以允许我们在这些互相重叠的反射探针之间进行混合，来实现平缓的反射过渡效果。\n使用Unity内置的反射探针的另一个好处是，我们可以模拟互相反射（interreflections）。我们曾在10.1节中讲到使用传统的Cubemap方法无法模拟互相反射的效果。例如，假设场景中有两面互相面对面的镜子，在理想情况下，它们不仅会反射自己对面的那面镜子，也会反射那面镜子里反射的图像。只要反射光线没有被完全吸收，反射就会一直进行下去。要实现这种效果，就需要追踪光线的反射轨迹，这是传统的反射方法所无法实现的。Unity5引入的GI系统让这种效果变成了可能，我们在本书资源的Scene_18_3_2场景中展示了这样的一个例子，如图18.16所示。\n我们可以在图18.16中看到，两个金属反射的图像包含了两次互相反射的效果。在图18.16所示的场景中，我们在每个金属球的位置处放置了一个反射探针，并把每个金属球上的MeshRenderer组件中的Reflection Probes设置为Simple，这样保证它们只会使用离它们最近的一个反射探针。默认情况下，反射探针只会捕捉一次反射，也就是说，左边金属球使用的反射探针只会捕捉到由右边的金属球第一次反射过来的光线。但在理想情况下，反射过来的光线会继续被左边的金属球反射，并对右边的金属球造成影响。Unity允许我们控制物体之间这样来回反射的次数，这可以通过改变图18.11中的Reflection Bounces参数来实现。在图18.16所示的场景中，我们把该值设为了2。\n然而，正如本节一开始所提到的，使用反射探针往往会需要更多的计算时间。这些探针实际上也是通过在它的位置上放置一个摄像机，来渲染得到一个Cubemap。如果我们把反弹次数设置的很大，或是使用实时渲染，那么这些探针很可能会造成性能瓶颈。更多关于如何优化反射探针以及它的高级用法，读者可以参见Unity的官方手册（http://docs.unity3d.com/Manual/ReflectionProbes.html）。\n18.3.3 调整材质 要得到真实可信的渲染效果，我们需要为场景中的物体指定合适的材质。需要再次提醒读者的是，基于物理的渲染并不意味着一定要模拟像照片真实的效果。基于物理的渲染更多的好处在于，可以让我们的场景在各种光照条件下都能得到令人满意的效果，同时不需要频繁地调整材质参数。\n在Unity中，要想和全局光照、反射探针等内置功能良好地配合来得到出色的渲染结果，就需要使用Unity内置的Standard Shader。我们已经在18.2.2节中学习了如何针对不同类别的物体来调整它们使用的材质属性。在本例中，我们使用了更复杂的纹理和模型，它们都来自于Unity官方的示例项目Viking Village。这些材质可以为读者制作自己的材质提供一些参考，例如，场景中所有物体都使用了高光反射纹理（Specular Texture）、遮挡纹理（Occlusion Texture）、法线纹理（Normal Texture），一些材质还使用了细节纹理来提供更多的细节表现。\n18.3.4 线性空间 在使用基于物理的渲染方法渲染整个场景时，我们应该使用线性空间（Linear Space）来得到最好的渲染效果。默认情况下，Unity会使用伽马空间（Gamma Space），如果要使用线性空间的话，我们需要在Edit→Project Settings→Player→Other Settings→Color Space中选择Linear选项。\n图18.17显示了分别在线性空间和伽马空间下场景的渲染结果。从图18.17中可以看出，使用线性空间可以得到更加真实的效果。但它的缺点在于，需要一些硬件支持来实现线性计算，但一些移动平台对它的支持并不好。这种情况下，我们往往只能退而求其次，选择伽马空间进行渲染和计算。\n那么，线性空间、伽马空间到底是什么意思？为什么线性空间可以得到更加真实的效果呢？这就需要介绍伽马校正（Gamma Correction）的相关内容了。实际上，当我们在默认的伽马空间下进行渲染计算时，由于使用了非线性的输入数据，导致很多计算都是在非线性空间下进行的，这意味着我们得到的结果并不符合真实的物理期望。除此之外，由于输出时没有考虑显示器的显示伽马的影响，会导致渲染出来的画面整体偏暗，总是和真实世界不像。\n尽管在Unity中我们可以通过之前所说的步骤直接选择在线性空间进行渲染，Unity会在背后为我们照顾好一切，但了解伽马校正的原理对我们理解渲染计算有很大帮助，读者可以在18.4.2节找到更多的解释。\n18.4 答疑解惑 在上面的内容中，我们首先介绍了PBS实现的数学和理论基础，并简单概括了Unity中Standard Shader的实现原理，以及如何使用它来为不同类型的物体调整适合它们的材质参数。随后，我们通过一个更加复杂的场景，来展示如何在Unity中使用环境光照、实时光源、反射探针以及Standard Shader来渲染一个基于物理渲染的场景。但我们相信，读者在读完后仍有很多困惑，考虑到内容的连贯性，我们未能在文中对某些概念进行展开。在本节中，我们将对一些重要的概念进行更为深入地解释。\n18.4.1 什么是全局光照 在上面的内容中，我们可以发现全局光照对得到真实的渲染结果有着举足轻重的作用。全局光照，指的就是模拟光线是如何在场景中传播的，它不仅会考虑那些直接光照的结果，还会计算光线被不同的物体表面反射而产生的间接光照。在使用基于物理的着色技术时，当渲染表面某一点时，我们需要计算该点的半球范围内所有会反射到观察方向的入射光线的光照结果，这些入射光线中就包含了直接光照和间接光照。\n通常来讲，这些间接光照的计算是非常耗时间的，通常不会用在实时渲染中。一个传统的方法是使用光线追踪，来追踪场景中每一条重要的光线的传播路径。使用光线追踪能得到非常出色\n","date":"2024-09-29T12:32:30Z","image":"https://Selaphina.github.io/p/18-%E5%9F%BA%E4%BA%8E%E7%89%A9%E7%90%86%E7%9A%84%E6%B8%B2%E6%9F%93/cover1_hu_5e2014920a4d8f6.png","permalink":"https://Selaphina.github.io/p/18-%E5%9F%BA%E4%BA%8E%E7%89%A9%E7%90%86%E7%9A%84%E6%B8%B2%E6%9F%93/","title":"18 基于物理的渲染"},{"content":"第17章 Unity的表面着色器探秘 在2009年的时候（当时Unity的版本是2.x），Unity的渲染工程师Aras（就是经常活跃在论坛和各种会议上的，大名鼎鼎的Aras Pranckevicius）连续发表了3篇名为《Shaders must die》的博客。在这些博客里，Aras认为，把渲染流程分为顶点和像素的抽象层面是错误的，是一种不易理解的抽象。目前，这种在顶点/几何/片元着色器上的操作是对硬件友好的一种方式，但不符合我们人类的思考方式。相反，他认为，应该划分成表面着色器、光照模型和光照着色器这样的层面。其中，表面着色器定义了模型表面的反射率、法线和高光等，光照模型则选择是使用兰伯特还是Blinn-Phong等模型。而光照着色器负责计算光照衰减、阴影等。这样，绝大部分时间我们只需要和表面着色器打交道，例如，混合纹理和颜色等。光照模型可以是提前定义好的，我们只需要选择哪种预定义的光照模型即可。而光照着色器一旦由系统实现后，更不会被轻易改动，从而大大减轻了Shader编写者的工作量。有了这样的想法，Aras在随后的文章中开始尝试把表面着色器整合到Unity中。最终，在2010年的Unity3中，Surface Shader被加入到Unity的大家族中了。\n虽然Unity换了一个新的“马甲”，但表面着色器（Surface Shader）实际上就是在顶点/片元着色器之上又添加了一层抽象。按Aras的话来解释就是，顶点/几何/片元着色器是硬件能“理解”的渲染方式，而开发者应该使用一种更容易理解的方式。很多时候，使用表面着色器，我们只需要告诉Shader：“嘿，使用这些纹理去填充颜色，使用这个法线纹理去填充表面法线，使用兰伯特光照模型，其他的就不要来烦我了！”我们不需要考虑是使用前向渲染路径还是延迟渲染路径，场景中有多少光源，它们的类型是什么，怎样处理这些光源，每个Pass需要处理多少个光源等问题（正是因为有这些事情，人们总会抱怨写一个Shader是多么的麻烦……………）。这时，Unity说：“不要急，我来干！”\n那么，表面着色器到底长什么样呢？它们又是如何工作的呢？这正是本章要学习的内容。\n表面着色器的一个例子 在学习原理之前，我们首先来看一下一个表面着色器长什么样子。为此，我们需要做如下的准备工作。\n在Unity中新创建一个场景。在本书资源中，该场景名为Scene_17_1。在Unity5.2中，默认情况下场景将包含一个摄像机和一个平行光，并且使用了内置的天空盒子。在Window→Lighting→Skybox中去掉场景中的天空盒子。 新创建一个材质。在本书资源中，该材质名为BumpedSpecularMat。 新创建一个Unity Shader。在本书资源中，该Unity Shader名为Chapter17-BumpedDiffuse，把新的Unity Shader赋给第2步中创建的材质。 在场景中创建一个胶囊体（capsule），并把第2步中的材质赋给该胶囊体。 保存场景。 我们将使用表面着色器来实现一个使用了法线纹理的漫反射效果。这可以参考Unity内置的“Legacy Shaders/BumpedDiffuse”的代码实现（可以在官方网站的内置Shader包中找到）。打开Chapter17-BumpedDiffuse，删除原有的代码，把下面的代码粘贴进去：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 Shader \u0026#34;Unity Shaders Book/Chapter 17/Bumped Diffuse\u0026#34; { Properties { _Color (\u0026#34;Main Color\u0026#34;, Color) = (1,1,1,1) _MainTex (\u0026#34;Base (RGB)\u0026#34;, 2D) = \u0026#34;white\u0026#34; {} _BumpMap (\u0026#34;Normalmap\u0026#34;, 2D) = \u0026#34;bump\u0026#34; {} } SubShader { Tags { \u0026#34;RenderType\u0026#34;=\u0026#34;Opaque\u0026#34; } LOD 300 CGPROGRAM #pragma surface surf Lambert #pragma target 3.0 sampler2D _MainTex; sampler2D _BumpMap; fixed4 _Color; struct Input { float2 uv_MainTex; float2 uv_BumpMap; }; void surf (Input IN, inout SurfaceOutput o) { fixed4 tex = tex2D(_MainTex, IN.uv_MainTex); o.Albedo = tex.rgb * _Color.rgb; o.Alpha = tex.a * _Color.a; o.Normal = UnpackNormal(tex2D(_BumpMap, IN.uv_BumpMap)); } ENDCG } FallBack \u0026#34;Legacy Shaders/Diffuse\u0026#34; } 保存程序后，返回Unity中查看。在BumpedDiffuseMat的面板上，我们把本书资源中的Assets/Textures/Chapter17/MudDiffuse.tif和Assets/Textures/Chapter17/MudNormal.tif分别拖拽到MainTex和_BumpMap属性上，就可以得到类似图17.1中左图的结果。我们还可以向场景中添加一些点光源和聚光灯，并改变它们的颜色，就可以得到类似图17.1中右图的结果。注意，在这个过程中，我们不需要对代码做任何改动。\n从上面的例子可以看出，相比之前所学的顶点/片元着色器技术，表面着色器的代码量很少（只需要三十多行），图17.1表面着色器的例子左边：在一个平行光下的效果。如果我们使用顶点/片元着色器来实现上述的功能，大概需要150多行代码（参考本书资源中的“UnityShadersBook/Common/BumpedDiffuse”）！而且，我们可以非常轻松地实现常见的光照模型，甚至不需要和任何光照变量打交道，Unity就帮我们处理好了每个光源的光照结果。\n读者可以在Unity官方手册的表面着色器的例子一文（http://docs.unity3d.com/Manual/SL-SurfaceShaderExamples.html）中找到更多的示例程序。下面，我们将具体学习表面着色器的特点和工作原理。\n和顶点/片元着色器需要包含到一个特定的Pass块中不同，表面着色器的CG代码是直接而且也必须写在SubShader块中，Unity会在背后为我们生成多个Pass。当然，可以在SubShader一开始处使用Tags来设置该表面着色器使用的标签。在Chapter17-BumpedDiffuse中，我们还使用LOD命令设置了该表面着色器的LOD值（详见16.8.1节）。然后，我们使用CGPROGRAM和ENDCG定义了表面着色器的具体代码。\n一个表面着色器中最重要的部分是两个结构体以及它的编译指令。其中，两个结构体是表面着色器中不同函数之间信息传递的桥梁，而编译指令是我们和Unity沟通的重要手段。\n编译指令 我们首先来看一下表面着色器的编译指令。编译指令是我们和Unity沟通的重要方式，通过它可以告诉Unity：“嘿，用这个表面函数设置表面属性，用这个光照模型模拟光照，我不要阴影和环境光，不要雾效！”只需要一行代码，我们就可以完成这么多事情！\n编译指令最重要的作用是指明该表面着色器使用的表面函数和光照函数，并设置一些可选参数。表面着色器的CG块中的第一句代码往往就是它的编译指令。编译指令的一般格式如下：\n1 #pragma surface surfaceFunction lightModel [optionalparams] 其中，#pragma surface用于指明该编译指令是用于定义表面着色器的，在它的后面需要指明使用的表面函数（surfaceFunction）和光照模型（lightModel），同时，还可以使用一些可选参数来控制表面着色器的一些行为。\n表面函数 我们之前说过，表面着色器的优点在于抽象出了“表面”这一概念。与之前遇到的顶点/片元抽象层不同，一个对象的表面属性定义了它的反射率、光滑度、透明度等值。而编译指令中的surfaceFunction就用于定义这些表面属性。surfaceFunction通常就是名为surf的函数（函数名可以是任意的），它的函数格式是固定的：\n1 2 3 void surf (Input IN, inout SurfaceOutput o) void surf (Input IN, inout SurfaceOutputStandard o) void surf (Input IN, inout SurfaceOutputStandardSpecular o) 其中，后两个是Unity5中由于引入了基于物理的渲染而新添加的两种结构体。SurfaceOutput、SurfaceOutputStandard和SurfaceOutputStandardSpecular都是Unity内置的结构体，它们需要配合不同的光照模型使用，我们会在下一节进行更详细地解释。\n在表面函数中，会使用输入结构体Input IN来设置各种表面属性，并把这些属性存储在输出结构体SurfaceOutput、SurfaceOutputStandard或SurfaceOutputStandardSpecular中，再传递给光照函数计算光照结果。读者可以在Unity手册中的表面着色器的例子一文（http://docs.unity3d.com/Manual/SL-SurfaceShaderExamples.html）中找到更多的示例表面函数。\n光照函数 除了表面函数，我们还需要指定另一个非常重要的函数——光照函数。光照函数会使用表面函数中设置的各种表面属性，来应用某些光照模型，进而模拟物体表面的光照效果。Unity内置了基于物理的光照模型函数Standard和StandardSpecular（在UnityPBSLighting.cginc文件中被定义），以及简单的非基于物理的光照模型函数Lambert和BlinnPhong（在Lighting.cginc文件中被定义）。例如，在Chapter17-BumpedDiffuse中，我们就指定了使用Lambert光照函数。\n当然，我们也可以定义自己的光照函数。例如，可以使用下面的函数来定义用于前向渲染中的光照函数：\n1 2 3 4 // 用于不依赖视角的光照模型，例如漫反射 half4 Lighting\u0026lt;Name\u0026gt; (SurfaceOutput s, half3 lightDir, half atten) // 用于依赖视角的光照模型，例如高光反射 half4 Lighting\u0026lt;Name\u0026gt; (SurfaceOutput s, half3 lightDir, half3 viewDir, half atten); 读者可以在Unity手册的表面着色器中的自定义光照模型一文（http://docs.unity3d.com/Manual/SL-SurfaceShaderLighting.html）中找到更全面的自定义光照模型的介绍。而一些例子可以参见手册中的表面着色器的光照例子一文（http://docs.unity3d.com/Manual/SL-SurfaceShaderLightingExamples.html)，这篇文档展示了如何使用表面着色器来自定义常见的漫反射、高光反射、基于光照纹理等常用的光照模型。\n其他可选参数 在编译指令的最后，我们还可以设置一些可选参数（optionalparams）。这些可选参数包含了很多非常有用的指令类型，例如，开启/设置透明度混合/透明度测试，指明自定义的顶点和颜色修改函数，控制生成的代码等。下面，我们选取了一些比较重要和常用的参数进行更深入地说明。读者可以在Unity官方手册的编写表面着色器一文（http://docs.unity3d.com/Manual/SL-SurfaceShaders.html）中找到更加详细的参数和设置说明。\n自定义的修改函数。除了表面函数和光照模型外，表面着色器还可以支持其他两种自定义的函数：顶点修改函数（vertex:VertexFunction）和最后的颜色修改函数（finalcolor:ColorFunction）。顶点修改函数允许我们自定义一些顶点属性，例如，把顶点颜色传递给表面函数，或是修改顶点位置，实现某些顶点动画等。最后的颜色修改函数则可以在颜色绘制到屏幕前，最后一次修改颜色值，例如实现自定义的雾效等。 阴影。我们可以通过一些指令来控制和阴影相关的代码。例如，addshadow参数会为表面着色器生成一个阴影投射的Pass。通常情况下，Unity可以直接在FallBack中找到通用的光照模式为ShadowCaster的Pass，从而将物体正确地渲染到深度和阴影纹理中（详见9.4节）。但对于一些进行了顶点动画、透明度测试的物体，我们就需要对阴影的投射进行特殊处理，来为它们产生正确的阴影，正如我们在13.3节中看到的一样。fullforwardshadows参数则可以在前向渲染路径中支持所有光源类型的阴影。默认情况下，Unity只支持最重要的平行光的阴影效果。如果我们需要让点光源或聚光灯在前向渲染中也可以有阴影，就可以添加这个参数。相反地，如果我们不想对使用这个Shader的物体进行任何阴影计算，就可以使用noshadow参数来禁用阴影。 透明度混合和透明度测试。我们可以通过alpha和alphatest指令来控制透明度混合和透明度测试。例如，alphatest:VariableName指令会使用名为VariableName的变量来剔除不满足条件的片元。此时，我们可能还需要使用上面提到的addshadow参数来生成正确的阴影投射的Pass。 光照。一些指令可以控制光照对物体的影响，例如，noambient参数会告诉Unity不要应用任何环境光照或光照探针（light probe）。novertexlights参数告诉Unity不要应用任何逐顶点光照。noforwardadd会去掉所有前向渲染中的额外的Pass。也就是说，这个Shader只会支持一个逐像素的平行光，而其他的光源会按照逐顶点或SH的方法来计算光照影响。这个参数通常会用于移动平台版本的表面着色器中。还有一些用于控制光照烘焙、雾效模拟的参数，如nolightmap、nofog等。 控制代码的生成。一些指令还可以控制由表面着色器自动生成的代码，默认情况下，Unity会为一个表面着色器生成相应的前向渲染路径、延迟渲染路径使用的Pass，这会导致生成的Shader文件比较大。如果我们确定该表面着色器只会在某些渲染路径中使用，就可以exclude_path:deferred、exclude_path:forward和exclude_path:prepass来告诉Unity不需要为某些渲染路径生成代码。 从上述可以看出，表面着色器支持的编译指令参数很多，为我们编写表面着色器提供了很大的方便。之前在顶点/片元着色器中需要耗费大量代码来完成的工作，在表面着色器中可能只需要一个参数就可以了。当然，相比于顶点/片元着色器，表面着色器也有它自身的限制，我们会在17.6节中对比它们的优缺点。\n两个结构体 在上一节我们已经讲过，表面着色器支持最多自定义4种关键的函数：表面函数（用于设置各种表面性质，如反射率、法线等），光照函数（定义表面使用的光照模型），顶点修改函数（修改或传递顶点属性），最后的颜色修改函数（对最后的颜色进行修改）。那么，这些函数之间的信息传递是怎么实现的呢？例如，我们想把顶点颜色传递给表面函数，添加到表面反射率的计算中，要怎么做呢？这就是两个结构体的工作。\n一个表面着色器需要使用两个结构体：表面函数的输入结构体Input，以及存储了表面属性的结构体SurfaceOutput（Unity5新引入了另外两个同种的结构体SurfaceOutputStandard和SurfaceOutputStandardSpecular）。\n数据来源：Input结构体 Input结构体包含了许多表面属性的数据来源，因此，它会作为表面函数的输入结构体（如果自定义了顶点修改函数，它还会是顶点修改函数的输出结构体）。Input支持很多内置的变量名，通过这些变量名，我们告诉Unity需要使用的数据信息。例如，在Chapter17-BumpedDiffuse中，Input结构体中包含了主纹理和法线纹理的采样坐标uv_MainTex和uv_BumpMap。这些采样坐标必须以“uv”为前缀（实际上也可用“uv2”为前缀，表明使用次纹理坐标集合），后面紧跟纹理名称。以主纹理_MainTex为例，如果需要使用它的采样坐标，就需要在Input结构体中声明float2 uv_MainTex来对应它的采样坐标。表17.1列出了Input结构体中内置的其他变量。\n表17.1\n变量 描述 float3 viewDir 包含了视角方向，可用于计算边缘光照等 float4 COLOR 使用COLOR语义定义的float4变量，包含了插值后的逐顶点颜色 float4 screenPos 包含了屏幕空间的坐标，可以用于反射或屏幕特效 float3 worldPos 包含了世界空间下的位置 float3 worldRefl 包含了世界空间下的反射方向。前提是没有修改表面法线o.Normal。如果修改了表面法线o.Normal，需要使用该变量告诉Unity要基于修改后的法线计算世界空间下的反射方向。在表面函数中，我们需要使用WorldReflectionVector(IN, o.Normal)来得到世界空间下的反射方向 float3 worldNormal 包含了世界空间的法线方向。前提是没有修改表面法线o.Normal。如果修改了表面法线o.Normal，需要使用该变量告诉Unity要基于修改后的法线计算世界空间下的法线方向。在表面函数中，我们需要使用WorldNormalVector(IN, o.Normal)来得到世界空间下的法线方向 需要注意的是，我们并不需要自己计算上述的各个变量，而只需要在Input结构体中按上述名称严格声明这些变量即可，Unity会在背后为我们准备好这些数据，而我们只需要在表面函数中直接使用它们即可。一个例外情况是，我们自定义了顶点修改函数，并需要向表面函数中传递一些自定义的数据。例如，为了自定义雾效，我们可能需要在顶点修改函数中根据顶点在视角空间下的位置信息计算雾效混合系数，这样我们就可以在Input结构体中定义一个名为half fog的变量，把计算结果存储在该变量后进行输出。\n表面属性：SurfaceOutput结构体 有了Input结构体来提供所需要的数据后，我们就可以据此计算各种表面属性。因此，另一个结构体就是用于存储这些表面属性的结构体，即SurfaceOutput、SurfaceOutputStandard和SurfaceOutputStandardSpecular，它会作为表面函数的输出，随后会作为光照函数的输入来进行各种光照计算。相比于Input结构体的自由性，这个结构体里面的变量是提前就声明好的，不可以增加也不会减少（如果没有对某些变量赋值，就会使用默认值）。SurfaceOutput的声明可以在Lighting.cginc文件中找到：\n1 2 3 4 5 6 7 8 struct SurfaceOutput { fixed3 Albedo; fixed3 Normal; fixed3 Emission; half Specular; fixed Gloss; fixed Alpha; }; 而SurfaceOutputStandard和SurfaceOutputStandardSpecular的声明可以在UnityPBSLighting.cginc中找到：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 struct SurfaceOutputStandard { fixed3 Albedo; // base (diffuse or specular) color fixed3 Normal; // tangent space normal, if written half3 Emission; half Metallic; // 0=non-metal, 1=metal half Smoothness; // 0=rough, 1=smooth half Occlusion; // occlusion (default 1) fixed Alpha; // alpha for transparencies }; struct SurfaceOutputStandardSpecular { fixed3 Albedo; // diffuse color fixed3 Specular; // specular color fixed3 Normal; // tangent space normal, if written half3 Emission; half Smoothness; // 0=rough, 1=smooth half Occlusion; // occlusion (default 1) fixed Alpha; // alpha for transparencies }; 在一个表面着色器中，只需要选择上述三者中的其一即可，这取决于我们选择使用的光照模型。Unity内置的光照模型有两种，一种是Unity5之前的、简单的、非基于物理的光照模型，包括了Lambert和BlinnPhong；另一种是Unity5添加的、基于物理的光照模型，包括Standard和StandardSpecular，这种模型会更加符合物理规律，但计算也会复杂很多。如果使用了非基于物理的光照模型，我们通常会使用SurfaceOutput结构体，而如果使用了基于物理的光照模型Standard或StandardSpecular，我们会分别使用SurfaceOutputStandard或SurfaceOutputStandardSpecular结构体。其中，SurfaceOutputStandard结构体用于默认的金属工作流程（Metallic Workflow），对应了Standard光照函数；而SurfaceOutputStandardSpecular结构体用于高光工作流程（Specular Workflow），对应了StandardSpecular光照函数。更多关于基于物理的渲染内容，我们会在第18章中讲到。\n在本节，我们着重介绍一下SurfaceOutput结构体中的变量和含义。在表面函数中，我们需要根据Input结构体传递的各个变量计算表面属性。在SurfaceOutput结构体，这些表面属性包括了：\nfixed3 Albedo：对光源的反射率。通常由纹理采样和颜色属性的乘积计算而得。 fixed3 Normal：表面法线方向。 fixed3 Emission：自发光。Unity通常会在片元着色器最后输出前（如果在最后的顶点函数被调用前，如果定义了的话），使用类似下面的语句进行简单的颜色叠加：c.rgb += o.Emission; half Specular：高光反射中的指数部分的系数，影响高光反射的计算。例如，如果使用了内置的BlinnPhong光照函数，它会使用如下语句计算高光反射的强度：float spec = pow (nh, s.Specular*128.0) * s.Gloss; fixed Gloss：高光反射中的强度系数。和上面的Specular类似，计算公式见上面的代码。一般在包含了高光反射的光照模型里使用。 fixed Alpha：透明通道。如果开启了透明度的话，会使用该值进行颜色混合。 尽管表面着色器极大地减少了我们的工作量，但它带来的一个问题是，我们经常不知道为什么会得到这样的渲染结果。如果你不是一个“好奇宝宝”的话，你可以高高兴兴地使用表面着色器来方便地实现一些不错的渲染效果。但是，一些好奇的初学者往往会提出这样的问题：“为什么我的场景里没有灯光，但物体不是全黑的呢？为什么我把光源的颜色调成黑色，物体还是有一些渲染颜色呢？”这些问题都源于表面着色器对我们隐藏了实现细节。而想要更加得心应手地使用表面着色器，我们就需要学习它的工作流水线，并了解Unity是如何为一个表面着色器生成对应的顶点/片元着色器的（时刻记着，表面着色器本质上就是包含了很多Pass的顶点/片元着色器）。\nUnity背后做了什么 在前面的内容中，我们已经了解到如何利用编译指令、自定义函数（表面函数、光照函数，以及可选的顶点修改函数和最后的颜色修改函数）和两个结构体来实现一个表面着色器。我们一直强调，Unity实际会在背后为表面着色器生成真正的顶点/片元着色器。那么，表面着色器中的各个函数、编译指令和结构体与顶点/片元着色器之间有什么关系呢？这正是本节要学习的内容。\n我们之前说过，Unity在背后会根据表面着色器生成一个包含了很多Pass的顶点/片元着色器。这些Pass有些是为了针对不同的渲染路径，例如，默认情况下Unity会为前向渲染路径生成LightMode为ForwardBase和ForwardAdd的Pass，为Unity5之前的延迟渲染路径生成LightMode为PrePassBase和PrePassFinal的Pass，为Unity5之后的延迟渲染路径生成LightMode为Deferred的Pass。还有一些Pass是用于产生额外的信息，例如，为了给光照映射和动态全局光照提取表面信息，Unity会生成一个LightMode为Meta的Pass。有些表面着色器由于修改了顶点位置，因此，我们可以利用addshadow编译指令为它生成相应的LightMode为ShadowCaster的阴影投射Pass。这些Pass的生成都是基于我们在表面着色器中的编译指令和自定义的函数，这是有规律可循的。\nUnity提供了一个功能，让那些“好奇宝宝”可以对表面着色器自动生成的代码一探究竟：在每个编译完成的表面着色器的面板上，都有一个“Show generated code”的按钮，如图17.2所示，我们只需要单击一下它就可以看到Unity为这个表面着色器生成的所有顶点/片元着色器。\n通过查看这些代码，我们就可以了解到Unity到底是如何根据表面着色器生成各个Pass的。以Unity生成的LightMode为ForwardBase的Pass（用于前向渲染）为例，它的渲染计算流水线如图17.3所示。从图17.3中我们可以看出，4个允许自定义的函数在流水线中的位置。\nUnity对该Pass的自动生成过程大致如下。\n直接将表面着色器中CGPROGRAM和ENDCG之间的代码复制过来，这些代码包括了我们对Input结构体、表面函数、光照函数（如果自定义了的话）等变量和函数的定义。这些函数和变量会在之后的处理过程中被当成正常的结构体和函数进行调用。 Unity会分析上述代码，并据此生成顶点着色器的输出——v2f_surf结构体，用于在顶点着色器和片元着色器之间进行数据传递。Unity会分析我们在自定义函数中所使用的变量，例如，纹理坐标、视角方向、反射方向等。如果需要，它就会在v2f_surf中生成相应的变量。而且，即使有时我们在Input中定义了某些变量（如某些纹理坐标），但Unity在分析后续代码时发现我们并没有使用这些变量，那么这些变量实际上是不会在v2f_surf中生成的。这也就是说，Unity做了一些优化。v2f_surf中还包含了一些其他需要的变量，例如阴影纹理坐标、光照纹理坐标、逐顶点光照等。 接着，生成顶点着色器。 如果我们自定义了顶点修改函数，Unity会首先调用顶点修改函数来修改顶点数据，或填充自定义的Input结构体中的变量。然后，Unity会分析顶点修改函数中修改的数据，在需要时通过Input结构体将修改结果存储到v2f_surf相应的变量中。 计算v2f_surf中其他生成的变量值。这主要包括了顶点位置、纹理坐标、法线方向、逐顶点光照、光照纹理的采样坐标等。当然，我们可以通过编译指令来控制某些变量是否需要计算。 最后，将v2f_surf传递给接下来的片元着色器。 生成片元着色器。 使用v2f_surf中的对应变量填充Input结构体，例如，纹理坐标、视角方向等。 调用我们自定义的表面函数填充SurfaceOutput结构体。 调用光照函数得到初始的颜色值。如果使用的是内置的Lambert或BlinnPhong光照函数，Unity还会计算动态全局光照，并添加到光照模型的计算中。 进行其他的颜色叠加。例如，如果没有使用光照烘焙，还会添加逐顶点光照的影响。 最后，如果自定义了最后的颜色修改函数，Unity就会调用它进行最后的颜色修改。 其他Pass的生成过程和上面类似，在此不再赘述。\n表面着色器实例分析 为了帮助读者更加深入地理解表面着色器背后的原理，我们在本节以一个表面着色器为例，分析Unity为它生成的代码。\n读者可以在本书资源中的Scene_17_4中找到相应的测试场景。它实现的效果是对模型进行膨胀，如图17.4所示。这种效果的实现非常简单，就是在顶点修改函数中沿着顶点法线方向扩张顶点位置。为了分析表面着色器中4个允许自定义函数（顶点修改函数、表面函数、光照函数和最后的颜色修改函数）的原理，在本例中我们对这4个函数全部采用了自定义的实现。读者可以在Chapter17-NormalExtrusion文件中找到该表面着色器，它的代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 Shader \u0026#34;Unity Shaders Book/Chapter 17/Normal Extrusion\u0026#34; { Properties { _ColorTint (\u0026#34;Color Tint\u0026#34;, Color) = (1,1,1,1) _MainTex (\u0026#34;Base (RGB)\u0026#34;, 2D) = \u0026#34;white\u0026#34; {} _BumpMap (\u0026#34;Normalmap\u0026#34;, 2D) = \u0026#34;bump\u0026#34; {} _Amount (\u0026#34;Extrusion Amount\u0026#34;, Range(-0.5, 0.5)) = 0.1 } SubShader { Tags { \u0026#34;RenderType\u0026#34;=\u0026#34;Opaque\u0026#34; } LOD 300 CGPROGRAM // surf - which surface function. // CustomLambert - which lighting model to use. // vertex:myvert - use custom vertex modification function. // finalcolor:mycolor - use custom final color modification function // addshadow - generate a shadow caster pass. Because we modify the vertex position, // the shader needs special shadows handling. // exclude_path:deferred / exclude_path:prepass - do not generate passes for // deferred / legacy deferred rendering path. // nometa - do not generate a \u0026#34;meta\u0026#34; pass (that\u0026#39;s used by lightmapping \u0026amp; dynamic // global illumination to extract surface information). #pragma surface surf CustomLambert vertex:myvert finalcolor:mycolor addshadow exclude_path:deferred exclude_path:prepass nometa #pragma target 3.0 fixed4 _ColorTint; sampler2D _MainTex; sampler2D _BumpMap; half _Amount; struct Input { float2 uv_MainTex; float2 uv_BumpMap; }; void myvert (inout appdata_full v) { v.vertex.xyz += v.normal * _Amount; } void surf (Input IN, inout SurfaceOutput o) { fixed4 tex = tex2D(_MainTex, IN.uv_MainTex); o.Albedo = tex.rgb; o.Alpha = tex.a; o.Normal = UnpackNormal(tex2D(_BumpMap, IN.uv_BumpMap)); } half4 LightingCustomLambert (SurfaceOutput s, half3 lightDir, half atten) { half NdotL = dot(s.Normal, lightDir); half4 c; c.rgb = s.Albedo * _LightColor0.rgb * (NdotL * atten); c.a = s.Alpha; return c; } void mycolor (Input IN, SurfaceOutput o, inout fixed4 color) { color *= _ColorTint; } ENDCG } FallBack \u0026#34;Legacy Shaders/Diffuse\u0026#34; } 在顶点修改函数中，我们使用顶点法线对顶点位置进行膨胀；表面函数使用主纹理设置了表面属性中的反射率，并使用法线纹理设置了表面法线方向；光照函数实现了简单的Lambert漫反射光照模型；在最后的颜色修改函数中，我们简单地使用了颜色参数对输出颜色进行调整。注意，除了4个函数外，我们在#pragma surface的编译指令一行中还指定了一些额外的参数。由于我们修改了顶点位置，因此，要对其他物体产生正确的阴影效果并不能直接依赖FallBack中找到的阴影投射Pass，addshadow参数可以告诉Unity要生成一个该表面着色器对应的阴影投射Pass。默认情况下，Unity会为所有支持的渲染路径生成相应的Pass，为了缩小自动生成的代码量，我们使用exclude_path:deferred和exclude_path:prepass来告诉Unity不要为延迟渲染路径生成相应的Pass。最后，我们使用nometa参数取消对提取元数据的Pass的生成。\n当在该表面着色器的导入面板中单击“Show generated code”按钮后，我们就可以看到Unity生成的顶点/片元着色器了。由于代码比较多，为了节省篇幅我们不再把全部代码粘贴到这里。因此，在往下阅读之前，请读者先打开生成的代码文件，以便明白我们接下来的分析。\n在这个将近600行代码的文件中，Unity一共为该表面着色器生成了3个Pass，它们的LightMode分别是ForwardBase、ForwardAdd和ShadowCaster，分别对应了前向渲染路径中的处理逐像素平行光的Pass、处理其他逐像素光的Pass、处理阴影投射的Pass。这些Pass的原理可以回顾9.1.1节和9.4节中的相关内容。读者可以在这些代码中看到大量的#ifdef和#if语句，这些语句可以判断一些渲染条件，例如，是否使用了动态光照纹理、是否使用了逐顶点光照、是否使用了屏幕空间的阴影等，Unity会根据这些条件来进行不同的光照计算，这正是表面着色器的魅力之一——把这些烦人的光照计算交给Unity来做！\n需要注意的是，不同的Unity版本可能生成的代码有少许不同。在本书中，我们以Unity5.2.1中的结果为准。下面，我们来分析Unity生成的ForwardBase Pass。\nUnity首先指明了一些编译指令： 1 2 3 4 5 6 7 8 9 10 11 Pass { Name \u0026#34;FORWARD\u0026#34; Tags { \u0026#34;LightMode\u0026#34;=\u0026#34;ForwardBase\u0026#34; } CGPROGRAM // compile directives #pragma vertex vert_surf #pragma fragment frag_surf #pragma target 3.0 #pragma multi_compile_fwdbase #include \u0026#34;HLSLSupport.cginc\u0026#34; #include \u0026#34;UnityShaderVariables.cginc\u0026#34; 顶点着色器vert_surf和片元着色器frag_surf都是自动生成的。 之后出现的是一些自动生成的注释，这些注释表明了Unity的分析过程和它的分析结果。 随后，Unity定义了一些宏来辅助计算，例如： 1 2 3 #define INTERNAL_DATA half3 internalSurfaceTtoW0; half3 internalSurfaceTtoW1; half3 internalSurfaceTtoW2; #define WorldReflectionVector(data,normal) reflect(data.worldRefl, half3(dot(data.internalSurfaceTtoW0, normal), dot(data.internalSurfaceTtoW1, normal), dot(data.internalSurfaceTtoW2, normal))) #define WorldNormalVector(data,normal) fixed3(dot(data.internalSurfaceTtoW0, normal), dot(data.internalSurfaceTtoW1, normal), dot(data.internalSurfaceTtoW2, normal)) 实际上，在本例中上述宏并没有被用到。这些宏是为了在修改了表面法线的情况下，辅助计算得到世界空间下的反射方向和法线方向。 接着，Unity把我们在表面着色器中编写的CG代码复制过来，作为Pass的一部分，以便后续调用。 然后，Unity定义了顶点着色器到片元着色器的插值结构体（即顶点着色器的输出结构体）v2f_surf。在定义之前，Unity使用#ifdef语句来判断是否使用了光照纹理，并为不同的情况生成不同的结构体。 随后，Unity定义了真正的顶点着色器。顶点着色器首先会调用我们自定义的顶点修改函数来修改一些顶点属性，然后计算v2f_surf中各个变量的值，例如，计算经过MVP矩阵变换后的顶点坐标、使用TRANSFORM_TEX内置宏计算两个纹理的采样坐标、计算从切线空间到世界空间的变换矩阵、判断是否使用了光照映射和动态光照映射、判断是否开启了逐顶点光照等。最后，计算阴影坐标并传递给片元着色器。 在Pass的最后，Unity定义了真正的片元着色器。Unity首先利用插值后的结构体v2f_surf来初始化Input结构体中的变量，然后声明了一个SurfaceOutput结构体的变量，并对其中的表面属性进行了初始化，再调用了表面函数surf来填充这些表面属性。之后，Unity进行了真正的光照计算，包括计算光照衰减和世界空间下的法线方向、判断是否关闭了光照映射、是否需要使用自定义的光照模型计算光照结果、是否开启了动态光照映射等。最后，Unity调用自定义的颜色修改函数，对输出颜色c进行最后的修改，并使用内置宏UNITY_OPAQUE_ALPHA来重置片元的透明通道。 至此，ForwardBase Pass就结束了。接下来的ForwardAdd Pass和上面的ForwardBase Pass基本类似，只是代码更加简单了，Unity去掉了对逐顶点光照和各种判断是否使用了光照映射的代码，因为这些额外的Pass不需要考虑这些。\n最后一个重要的Pass是ShadowCaster Pass。相比于之前的两个Pass，它的代码比较简单短小，它的生成原理很简单，就是通过调用自定义的顶点修改函数来保证计算阴影时使用的是和之前一致的顶点坐标。正如我们在11.3.3节和15.1节中看到的一样，这个自定义的阴影投射的Pass同样使用了内置的V2F_SHADOW_CASTER、TRANSFER_SHADOW_CASTER_NORMALOFFSET和SHADOW_CASTER_FRAGMENT来计算阴影投射。\nSurface Shader的缺点 从上面的内容中我们可以看出，表面着色器给我们带来了很大的便利。那么，我们之前为什么还要花那么久的时间学习顶点/片元着色器？直接与表面着色器就好了嘛。\n正如我们一直强调的那样，表面着色器只是Unity在顶点/片元着色器上面提供的一种封装，是一种更高层的抽象。但任何在表面着色器中完成的事情，我们都可以在顶点/片元着色器中重现。但不幸的是，这句话反过来并不成立。\n这世上任何事情都是有代价的，如果我们想要得到便利，就需要以牺牲自由度为代价。表面着色器虽然可以快速实现各种光照效果，但我们失去了对各种优化和各种特效实现的控制。因此使用表面着色器往往会对性能造成一定的影响，而内置的Shader，例如Diffuse、BumpedSpecular等都是使用表面着色器编写的。尽管Unity提供了移动平台的相应版本，例如Mobile/Diffuse和Mobile/BumpedSpecular等，但这些版本的Shader往往只是去掉了额外的逐像素Pass、不计算全局光照和其他一些光照计算上的优化。但要想进行更多深层的优化，表面着色器就不能满足我们的需求了。\n除了性能比较差以外，表面着色器还无法完成一些自定义的渲染效果，例如10.2.2节中透明玻璃的效果。表面着色器的这些缺点让很多人更愿意使用自由的顶点/片元着色器来实现各种效果，尽管处理光照时这可能难度更大些。\n因此，我们给出一些建议供读者参考：\n如果你需要和各种光源打交道，尤其是想要使用Unity中的全局光照的话，你可能更喜欢使用表面着色器，但要时刻小心它的性能； 如果你需要处理的光源数目非常少，例如只有一个平行光，那么使用顶点/片元着色器是一个更好的选择； 最重要的是，如果你有很多自定义的渲染效果，那么请选择顶点/片元着色器。 ","date":"2024-09-05T22:12:30Z","image":"https://Selaphina.github.io/p/17-%E8%A1%A8%E9%9D%A2%E7%9D%80%E8%89%B2%E5%99%A8/cover1_hu_5e2014920a4d8f6.png","permalink":"https://Selaphina.github.io/p/17-%E8%A1%A8%E9%9D%A2%E7%9D%80%E8%89%B2%E5%99%A8/","title":"17 表面着色器"},{"content":"在本章中,我们将会学习如何使用噪声来模拟各种看似“神奇”的特效。\n在 15.1节中，我们将使用一张噪声纹理来模拟火焰的消融效果。\n15.2节则把噪声应用在模拟水面的波动上，从而产生波光粼粼的视觉效果。\n在15.3 节中，我们会回顾13.3节中实现的全局雾效，并向其中添加噪声来模拟不均匀的飘渺雾效。\n1.消融效果 消融(dissolve)效果常见于游戏中的角色死亡、地图烧毁等效果。在这些效果中，消融往往从不同的区域开始，并向看似随机的方向扩张，最后整个物体都将消失不见。\n消融原理非常简单。概括来说就是噪声纹理+透明度测试。\n我们使用对噪声纹理采样的结果和某个控制消融程度的阈值比较，如果小于阈值，就使用 clip 函数把它对应的像素裁剪掉，这些部分就对应了图中被“烧毁”的区域。而镂空区域边缘的烧焦效果则是将两种颜色混合，再用 pow函数处理后，与原纹理颜色混合后的结果。\n(1)首先，声明消融效果需要的各个属性:\n1 2 3 4 5 6 7 8 9 Properties { _BurnAmount (\u0026#34;Burn Amount\u0026#34;, Range(0.0, 1.0)) = 0.0 _LineWidth(\u0026#34;Burn Line Width\u0026#34;, Range(0.0, 0.2)) = 0.1 _MainTex (\u0026#34;Base (RGB)\u0026#34;, 2D) = \u0026#34;white\u0026#34; {} _BumpMap (\u0026#34;Normal Map\u0026#34;, 2D) = \u0026#34;bump\u0026#34; {} _BurnFirstColor(\u0026#34;Burn First Color\u0026#34;, Color) = (1, 0, 0, 1) _BurnSecondColor(\u0026#34;Burn Second Color\u0026#34;, Color) = (1, 0, 0, 1) _BurnMap(\u0026#34;Burn Map\u0026#34;, 2D) = \u0026#34;white\u0026#34;{} } BurnAmount属性用于控制消融程度，当值为0时，物体为正常效果，当值为1时，物体会完全消融。 LineWidth 属性用于控制模拟烧焦效果时的线宽，它的值越大，火焰边缘的蔓延范围越广。 MainTex和 BumpMap 分别对应了物体原本的漫反射纹理和法线纹理。BurnFirstColor 和BurnSecondColor 对应了火焰边缘的两种颜色值。 BurnMap 则是关键的噪声纹理。\n(2)我们在SubShader 块中定义消融所需的Pass:\n1 2 3 4 5 6 7 8 9 10 11 Pass { Tags { \u0026#34;LightMode\u0026#34;=\u0026#34;ForwardBase\u0026#34; } Cull Off CGPROGRAM #include \u0026#34;Lighting.cginc\u0026#34; #include \u0026#34;AutoLight.cginc\u0026#34; #pragma multi_compile_fwdbase 为了得到正确的光照，我们设置了Pass的LightMode和 multi compile fwdbase 的编译指令。值得注意的是，我们还使用Cu命令关闭了该Shader 的面片剔除，也就是说，模型的正面和背面都会被渲染。这是因为，消融会导致裸露模型内部的构造，如果只渲染正面会出现错误的结果。\n(3)定义顶点着色器\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 struct v2f { float4 pos : SV_POSITION; float2 uvMainTex : TEXCOORD0; float2 uvBumpMap : TEXCOORD1; float2 uvBurnMap : TEXCOORD2; float3 lightDir : TEXCOORD3; float3 worldPos : TEXCOORD4; SHADOW_COORDS(5) }; v2f vert(a2v v) { v2f o; o.pos = UnityObjectToClipPos(v.vertex); o.uvMainTex = TRANSFORM_TEX(v.texcoord, _MainTex); o.uvBumpMap = TRANSFORM_TEX(v.texcoord, _BumpMap); o.uvBurnMap = TRANSFORM_TEX(v.texcoord, _BurnMap); TANGENT_SPACE_ROTATION; o.lightDir = mul(rotation, ObjSpaceLightDir(v.vertex)).xyz; o.worldPos = mul(unity_ObjectToWorld, v.vertex).xyz; TRANSFER_SHADOW(o); return o; } 顶点着色器的代码很常规。我们使用宏TRANSFORM TEX计算了三张纹理对应的纹理坐标再把光源方向从模型空间变换到了切线空间。最后，为了得到阴影信息，计算了世界空间下的顶点位置和阴影纹理的采样坐标(使用了TRANSFERSHADOW宏)。具体原理可参见9.4节。\n(4)我们还需要实现片元着色器来模拟消融效果:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 fixed4 frag(v2f i) : SV_Target { fixed3 burn = tex2D(_BurnMap, i.uvBurnMap).rgb; clip(burn.r - _BurnAmount); float3 tangentLightDir = normalize(i.lightDir); fixed3 tangentNormal = UnpackNormal(tex2D(_BumpMap, i.uvBumpMap)); fixed3 albedo = tex2D(_MainTex, i.uvMainTex).rgb; fixed3 ambient = UNITY_LIGHTMODEL_AMBIENT.xyz * albedo; fixed3 diffuse = _LightColor0.rgb * albedo * max(0, dot(tangentNormal, tangentLightDir)); fixed t = 1 - smoothstep(0.0, _LineWidth, burn.r - _BurnAmount); fixed3 burnColor = lerp(_BurnFirstColor, _BurnSecondColor, t); burnColor = pow(burnColor, 5); UNITY_LIGHT_ATTENUATION(atten, i, i.worldPos); fixed3 finalColor = lerp(ambient + diffuse * atten, burnColor, t * step(0.0001, _BurnAmount)); return fixed4(finalColor, 1); } 我们首先对噪声纹理进行采样,并将采样结果和用于控制消融程度的属性 BurnAmount 相减,传递给 clip函数。\n当结果小于0时，该像素将会被剔除，从而不会显示到屏幕上。如果通过了测试，则进行正常的光照计算。我们首先根据漫反射纹理得到材质的反射率albedo，并由此计算得到环境光照，进而得到漫反射光照。\n然后，我们计算了烧焦颜色burnColor。我们想要在宽度为LineWidth 的范围内模拟一个烧焦的颜色变化，第一步就使用了 smoothstep 函数来计算混合系数4。当t值为1时，表明该像素位于消融的边界处，当值为0时，表明该像素为正常的模型颜色而中间的插值则表示需要模拟一个烧焦效果。我们首先用来混合两种火焰颜色BurnFirstColor和 BurnSecondColor，为了让效果更接近烧焦的痕迹，我们还使用pow函数对结果进行处理。然后,我们再次使用1来混合正常的光照颜色(环境光+漫反射)和烧焦颜色。我们这里又使用了 step函数来保证当 BurnAmount为0时,不显示任何消融效果。\n最后,返回混合后的颜色值 finalColor。\n(5)与之前的实现不同，我们在本例中还定义了一个用于投射阴影的Pass。正如我们在 9.4.5节中的解释一样，使用透明度测试的物体的阴影需要特别处理，如果仍然使用普通的阴影Pass,那么被剔除的区域仍然会向其他物体投射阴影，造成“穿帮”。为了让物体的阴影也能配合透明度测试产生正确的效果，我们需要自定义一个投射阴影的Pass:\n1 2 3 4 5 6 7 8 9 10 // Pass to render object as a shadow caster Pass { Tags { \u0026#34;LightMode\u0026#34; = \u0026#34;ShadowCaster\u0026#34; } CGPROGRAM #pragma vertex vert #pragma fragment frag #pragma multi_compile_shadowcaster 在 Unity 中，用于投射阴影的Pass 的LightMode 需要被设置为 ShadowCaster，同时，还需要使用#pragma multi compile shadowcaster指明它需要的编译指令。\n顶点着色器和片元着色器的代码很简单:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 v2f vert(appdata_base v) { v2f o; TRANSFER_SHADOW_CASTER_NORMALOFFSET(o) o.uvBurnMap = TRANSFORM_TEX(v.texcoord, _BurnMap); return o; } fixed4 frag(v2f i) : SV_Target { fixed3 burn = tex2D(_BurnMap, i.uvBurnMap).rgb; clip(burn.r - _BurnAmount); SHADOW_CASTER_FRAGMENT(i) } 阴影投射的重点在于我们需要按正常 Pass的处理来剔除片元或进行顶点动画，以便阴影可以和物体正常渲染的结果相匹配。\n在自定义的阴影投射的Pass中，我们通常会使用 Unity 提供的内置宏V2F SHADOW CASTER、TRANSFER SHADOW CASTER NORMALOFFSET(旧版本中会使用TRANSFERSHADOWCASTER)和SHADOWCASTERFRAGMENT来帮助我们计算阴影投射时需要的各种变量，而我们可以只关注自定义计算的部分。\n在上面的代码中，我们首先在v2f结构体中利用V2FSHADOWCASTER来定义阴影投射需要定义的变量。随后，在顶点着色器中，我们使用TRANSFERSHADOWCASTERNORMALOFFSET来填充V2FSHADOWCASTER 在背后声明的一些变量，这是由Unity 在背后为我们完成的。\n我们需要在顶点着色器中关注自定义的计算部分，这里指的就是我们需要计算噪声纹理的采样坐标uvBurnMap。\n在片元着色器中，我们首先按之前的处理方法使用噪声纹理的采样结果来剔除片元，最后再利用SHADOWCASTERFRAGMENT来让Unity为我们完成阴影投射的部分，把结果输出到深度图和阴影映射纹理中。\n通过 Unity 提供的这3个内置宏(在 UnityCGcginc 文件中被定义)，我们可以方便地自定义需要的阴影投射的Pass，但由于这些宏需要使用一些特定的输入变量，因此我们需要保证为它们提供了这些变量。例如，TRANSFERSHADOWCASTERNORMALOFFSET会使用名称v作为输入结构体,v中需要包含顶点位置v.vertex和顶点法线v.normal的信息，我们可以直接使用内置的appdata base 结构体，它包含了这些必需的顶点变量。如果我们需要进行顶点动画，可以在顶点着色器中直接修改v.vertex,再传递给 TRANSFER SHADOWCASTER NORMALOFFSET即可(可参见11.3.3节)。\n在本例中，我们使用的噪声纹理(对应本书资源的Assets/Textures/Chapter15/Burn Noise.png)如图15.2所示。把它拖曳到材质的BurnMap属性上，再调整材质的BurnAmount性，就可以看到木箱逐渐消融的效果。在本书资源的实现中，我们实现了一个辅助脚本，用来随时间调整材质的BurnAmount值，因此，当读者单击运行后，也可以看到消融的动画效果。使用不同的噪声和纹理属性(即材质面板上纹理的Tiling和 Ofset值)都会得到不同的消融效果。因此，要想得到好的消融效果，也需要美术人员提供合适的噪声纹理来配合。\n2.水波效果 在模拟实时水面的过程中，我们往往也会使用噪声纹理。此时，噪声纹理通常会用作一个高度图，以不断修改水面的法线方向。为了模拟水不断流动的效果，我们会使用和时间相关的变量来对噪声纹理进行采样，当得到法线信息后，再进行正常的反射+折射计算，得到最后的水面波动效果。 在本节中，我们将会使用一个由噪声纹理得到的法线贴图，实现一个包含菲涅耳反射(详见10.1.5节)的水面效果，如图15.3所示。\n我们曾在10.2.2节介绍过如何使用反射和折射来模拟一个透明玻璃的效果。本节使用的Shader 和10.2.2节中的实现基本相同。我们使用一张立方体纹理(Cubemap)作为环境纹理，模拟反射。\n为了模拟折射效果，我们使用GrabPass来获取当前屏幕的渲染纹理，并使用切线空间下的法线方向对像素的屏幕坐标进行偏移，再使用该坐标对渲染纹理进行屏幕采样，从而模拟近似的折射效果。与 10.2.2节中的实现不同的是，水波的法线纹理是由一张噪声纹理生成而得，而且会随着时间变化不断平移，模拟波光粼粼的效果。\n除此之外，我们没有使用一个定值来混合反射和折射颜色，而是使用之前提到的菲涅耳系数来动态决定混合系数。我们使用如下公式来计算菲涅耳系数:\n其中，v和n分别对应了视角方向和法线方向。它们之间的夹角越小，fesnel值越小，反射越弱，折射越强。菲涅耳系数还经常会用于边缘光照的计算中。 为此，我们需要做如下准备工作。 (1)新建一个场景。在本书资源中，该场景名为Scene152。在 Unity5.2中，默认情况下场景将包含一个摄像机和一个平行光,并且使用了内置的天空盒子。在 Window-\u0026gt;Lighting-\u0026gt;Skybox， 中去掉场景中的天空盒子。\n(2)新建一个材质。在本书资源中，该材质名为WaterWaveMat。\n(3)新建一个 Unity Shader。在本书资源中，该Shader名为Chapter15-WaterWave。把新的Shader赋给第2步中创建的材质。\n(4)构建一个测试水波效果的场景。\n在本书资源的实现中，我们构建了一个由6面墙围成的封闭房间，它们都使用了我们在9.5节中创建的标准材质。我们还在房间中放置了一个平面来模拟水面。把第2步中创建的材质赋给该平面。\n们使用了10.1.2节中实现的创建立方体纹理的脚本(通过 Gameobject-\u0026gt;Render into Cubemap 打开编辑窗口)来创建它，如图 15.4所示。在本书资源中，该Cubemap名为Water Cubemap。\n1）声明\n1 2 3 4 5 6 7 8 9 Properties { _Color (\u0026#34;Main Color\u0026#34;, Color) = (0, 0.15, 0.115, 1) _MainTex (\u0026#34;Base (RGB)\u0026#34;, 2D) = \u0026#34;white\u0026#34; {} _WaveMap (\u0026#34;Wave Map\u0026#34;, 2D) = \u0026#34;bump\u0026#34; {} _Cubemap (\u0026#34;Environment Cubemap\u0026#34;, Cube) = \u0026#34;_Skybox\u0026#34; {} _WaveXSpeed (\u0026#34;Wave Horizontal Speed\u0026#34;, Range(-0.1, 0.1)) = 0.01 _WaveYSpeed (\u0026#34;Wave Vertical Speed\u0026#34;, Range(-0.1, 0.1)) = 0.01 _Distortion (\u0026#34;Distortion\u0026#34;, Range(0, 100)) = 10 } 其中，Color 用于控制水面颜色;MainTex是水面波纹材质纹理默认为白色纹理；WaveMap是一个由噪声纹理生成的法线纹理;Cubemap 是用于模拟反射的立方体纹理;Distortion 则用于控制模拟折射时图像的扭曲程度;WaveXSpeed和WaveYSpeed 分别用于控制法线纹理在X和Y方向上的平移速度。\n(2)定义相应的渲染队列，并使用GrabPass来获取屏幕图像:\n1 2 3 4 5 6 7 SubShader { // We must be transparent, so other objects are drawn before this one. Tags { \u0026#34;Queue\u0026#34;=\u0026#34;Transparent\u0026#34; \u0026#34;RenderType\u0026#34;=\u0026#34;Opaque\u0026#34; } // This pass grabs the screen behind the object into a texture. // We can access the result in the next pass as _RefractionTex GrabPass { \u0026#34;_RefractionTex\u0026#34; } 我们首先在 SubShader 的标签中将渲染队列设置成Transparent,并把后面的 RenderType 设置为Opaque。把Queue 设置成 Transparent 可以确保该物体渲染时，其他所有不透明物体都已经被渲染到屏幕上了，否则就可能无法正确得到“透过水面看到的图像”。而设置RenderType 则是为了在使用着色器替换(Shader Replacement)时，该物体可以在需要时被正确渲染。这通常发生在我们需要得到摄像机的深度和法线纹理时,这在第13 章中介绍过。随后,我们通过关键词 GrabPass定义了一个抓取屏幕图像的Pass。\n在这个 Pass中我们定义了一个字符串，该字符串内部的名称决定了抓取得到的屏幕图像将会被存入哪个纹理中(可参见1022节)。\n定义渲染水面所需的Pass。为了在Shader 中访问各个属性，我们首先需要定义它们对应的变量: 1 2 3 4 5 6 7 8 9 10 11 fixed4 _Color; sampler2D _MainTex; float4 _MainTex_ST; sampler2D _WaveMap; float4 _WaveMap_ST; samplerCUBE _Cubemap; fixed _WaveXSpeed; fixed _WaveYSpeed; float _Distortion;\tsampler2D _RefractionTex; float4 _RefractionTex_TexelSize; 需要注意的是，我们还定义了_RefractionTex和RefractionTex_TexelSize 变量，这对应了在使用 GrabPass时，指定的纹理名称。RefractionTex TexelSize 可以让我们得到该纹理的纹素大小,例如一个大小为 256x512 的纹理，它的纹素大小为(1/256,1/512)。我们需要在对屏幕图像的采样坐标进行偏移时使用该变量。\n(4)定义顶点着色器，这和10.2.2节中的实现完全一样:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 struct v2f { float4 pos : SV_POSITION; float4 scrPos : TEXCOORD0; float4 uv : TEXCOORD1; float4 TtoW0 : TEXCOORD2; float4 TtoW1 : TEXCOORD3; float4 TtoW2 : TEXCOORD4; }; v2f vert(a2v v) { v2f o; o.pos = UnityObjectToClipPos(v.vertex); o.scrPos = ComputeGrabScreenPos(o.pos); o.uv.xy = TRANSFORM_TEX(v.texcoord, _MainTex); o.uv.zw = TRANSFORM_TEX(v.texcoord, _WaveMap); float3 worldPos = mul(unity_ObjectToWorld, v.vertex).xyz; fixed3 worldNormal = UnityObjectToWorldNormal(v.normal); fixed3 worldTangent = UnityObjectToWorldDir(v.tangent.xyz); fixed3 worldBinormal = cross(worldNormal, worldTangent) * v.tangent.w; o.TtoW0 = float4(worldTangent.x, worldBinormal.x, worldNormal.x, worldPos.x); o.TtoW1 = float4(worldTangent.y, worldBinormal.y, worldNormal.y, worldPos.y); o.TtoW2 = float4(worldTangent.z, worldBinormal.z, worldNormal.z, worldPos.z); return o; } 在进行了必要的顶点坐标变换后，我们通过调用ComputeGrabScreenPos来得到对应被抓取屏幕图像的采样坐标。读者可以在UnityCGcginc文件中找到它的声明，它的主要代码和ComputeScreenPos基本类似，最大的不同是针对平台差异造成的采样坐标问题(见5.6.1节)进行了处理。\n接着,我们计算了 MainTex和 BumpMap的采样坐标,并把它们分别存储在一个 foat4类型变量的xy 和zw 分量中。由于我们需要在片元着色器中把法线方向从切线空间(由法线纹理采样得到)变换到世界空间下，以便对Cubemap进行采样，因此，我们需要在这里计算该顶点对应的从切线空间到世界空间的变换矩阵,并把该矩阵的每一行分别存储在TtoW0、TtoW1和TtoW2的x”z分量中。\n这里面使用的数学方法就是，得到切线空间下的3个坐标轴(x、y、z轴分别对应了切线、副切线和法线的方向)在世界空间下的表示，再把它们依次按列组成一个变换矩阵即可TtoW0 等值的w分量同样被利用起来，用于存储世界空间下的顶点坐标。\n(5)定义片元着色器:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 fixed4 frag(v2f i) : SV_Target { float3 worldPos = float3(i.TtoW0.w, i.TtoW1.w, i.TtoW2.w); fixed3 viewDir = normalize(UnityWorldSpaceViewDir(worldPos)); float2 speed = _Time.y * float2(_WaveXSpeed, _WaveYSpeed); // Get the normal in tangent space fixed3 bump1 = UnpackNormal(tex2D(_WaveMap, i.uv.zw + speed)).rgb; fixed3 bump2 = UnpackNormal(tex2D(_WaveMap, i.uv.zw - speed)).rgb; fixed3 bump = normalize(bump1 + bump2); // Compute the offset in tangent space float2 offset = bump.xy * _Distortion * _RefractionTex_TexelSize.xy; i.scrPos.xy = offset * i.scrPos.z + i.scrPos.xy; fixed3 refrCol = tex2D( _RefractionTex, i.scrPos.xy/i.scrPos.w).rgb; // Convert the normal to world space bump = normalize(half3(dot(i.TtoW0.xyz, bump), dot(i.TtoW1.xyz, bump), dot(i.TtoW2.xyz, bump))); fixed4 texColor = tex2D(_MainTex, i.uv.xy + speed); fixed3 reflDir = reflect(-viewDir, bump); fixed3 reflCol = texCUBE(_Cubemap, reflDir).rgb * texColor.rgb * _Color.rgb; fixed fresnel = pow(1 - saturate(dot(viewDir, bump)), 4); fixed3 finalColor = reflCol * fresnel + refrCol * (1 - fresnel); return fixed4(finalColor, 1); } 我们首先通过 TtoW0 等变量的w分量得到世界坐标，并用该值得到该片元对应的视角方向。\n除此之外，我们还使用内置的Time.y变量和WaveXSpeed、WaveYSpeed 属性计算了法线纹理的当前偏移量,并利用该值对法线纹理进行两次采样(这是为了模拟两层交叉的水面波动的效果),对两次结果相加并归一化后得到切线空间下的法线方向。\n然后，和10.2.2节中的处理一样，我们使用该值和 Distortion属性以及 RefactionTex TexelSize 来对屏幕图像的采样坐标进行偏移，模拟折射效果。 Distortion 值越大，偏移量越大，水面背后的物体看起来变形程度越大。在这里，我们选择使用切线空间下的法线方向来进行偏移，是因为该空间下的法线可以反映顶点局部空间下的法线方向。\n需要注意的是，在计算偏移后的屏幕坐标时，我们把偏移量和屏幕坐标的z分量相乘，这是为了模拟深度越大、折射程度越大的效果。如果读者不希望产生这样的效果，可以直接把偏移值叠加到屏幕坐标上。随后，我们对scrPos进行了透视除法，再使用该坐标对抓取的屏幕图像 RefractionTex 进行采样，得到模拟的折射颜色。\n之后，我们把法线方向从切线空间变换到了世界空间下(使用变换矩阵的每一行，即TtoW0、TtoW1和TtoW2，分别和法线方向点乘，构成新的法线方向)，并据此得到视角方向相对于法线方向的反射方向。随后，使用反射方向对Cubemap进行采样，并把结果和主纹理颜色相乘后得到反射颜色。我们也对主纹理进行了纹理动画，以模拟水波的效果。 为了混合折射和反射颜色，我们随后计算了菲涅耳系数。我们使用之前的公式来计算菲涅耳系数，并据此来混合折射和反射颜色，作为最终的输出颜色。 在本例中,我们使用的噪声纹理(对应本书资源的 Assets/Textures/Chapter15/Water Noise.png)\n如图15.5左图所示。由于在本例中，我们需要的是一张法线纹理，因此我们可以从该噪声纹理的灰度值中生成需要的法线信息，这是通过在它的纹理面板中把纹理类型设置为Normalmap，并选中Createfrom grayscale 来完成的。最后生成的法线纹理如图15.5右图所示。我们把生成的法线纹理拖曳到材质的WaveMap属性上，再单击运行后，就可以看到水面波动的效果了。\n3.再谈全局雾效 我们在13.3节讲到了如何使用深度纹理来实现一种基于屏幕后处理的全局雾效。我们由深度纹理重建每个像素在世界空间下的位置，再使用一个基于高度的公式来计算雾效的混合系数，最后使用该系数来混合雾的颜色和原屏幕颜色。\n13.3节的实现效果是一个基于高度的均匀雾效，即在同一个高度上，雾的浓度是相同的，如图15.6左图所示。然而，一些时候我们希望可以模拟一种不均匀的雾效，同时让雾不断飘动，使雾看起来更加飘渺，如图15.6右图所示。而这就可以通过使用一张噪声纹理来实现。\n本节的实现非常简单,绝大多数代码和13.3节中的完全一样,我们只是添加了噪声相关的参数和属性，并在Shader的片元着色器中对高度的计算添加了噪声的影响。为了完整性，我们会给出本节使用的脚本和Shader的实现，但其中使用的原理不再赘述，读者可参见13.3节。\n我们首先需要进行如下准备工作。\n(1)新建一个场景。在本书资源中，该场景名为 Scene 15.3。在 Unity 5.2中，默认情况下场景将包含一个摄像机和一个平行光，并且使用了内置的天空盒子。在 Window -\u0026gt;Lighting-\u0026gt;Skybox 中去掉场景中的天空盒子。\n(2)我们需要搭建一个测试雾效的场景。在本书资源的实现中，我们构建了一个包含3面墙的房间，并放置了两个立方体和两个球体，它们都使用了我们在9.5节中创建的标准材质。\n(3)新建一个脚本。在本书资源中，该脚本名为FogWithNoise.cs。把该脚本拖曳到摄像机上。\n(4)新建一个 Unity Shader。在本书资源中，该Shader 名为 Chapter15-FogWithNoise。\n我们首先来编写FogWithNoise.cs脚本。打开该脚本，并进行如下修改。\nFogWithNoise.cs (1)首先，继承12.1节中创建的基类:\n1 public class FogwithNoise:PostEffectsBase (2)声明该效果需要的Shader，并据此创建相应的材质:\n1 2 3 4 5 6 7 8 9 public Shader fogShader; private Material fogMaterial = null; public Material material { get { fogMaterial = CheckShaderAndCreateMaterial(fogShader, fogMaterial); return fogMaterial; } } 在本节中，我们需要获取摄像机的相关参数，如近裁剪平面的距离、FOV 等，同时还需要获取摄像机在世界空间下的前方、上方和右方等方向，因此我们用两个变量存储摄像机的Camera组件和Transform组件: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 private Camera myCamera; public Camera camera { get { if (myCamera == null) { myCamera = GetComponent\u0026lt;Camera\u0026gt;(); } return myCamera; } } private Transform myCameraTransform; public Transform cameraTransform { get { if (myCameraTransform == null) { myCameraTransform = camera.transform; } return myCameraTransform; } } (4)定义模拟雾效时使用的各个参数:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 [Range(0.1f, 3.0f)] public float fogDensity = 1.0f; public Color fogColor = Color.white; public float fogStart = 0.0f; public float fogEnd = 2.0f; public Texture noiseTexture; [Range(-0.5f, 0.5f)] public float fogXSpeed = 0.1f; [Range(-0.5f, 0.5f)] public float fogYSpeed = 0.1f; [Range(0.0f, 3.0f)] public float noiseAmount = 1.0f; fogDensity 用于控制雾的浓度，fogColor 用于控制雾的颜色。我们使用的雾效模拟函数是基于高度的，因此参数fogStant 用于控制雾效的起始高度，fogEnd用于控制雾效的终止高度。\nnoiseTexture 是我们使用的噪声纹理，fogXSpeed和fogYSpeed 分别对应了噪声纹理在X和Y方向上的移动速度，以此来模拟雾的飘动效果。最后，noiseAmount用于控制噪声程度，当noiseAmount为0时，表示不应用任何噪声，即得到一个均匀的基于高度的全局雾效。\n(5)由于本例需要获取摄像机的深度纹理,我们在脚本的 OnEnable 函数中设置摄像机的相应状态:\n1 2 3 void OnEnable() { GetComponent\u0026lt;Camera\u0026gt;().depthTextureMode |= DepthTextureMode.Depth; } (6) 最后，我们实现了 OnRenderlmage函数:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 void OnRenderImage (RenderTexture src, RenderTexture dest) { if (material != null) { Matrix4x4 frustumCorners = Matrix4x4.identity; float fov = camera.fieldOfView; float near = camera.nearClipPlane; float aspect = camera.aspect; float halfHeight = near * Mathf.Tan(fov * 0.5f * Mathf.Deg2Rad); Vector3 toRight = cameraTransform.right * halfHeight * aspect; Vector3 toTop = cameraTransform.up * halfHeight; Vector3 topLeft = cameraTransform.forward * near + toTop - toRight; float scale = topLeft.magnitude / near; topLeft.Normalize(); topLeft *= scale; Vector3 topRight = cameraTransform.forward * near + toRight + toTop; topRight.Normalize(); topRight *= scale; Vector3 bottomLeft = cameraTransform.forward * near - toTop - toRight; bottomLeft.Normalize(); bottomLeft *= scale; Vector3 bottomRight = cameraTransform.forward * near + toRight - toTop; bottomRight.Normalize(); bottomRight *= scale; frustumCorners.SetRow(0, bottomLeft); frustumCorners.SetRow(1, bottomRight); frustumCorners.SetRow(2, topRight); frustumCorners.SetRow(3, topLeft); material.SetMatrix(\u0026#34;_FrustumCornersRay\u0026#34;, frustumCorners); material.SetFloat(\u0026#34;_FogDensity\u0026#34;, fogDensity); material.SetColor(\u0026#34;_FogColor\u0026#34;, fogColor); material.SetFloat(\u0026#34;_FogStart\u0026#34;, fogStart); material.SetFloat(\u0026#34;_FogEnd\u0026#34;, fogEnd); material.SetTexture(\u0026#34;_NoiseTex\u0026#34;, noiseTexture); material.SetFloat(\u0026#34;_FogXSpeed\u0026#34;, fogXSpeed); material.SetFloat(\u0026#34;_FogYSpeed\u0026#34;, fogYSpeed); material.SetFloat(\u0026#34;_NoiseAmount\u0026#34;, noiseAmount); Graphics.Blit (src, dest, material); } else { Graphics.Blit(src, dest); } } 我们首先利用 13.3节学习的方法计算近裁剪平面的4个角对应的向量，并把它们存储在一个矩阵类型的变量(frustumCormers)中。计算过程和原理均可参见13.3节。随后，我们把结果和其他参数传递给材质，并调用Graphics.Blit(src,dest,material)把渲染结果显示在屏幕上。\nChapter15-FogWithNoise.shader\n下面，我们来实现 Shader 的部分。打开Chapter15-FogWithNoise，进行如下修改。\n(1)我们首先需要声明本例使用的各个属性:\n1 2 3 4 5 6 7 8 9 10 11 Properties { _MainTex (\u0026#34;Base (RGB)\u0026#34;, 2D) = \u0026#34;white\u0026#34; {} _FogDensity (\u0026#34;Fog Density\u0026#34;, Float) = 1.0 _FogColor (\u0026#34;Fog Color\u0026#34;, Color) = (1, 1, 1, 1) _FogStart (\u0026#34;Fog Start\u0026#34;, Float) = 0.0 _FogEnd (\u0026#34;Fog End\u0026#34;, Float) = 1.0 _NoiseTex (\u0026#34;Noise Texture\u0026#34;, 2D) = \u0026#34;white\u0026#34; {} _FogXSpeed (\u0026#34;Fog Horizontal Speed\u0026#34;, Float) = 0.1 _FogYSpeed (\u0026#34;Fog Vertical Speed\u0026#34;, Float) = 0.1 _NoiseAmount (\u0026#34;Noise Amount\u0026#34;, Float) = 1 } (2)在本节中,我们使用 CGINCLUDE来组织代码。我们在 SubShader 块中利用CGINCLUDE和ENDCG语义来定义一系列代码:\n1 2 3 4 5 6 7 8 9 SubShader { CGINCLUDE …… ENDCG …… } (3)声明代码中需要使用的各个变量:\n1 2 3 4 5 6 7 8 9 10 11 12 13 float4x4 _FrustumCornersRay; sampler2D _MainTex; half4 _MainTex_TexelSize; sampler2D _CameraDepthTexture; half _FogDensity; fixed4 _FogColor; float _FogStart; float _FogEnd; sampler2D _NoiseTex; half _FogXSpeed; half _FogYSpeed; half _NoiseAmount; FrustumCornersRay虽然没有在Properties中声明，但仍可由脚本传递给 Shader。除了Properties 中声明的各个属性，我们还声明了深度纹理 CameraDepthTexture，Unity 会在背后把得到的深度纹理传递给该值。\n(4)定义顶点着色器，这和13.3节中的实现完全一致。读者可以在 13.3 节找到它的实现和相关解释。\n(5)定义片元着色器:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 fixed4 frag(v2f i) : SV_Target { float linearDepth = LinearEyeDepth(SAMPLE_DEPTH_TEXTURE(_CameraDepthTexture, i.uv_depth)); float3 worldPos = _WorldSpaceCameraPos + linearDepth * i.interpolatedRay.xyz; float2 speed = _Time.y * float2(_FogXSpeed, _FogYSpeed); float noise = (tex2D(_NoiseTex, i.uv + speed).r - 0.5) * _NoiseAmount; float fogDensity = (_FogEnd - worldPos.y) / (_FogEnd - _FogStart); fogDensity = saturate(fogDensity * _FogDensity * (1 + noise)); fixed4 finalColor = tex2D(_MainTex, i.uv); finalColor.rgb = lerp(finalColor.rgb, _FogColor.rgb, fogDensity); return finalColor; } 我们首先根据深度纹理来重建该像素在世界空间中的位置。然后，我们利用内置的_Time.y变量和 FogXSpeed、FogYSpeed 属性计算出当前噪声纹理的偏移量，并据此对噪声纹理进行采样，得到噪声值。我们把该值减去0.5，再乘以控制噪声程度的属性NoiseAmount，得到最终的噪声值。随后，我们把该噪声值添加到雾效浓度的计算中，得到应用噪声后的雾效混合系数fogDensity。最后，我们使用该系数将雾的颜色和原始颜色进行混合后返回。\n(6)随后，我们定义了雾效渲染所需的Pass:\n1 2 3 4 5 6 7 8 Pass { CGPROGRAM #pragma vertex vert #pragma fragment frag ENDCG } (7)最后，我们关闭了 Shader的 Fallback:\n1 Fallback off 完成后返回编辑器，并把Chapter15-FogWithNoise 拖曳到摄像机的 FogWithNoise.cs脚本中的 fogShader 参数中。\n当然，我们可以在 FogWithNoise.cs的脚本面板中将 fogShader 参数的默认值设置为Chapter15-FogWithNoise，这样就不需要以后使用时每次都手动拖曳了。本节使用的噪声纹理(对应本书资源的Assets/Textures/Chapter15/Fog_Noise.jpg)如图 15.7 所示。我们把该噪声纹理拖曳到FogWithNoise.cs脚本中的noiseTexture参数中，我们也可以参照之前的方法，直接在FogWithNoise.cs的脚本面板中将 noiseTexture 参数的默认值设置为FogNoise.jpg，这样就不需要以后使用时每次都手动拖曳了。\n4.扩展阅读 读者在阅读本章时，可能会有一个疑问:这些噪声纹理都是如何构建出来的?这些噪声纹理可以被认为是一种程序纹理(Procedure Texture)，它们都是由计算机利用某些算法生成的。\nPerlin 噪声(https:/en.wikipedia.org/wiki/Perlin noise)和 Worley 噪声(htps://en.wikipedia.org/wiki/Worley noise )是两种最常使用的噪声类型，例如我们在15.3节中使用的噪声纹理由 Perlin 噪声生成而来。\nPerlin噪声可以用于生成更自然的噪声纹理，而 Worley 噪声则通常用于模拟诸如石头、水、纸张等多孔噪声。现代的图像编辑软件，如Photoshop等，往往提供了类似的功能或插件，以帮助美术人员生成需要的噪声纹理，但如果读者想要更加自由地控制噪声纹理的生成，可能就需要了解它们的生成原理。\n读者可以在这个博客(http://lafla2.github.io/2014/08/09/perlinnoise.html)中找到一篇关于理解 Perlin 噪声的非常好的文章，在文章的最后，作者还给出了很多其他出色的参考链接。关于 Worley 噪声，读者可以在作者 Worley1998年发表的论文四中找到它的算法和实现细节。在另-个非常好的博客(htp://scrawkblog.com/category/procedural-noise/)中，博主给出了很多程序噪声在 Unity中的实现，并包含了实现源码。\n","date":"2024-07-25T22:12:30Z","image":"https://Selaphina.github.io/p/15-%E5%99%AA%E5%A3%B0%E8%BF%90%E7%94%A8/cover1_hu_5e2014920a4d8f6.png","permalink":"https://Selaphina.github.io/p/15-%E5%99%AA%E5%A3%B0%E8%BF%90%E7%94%A8/","title":"15 噪声运用"},{"content":"在第12章中,我们学习的屏幕后处理效果都只是在屏幕颜色图像上进行各种操作来实现的。然而,很多时候我们不仅需要当前屏幕的颜色信息,还希望得到深度和法线信息。例如,在进行边缘检测时,直接利用颜色信息会使检测到的边缘信息受物体纹理和光照等外部因素的影响,得 到很多我们不需要的边缘点。\n一种更好的方法是,我们可以在深度纹理和法线纹理上进行边缘检测,这些图像不会受纹理和光照的影响,而仅仅保存了当前渲刻染物体的模型信息,通过这样的方式检测出来的边缘更加可靠。\n1.获取深度和法线纹理 1）背后的原理 深度纹理实际就是一张渲染纹理，只不过它里面存储的像素值不是颜色值，而是一个高精度的深度值。由于被存储在一张纹理中，深度纹理里的深度值范围是[0,1]，而且通常是非线性分布的。\n那么，这些深度值是从哪里得到的呢?要回答这个问题，我们需要回顾在第4章学过的顶点变换的过程。总体来说，这些深度值来自于顶点变换后得到的归一化的设备坐标(NormalizedDevice Coordinates ，NDC)。回顾一下，一个模型要想最终被绘制在屏幕上，需要把它的顶点从模型空间变换到齐次裁剪坐标系下，这是通过在顶点着色器中乘以MVP变换矩阵得到的。在变换的最后一步，我们需要使用一个投影矩阵来变换顶点，当我们使用的是透视投影类型的摄像机时，这个投影矩阵就是非线性的，具体过程可回顾4.6.7小节。\n图13.1显示了4.6.7小节中给出的Unity 中透视投影对顶点的变换过程。图13.1中最左侧的图显示了投影变换前，即观察空间下视锥体的结构及相应的顶点位置，中间的图显示了应用透视裁剪矩阵后的变换结果，即顶点着色器阶段输出的顶点变换结果，最右侧的图则是底层硬件进行了透视除法后得到的归一化的设备坐标。\n需要注意的是，这里的投影过程是建立在Unity 对坐标系的假定上的，也就是说，我们针对的是观察空间为右手坐标系，使用列矩阵在矩阵右侧进行相乘，且变换到 NDC后z分量范围将在[-1,1]之间的情况。而在类似DirectX 这样的图形接口中，变换后z分量范围将在[0,1]之间。如果需要在其他图形接口下实现本章的类似效果，需要对一些计算参数做出相应变化。关于变换时使用的矩阵运算，读者可以参考4.6.7小节。\n图13.2显示了在使用正交摄像机时投影变换的过程。同样，变换后会得到一个范围为[-1,1]的立方体。正交投影使用的变换矩阵是线性的。\n在得到 NDC后,深度纹理中的像素值就可以很方便地计算得到了,这些深度值就对应了NDC中顶点坐标的z分量的值。由于NDC中z分量的范围在[-1,1]，为了让这些值能够存储在一张图像中，我们需要使用下面的公式对其进行映射: $$ d = 0.5 \\cdot z_{ndc} + 0.5 $$ 其中，d对应了深度纹理中的像素值，z-ndc-对应了NDC坐标中的z分量的值。那么 Unity 是怎么得到这样一张深度纹理的呢?\n在 Unity 中，深度纹理可以直接来自于真正的深度缓存，也可以是由一个单独的 Pass 渲染而得，这取决于使用的渲染路径和硬件。通常来讲,当使用延迟渲染路径(包括遗留的延迟渲染路径)时，深度纹理理所当然可以访问到，因为延迟渲染会把这些信息渲染到 G-bufer 中。\n而当无法直接获取深度缓存时，深度和法线纹理是通过一个单独的 Pass 渲染而得的。具体实现是，Unity会使用着色器替换 ( Shader Replacement ) 技术选择那些渲染类型 (即 SubShader的RenderType 标签 ) 为Opaque 的物体，判断它们使用的渲染队列是否小于等于2500 ( 内置的Background、Geometry和AlphaTest 渲染队列均在此范围内 )，如果满足条件，就把它渲染到深度和法线纹理中。因此，要想让物体能够出现在深度和法线纹理中，就必须在 Shader 中设置正确的 RenderType 标签 在 Unity 中，我们可以选择让一个摄像机生成一张深度纹理或是一张深度+法线纹理。当选择前者，即只需要一张单独的深度纹理时，Unity会直接获取深度缓存或是按之前讲到的着色器替换技术，选取需要的不透明物体，并使用它投射阴影时使用的Pass(即LightMode被设置为ShadowCaster 的Pass，详见9.4节)来得到深度纹理。如果 Shader 中不包含这样一个Pass，那么这个物体就不会出现在深度纹理中(当然，它也不能向其他物体投射阴影)。深度纹理的精度通常是24位或16位，这取决于使用的深度缓存的精度。如果选择生成一张深度+法线纹理，Unity会创建一张和屏幕分辨率相同、精度为32位(每个通道为8位)的纹理，其中观察空间下的法线信息会被编码进纹理的R和G通道，而深度信息会被编码进B和A通道。法线信息的获取在延迟渲染中是可以非常容易就得到的，Unity只需要合并深度和法线缓存即可。而在前向渲染中，默认情况下是不会创建法线缓存的，因此 Unity底层使用了一个单独的 Pass 把整个场景再次渲染一遍来完成。这个Pass被包含在Unity内置的一个UnityShader中，我们可以在内置的builtin shaders-xxx/DefaultResources/Camera-DepthNormalTexture.shader 文件中找到这个用于渲染深度和法线信息的 Pass。\n2）获取深度纹理 在 Unity 中，获取深度纹理是非常简单的，我们只需要告诉 Unity:“嘿，把深度纹理给我!”然后再在 Shader 中直接访问特定的纹理属性即可。这个与Unity 沟通的过程是通过在脚本中设置摄像机的 depthTextureMode 来完成的，例如我们可以通过下面的代码来获取深度纹理:\n1 camera,depthTextureMode = DepthTextureMode.Depth; 一旦设置好了上面的摄像机模式后，我们就可以在Shader中通过声明CameraDepthTexture变量来访问它。这个过程非常简单，但我们需要知道这两行代码的背后，Unity为我们做了许多工作(见13.1.1节)。\n同理，如果想要获取深度+法线纹理，我们只需要在代码中这样设置:\n1 camera.depthTextureMode = DepthTextureMode.DepthNormals; 然后在 Shader中通过声明 CameraDepth变量来访问它。TaSexture我们还可以组合这些模式，让一个摄像机同时产生一张深度和深度+法线纹理:\n1 2 camera.depthTextureMode=DepthTextureMode.Depth; camera.depthTextureMode=DepthTextureMode.DepthNormals; 在 Unity5中，我们还可以在摄像机的Camera 组件上看到当前摄像机是否需要渲染深度或深度+法线纹理。当在 Shader 中访问到深度纹理CameraDepthTexture后，我们就可以使用当前像素的纹理坐标对它进行采样。绝大多数情况下，我们直接使用tex2D函数采样即可，但在某些平台(例如PS3和PSP2)上，我们需要一些特殊处理。Uniy为我们提供了一个统一的宏SAMPLE DEPTH TEXTURE，用来处理这些由于平台差异造成的问题。而我们只需要在 Shader中使用 SAMPLE DEPTH TEXTURE 宏对深度纹理进行采样，例如:\n1 float d=SAMPLE DEPTH TEXTURE( CameraDepthTexture,i.uv); 其中，i.uv是一个foat2类型的变量，对应了当前像素的纹理坐标。类似的宏还有SAMPLE DEPTH TEXTURE PROJ和 SAMPLE DEPTH TEXTURE LOD。 SAMPLE DEPTHTEXTURE PROJ宏同样接受两个参数\u0026ndash;深度纹理和一个 foat3 或 foat4 类型的纹理坐标,它的内部使用了 tex2Dproj这样的函数进行投影纹理采样，纹理坐标的前两个分量首先会除以最后一个分量，再进行纹理采样。如果提供了第四个分量，还会进行一次比较，通常用于阴影的实现中。SAMPLE DEPTH TEXTUREPROJ的第二个参数通常是由顶点着色器输出插值而得的屏幕坐标，例如:\n1 float d=SAMPLE DEPTH TEXTURE PROJ( CameraDepthTexture,UNITY PROJ COORD(i.scrPos)); 其中，i.scrPos 是在顶点着色器中通过调用 ComputeScreenPos(o.pos)得到的屏幕坐标。上述这 些宏的定义，读者可以在 Unity内置的HSLSupport.cginc 文件中找到。当通过纹理采样得到深度值后，这些深度值往往是非线性的，这种非线性来自于透视投影使用的裁剪矩阵。然而，在我们的计算过程中通常是需要线性的深度值，也就是说，我们需要把投影后的深度值变换到线性空间下，例如视角空间下的深度值。那么，我们应该如何进行这个转换呢?实际上，我们只需要倒推顶点变换的过程即可。下面我们以透视投影为例，推导如何由深度纹理中的深度信息计算得到视角空间下的深度值。 由4.6.7节可知，当我们使用透视投影的裁剪矩阵P\u0026quot;对视角空间下的一个顶点进行变换后，裁剪空间下顶点的z和w分量为: $$ z_{\\text{clip}} = -z_{\\text{visw}} \\frac{ \\text{Far} + \\text{Near} }{ \\text{Far} - \\text{Near} } - \\frac{2 \\cdot \\text{Near} \\cdot \\text{Far} }{ \\text{Far} - \\text{Near} } $$$$ w_{\\text{clip}} = -z_{\\text{visw}} $$其中，Far 和Near 分别是远近裁剪平面的距离。然后，我们通过齐次除法就可以得到 NDC下的z分量: $$ z_{\\text{ndc}} = \\frac{z_{\\text{clip}}}{w_{\\text{clip}}} = \\frac{ \\text{Far} + \\text{Near} }{ \\text{Far} - \\text{Near} } + \\frac{2 \\cdot \\text{Near} \\cdot \\text{Far} }{ (\\text{Far} - \\text{Near}) \\cdot z_{\\text{visw}} } $$ 在 13.1.1节中我们知道，深度纹理中的深度值是通过下面的公式由NDC计算而得的:\n$$ d = 0.5 \\cdot z_{ndc} + 0.5 $$ 由上面的这些式子，我们可以推导出用d表示而得的z的表达式: $$ z'_{\\text{visw}} = \\frac{ 1 }{ \\frac{ \\text{Far} - \\text{Near}}{\\text{Near} \\cdot \\text{Far} } d - \\frac{1}{\\text{Near}}} $$ 由于在 Unity 使用的视角空间中，摄像机正向对应的z值均为负值，因此为了得到深度值的正数表示，我们需要对上面的结果取反，最后得到的结果如下: $$ z_{01} = \\frac{ 1 }{ \\frac{ \\text{Near} - \\text{Far}}{\\text{Near}} d + \\frac{Far}{\\text{Near}}} $$ 幸运的是，Unity 提供了两个辅助函数来为我们进行上述的计算过程一LinearEyeDepth和Linear01Depth。\nLinearEyeDepth负责把深度纹理的采样结果转换到视角空间下的深度值，也就是我们上面得到的Z\u0026rsquo;visw。\n而Linear01Depth 则会返回一个范围在[0,1]的线性深度值，也就是我们上面得到的 Z01。这两个函数内部使用了内置的 ZBuferParams 变量来得到远近裁剪平面的距离。如果我们需要获取深度+法线纹理，可以直接使用tex2D数对CameraDepthNormalsTexture进行采样，得到里面存储的深度和法线信息。Unity 提供了辅助函数来为我们对这个采样结果进行解码，从而得到深度值和法线方向。这个函数是DecodeDepthNormal，它在UnityCG.cginc 里被定义：\n1 2 3 4 5 inline void DecodeDepthNormal( float4 enc,out float depth, out float3 normal) { depth =DecodeFloatRG(enc.zw); normal=DecodeViewNormalStereo(enc); } DecodeDepthNormal的第一个参数是对深度+法线纹理的采样结果，这个采样结果是 Unity 对深度和法线信息编码后的结果,它的xy分量存储的是视角空间下的法线信息,而深度信息被编码进了 zw 分量。通过调用 DecodeDepthNormal 函数对采样结果解码后，我们就可以得到解码后的深度值和法线。这个深度值是范围在[0.11的线性深度值(这与单独的深度纹理中存储的深度值不同)，而得到的法线则是视角空间下的法线方向。同样，我们也可以通过调用DecodeFloatRG 和DecodeViewNormalStereo 来解码深度+法线纹理中的深度和法线信息。\n至此，我们已经学会了如何在 Unity 里获取及使用深度和法线纹理。下面，我们会学习如何使用它们实现各种屏幕特效。\n3）查看深度和法线纹理 很多时候，我们希望可以查看生成的深度和法线纹理，以便对Shader 进行调试。Unity5提供了一个方便的方法来查看摄像机生成的深度和法线纹理，这个方法就是利用帧调试器(FrameDebugger)。图13.3 显示了使用帧调试器査看到的深度纹理和深度+法线纹理。\n使用帧调试器查看到的深度纹理是非线性空间的深度值,而深度+法线纹理都是由Unity 编码后的结果。有时，显示出线性空间下的深度信息或解码后的法线方向会更加有用。此时，我们可以自行在片元着色器中输出转换或解码后的深度和法线值，如图13.4所示。输出代码非常简单，我们可以使用类似下面的代码来输出线性深度值:\n1 2 3 float depth = SAMPLE DEPTH TEXTURE( CameraDepthTexture,i.uv); float linearDepth = Linear01Depth(depth); return fixed4(linearDepth,linearDepth,linearDepth，1.0); 或是输出法线方向:\n1 2 fixed3 normal = DecodeViewNormalStereo(tex2D(_CameraDepthNormalsTexture, i.uv).xy); return fixed4(normal*0.5+0.5, 1.0); 在查看深度纹理时，读者得到的画面有可能几乎是全黑或全白的。这时候读者可以把摄像机的远裁剪平面的距离(Unity默认为1000)调小，使视锥体的范围刚好覆盖场景的所在区域。这是因为，由于投影变换时需要覆盖从近裁剪平面到远裁剪平面的所有深度区域，当远裁剪平面的距离过大时，会导致离摄像机较近的距离被映射到非常小的深度值，如果场景是一个封闭的区域(如图13.4所示)，那么这就会导致画面看起来几乎是全黑的。相反，如果场景是一个开放区域，且物体离摄像机的距离较远，就会导致画面几乎是全白的。\n2.再谈运动模糊 在12.6节中，我们学习了如何通过混合多张屏幕图像来模拟运动模糊的效果。但是，另一种应用更加广泛的技术则是使用速度映射图。速度映射图中存储了每个像素的速度，然后使用这个速度来决定模糊的方向和大小。速度缓冲的生成有多种方法，一种方法是把场景中所有物体的速度渲染到一张纹理中。但这种方法的缺点在于需要修改场景中所有物体的 Shader 代码，使其添加计算速度的代码并输出到一个渲染纹理中。\n《GPU Gems3》在第27章(http://http.developer,nvidia.com/GPUGems3/gpugems3 ch27.html)中介绍了一种生成速度映射图的方法。这种方法利用深度纹理在片元着色器中为每个像素计算其在世界空间下的位置，这是通过使用当前的视角投影矩阵的逆矩阵对NDC下的顶点坐标进行变换得到的。当得到世界空间中的顶点坐标后，我们使用前一帧的视角投影矩阵对其进行变换，得到该位置在前一帧中的 NDC坐标。然后，我们计算前一帧和当前帧的位置差，生成该像素的速度。这种方法的优点是可以在一个屏幕后处理步骤中完成整个效果的模拟，但缺点是需要在片元着色器中进行两次矩阵乘法的操作，对性能有所影响。为了使用深度纹理模拟运动模糊，我们需要进行如下准备工作。\n为了使用深度纹理模拟运动模糊，我们需要进行如下准备工作。(1)新建一个场景。在本书资源中，该场景名为Scene132。在 Unity5.2中，默认情况下场景将包含一个摄像机和一个平行光,并且使用了内置的天空盒子。在Window→ Lighting→Skybox中去掉场景中的天空盒子。 (2)我们需要搭建一个测试运动模糊的场景。在本书资源的实现中，我们构建了一个包含3面墙的房间，并放置了4个立方体，它们都使用了我们在95节中创建的标准材质。同时，我们把本书资源中的 Translating.cs脚本拖曳给摄像机，让其在场景中不断运动。(3)新建一个脚本。在本书资源中，该脚本名为MotionBlurWithDepthTexture.cs。把该脚本拖曳到摄像机上。 (4)新建一个Unity Shader。在本书资源中，该Shader 名为Chapter13-MotionBlurWithDepthTexture。\nMotionBlurWithDepthTexture.cs 我们首先来编写 MotionBlurWithDepthTexture.cs脚本。打开该脚本，并进行如下修改。\n(1)首先，继承12.1节中创建的基类\n1 public class MotionBlurWithDepthTexture :PostEffectsBase { (2)声明该效果需要的Shader，并据此创建相应的材质:\n1 2 public Shader motionBlurShader; private Material motionBlurMaterial = null; 1 2 3 4 5 6 public Material material { get { motionBlurMaterial = CheckShaderAndCreateMaterial(motionBlurShader, motionBlurMaterial); return motionBlurMaterial; } } 3)定义运动模糊时模糊图像使用的大小:\n1 2 [Range(0.0f, 1.0f)] public float blurSize = 0.5f; (4)由于本节需要得到摄像机的视角和投影矩阵，我们需要定义一个Camera 类型的变量,以获取该脚本所在的摄像机组件:\n1 2 3 4 5 6 7 8 9 private Camera myCamera; public Camera camera { get { if (myCamera == null) { myCamera = GetComponent\u0026lt;Camera\u0026gt;(); } return myCamera; } } (5)我们还需要定义一个变量来保存上一帧摄像机的视角*投影矩阵:\n1 private Matrix4x4 previousViewProjectionMatrix; (6)由于本例需要获取摄像机的深度纹理，我们在脚本的 OnEnable 函数中设置摄像机的状态:\n1 2 3 void onEnable(){ camera.depthTextureMode= DepthTextureMode .Depth; } (7)最后，我们实现了OnRenderlmage 函数:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 void OnRenderImage (RenderTexture src, RenderTexture dest) { if (material != null) { material.SetFloat(\u0026#34;_BlurSize\u0026#34;, blurSize); material.SetMatrix(\u0026#34;_PreviousViewProjectionMatrix\u0026#34;, previousViewProjectionMatrix); Matrix4x4 currentViewProjectionMatrix = camera.projectionMatrix * camera.worldToCameraMatrix; Matrix4x4 currentViewProjectionInverseMatrix = currentViewProjectionMatrix.inverse; material.SetMatrix(\u0026#34;_CurrentViewProjectionInverseMatrix\u0026#34;, currentViewProjectionInverseMatrix); previousViewProjectionMatrix = currentViewProjectionMatrix; Graphics.Blit (src, dest, material); } else { Graphics.Blit(src, dest); } } 上面的 OnRenderImage 函数很简单，我们首先需要计算和传递运动模糊使用的各个属性。才例需要使用两个变换矩阵——前一帧的视角\\*投影矩阵以及当前帧的视角\\*投影矩阵的逆矩阵。因此，我们通过调用 camera.worldToCameraMatrix和 camera.projectionMatrix来分别得到当前摄像机的视角矩阵和投影矩阵。对它们相乘后取逆，得到当前帧的视角*投影矩阵的逆矩阵，并传递给材质。然后，我们把取逆前的结果存储在 previousViewProjectionMatrix 变量中，以便在下一帧时传递给材质的 PreviousViewProjectionMatrix属性。\nChapter13-MotionBlurWithDepthTexture 下面，我们来实现 Shader 的部分。打开 Chapter13-MotionBlurWithDepthTexture，进行如下修改。\n(1)我们首先需要声明本例使用的各个属性:\n1 2 3 4 Properties{ _MainTex(\u0026#34;Base(RGB)\u0026#34;2D)=\u0026#34;white\u0026#34;{} _BlurSize (\u0026#34;Blur Size\u0026#34;Float)=1.0 } MainTex 对应了输入的渲染纹理，BlurSize 是模糊图像时使用的参数。我们注意到，虽然在脚本里设置了材质的 PreviousViewProjectionMatrix和CurrentViewProjectionInverseMatrix属性，但并没有在Properties块中声明它们。这是因为Unity 没有提供矩阵类型的属性，但我们仍然可以在CG代码块中定义这些矩阵，并从脚本中设置它们。\n(2)在本节中,我们使用 CGINCLUDE来组织代码。我们在 SubShader块中利用CGINCLUDE和 ENDCG 语义来定义一系列代码:\n1 2 3 4 5 6 7 SubShader { CGINCLUDE …… ENDCG …… } (3)声明代码中需要使用的各个变量:\n1 2 3 4 5 6 sampler2DMainTex; half4 MainTex TexelSize; sampler2DCameraDepthTexture; float4x4 CurrentViewProjectionInverseMatrix; float4x4PreviousViewProjectionMatrix; half Blursize; 在上面的代码中，除了定义在 Properties 声明的 MainTex和 BlurSize 属性，我们还声明了其他三个变量。CameraDepthTexture 是Unity 传递给我们的深度纹理，而 CurrentViewProjectionInverseMatrix和PreviousViewProiectionMatrix是由脚本传递而来的矩阵。除此之外，我们还声明了MainTex TexelSize 变量，它对应了主纹理的纹素大小，我们需要使用该变量来对深度纹理的采样坐标进行平台差异化处理(详见5.6.1节)。\n(4)顶点着色器的代码和之前使用多次的代码基本一致，只是增加了专门用于对深度纹理采样的纹理坐标变量:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 struct v2f { float4 pos : SV_POSITION; half2 uv : TEXCOORD0; half2 uv_depth : TEXCOORD1; }; v2f vert(appdata_img v) { v2f o; o.pos = UnityObjectToClipPos(v.vertex); o.uv = v.texcoord; o.uv_depth = v.texcoord; #if UNITY_UV_STARTS_AT_TOP if (_MainTex_TexelSize.y \u0026lt; 0) o.uv_depth.y = 1 - o.uv_depth.y; #endif return o; } 由于在本例中，我们需要同时处理多张渲染纹理，因此在DirectX这样的平台上，我们需要处理平台差异导致的图像翻转问题。在上面的代码中，我们对深度纹理的采样坐标进行了平台差异化处理，以便在类似 Directx的平台上，在开启了抗锯齿的情况下仍然可以得到正确的结果,\n(5)片元着色器是算法的重点所在:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 fixed4 frag(v2f i) : SV_Target { // Get the depth buffer value at this pixel. float d = SAMPLE_DEPTH_TEXTURE(_CameraDepthTexture, i.uv_depth); // H is the viewport position at this pixel in the range -1 to 1. float4 H = float4(i.uv.x * 2 - 1, i.uv.y * 2 - 1, d * 2 - 1, 1); // Transform by the view-projection inverse. float4 D = mul(_CurrentViewProjectionInverseMatrix, H); // Divide by w to get the world position. float4 worldPos = D / D.w; // Current viewport position float4 currentPos = H; // Use the world position, and transform by the previous view-projection matrix. float4 previousPos = mul(_PreviousViewProjectionMatrix, worldPos); // Convert to nonhomogeneous points [-1,1] by dividing by w. previousPos /= previousPos.w; // Use this frame\u0026#39;s position and last frame\u0026#39;s to compute the pixel velocity. float2 velocity = (currentPos.xy - previousPos.xy)/2.0f; float2 uv = i.uv; float4 c = tex2D(_MainTex, uv); uv += velocity * _BlurSize; for (int it = 1; it \u0026lt; 3; it++, uv += velocity * _BlurSize) { float4 currentColor = tex2D(_MainTex, uv); c += currentColor; } c /= 3; return fixed4(c.rgb, 1.0); } 我们首先需要利用深度纹理和当前帧的视角投影矩阵的逆矩阵来求得该像素在世界空间下的坐标。过程开始于对深度纹理的采样，我们便用内置的SAMPLE DEPTH TEXTURE宏和纹理坐标对深度纹理进行采样，得到了深度值 d。由 13.1.2节可知,d是由 NDC下的坐标映射而来的。我们想要构建像素的 NDC 坐标 ，就需要把这个深度值重新映射回 NDC。这个映射很简单，只需要使用原映射的反函数即可，即d2-1。同样，NDC的x分量可以由像素的纹理坐标映射而来(NDC下的xyz分量范围均为[-1,1])。当得到NDC下的坐标H后，我们就可以使用当前帧的视角*投影矩阵的逆矩阵对其进行变换，并把结果值除以它的w分量来得到世界空间下的坐标表示 worldPos.\n一旦得到了世界空间下的坐标，我们就可以使用前一帧的视角*投影矩阵对它进行变换，得到前一帧在 NDC下的坐标 previousPos。然后，我们计算前一帧和当前帧在屏幕空间下的位置差,得到该像素的速度 velocity。 当得到该像素的速度后，我们就可以使用该速度值对它的邻域像素进行采样，相加后取平均值得到一个模糊的效果。采样时我们还使用了_BlurSize 来控制采样距离。\n(6)然后，我们定义了运动模糊所需的 Pass:\n1 2 3 4 5 6 7 8 9 10 Pass { ZTest Always Cull Off ZWrite Off CGPROGRAM #pragma vertex vert #pragma fragment frag ENDCG } (7)最后，我们关闭了shader的Fallback:\n1 Fallback Off 完成后返回编辑器，并把 Chapter13-MotionBlurWithDepthTexture 拖曳到摄像机的 MotionBlurWithDepthTexture.cs脚本中的 motionBlurShader 参数中。当然，我们可以在 MotionBlurWithDepthTexture.cs的脚本面板中将 motionBlurShader 参数的默认值设置为 Chapter13-MotionBlurWithDepthTexture，这样就不需要以后使用时每次都手动拖曳了。\n本节实现的运动模糊适用于场景静止、摄像机快速运动的情况，这是因为我们在计算时只考虑了摄像机的运动。因此，如果读者把本节中的代码应用到一个物体快速运动而摄像机静止的场景，会发现不会产生任何运动模糊效果。如果我们想要对快速移动的物体产生运动模糊的效果，就需要生成更加精确的速度映射图。读者可以在 Unity 自带的ImageEfect包中找到更多的运动模糊的实现方法。 本节选择在片元着色器中使用逆矩阵来重建每个像素在世界空间下的位置。但是，这种做法往往会影响性能，在13.3节中，我们会介绍一种更快速的由深度纹理重建世界坐标的方法。\n3.全局雾效 雾效(Fog)是游戏里经常使用的一种效果。Unity内置的雾效可以产生基于距离的线性或指数雾效。然而，要想在自己编写的顶点/片元着色器中实现这些雾效，我们需要在Shader 中添加#pragma multi compile fog指令，同时还需要使用相关的内置宏，例如 UNITY FOG COORDS、UNITY TRANSFERFOG和UNITY APPLY FOG等。这种方法的缺点在于，我们不仅需要为场景中所有物体添加相关的渲染代码，而且能够实现的效果也非常有限。当我们需要对雾效进行些个性化操作时，例如使用基于高度的雾效等，仅仅使用Unity内置的雾效就变得不再可行。\n在本节中，我们将会学习一种基于屏幕后处理的全局雾效的实现。使用这种方法，我们不需要更改场景内渲染的物体所使用的Shader代码，而仅仅依靠一次屏幕后处理的步骤即可这种方法的自由性很高，我们可以方便地模拟各种雾效，例如均匀的雾效、基于距离的线性指数雾效、基于高度的雾效等。在学习完本节后，我们可以得到类似图13.5中的效果。\n基于屏幕后处理的全局雾效的关键是，根据深度纹理来重建每个像素在世界空间下的位置。尽管在 13.2节中，我们在模拟运动模糊时已经实现了这个要求，即构建出当前像素的NDC坐标，再通过当前摄像机的视角*投影矩阵的逆矩阵来得到世界空间下的像素坐标，但是，这样的实现需要在片元着色器中进行矩阵乘法的操作，而这通常会影响游戏性能。\n在本节中，我们将会学习一个快速从深度纹理中重建世界坐标的方法。这种方法首先对图像空间下的视锥体射线(从摄像机出发，指向图像上的某点的射线)进行插值，这条射线存储了该像素在世界空间下到摄像机的方向信息。然后，我们把该射线和线性化后的视角空间下的深度值相乘，再加上摄像机的世界位置，就可以得到该像素在世界空间下的位置。\n当我们得到世界坐标后，就可以轻松地使用各个公式来模拟全局雾效了。\n重建世界坐标 在开始动手写代码之前，我们首先来了解如何从深度纹理中重建世界坐标。我们知道，坐标系中的一个顶点坐标可以通过它相对于另一个顶点坐标的偏移量来求得。\n重建像素的世界坐标也是基于这样的思想。我们只需要知道摄像机在世界空间下的位置，以及世界空间下该像素相对于摄像机的偏移量，把它们相加就可以得到该像素的世界坐标。整个过程可以使用下面的代码来表示：\n1 float3 worldPos = _WorldSpaceCameraPos + linearDepth * i.interpolatedRay.xyz; 其中，WorldSpaceCameraPos是摄像机在世界空间下的位置，这可以由Unity 的内置变量直接访问得到。而 linearDepth*interpolatedRay 则可以计算得到该像素相对于摄像机的偏移量,linearDepth 是由深度纹理得到的线性深度值，interpolatedRay 是由顶点着色器输出并插值后得到的射线，它不仅包含了该像素到摄像机的方向，也包含了距离信息。linearDepth的获取我们已经在 13.1.2节中详细解释过了，因此，本节着重解释imnterpolatedRay的求法。 interpolatedRay 来源于对近裁剪平面的4个角的某个特定向量的插值，这4个向量包含了它们到摄像机的方向和距离信息，我们可以利用摄像机的近裁剪平面距离、FOV、横纵比计算而得。图 13.6显示了计算时使用的一些辅助向量。为了方便计算，我们可以先计算两个向量——toTop和toRight,它们是起点位于近裁剪平面中心、分别指向摄像机正上方和正右方的向量。它们的计算公式如下:\n其中，Near 是近裁剪平面的距离，FOV是竖直方向的视角范围，camera.up、camera.right 分别对应了摄像机的正上方和正右方。\n当得到这两个辅助向量后，我们就可以计算4个角相对于摄像机的方向了。我们以左上角为例(见图13.6中的TL点)，它的计算公式如下: $$ TL=camera.forwardNear+tolop-toRight $$ 读者可以依靠基本的矢量运算验证上面的结果。同理，其他3个角的计算也是类似的: $$ TR=camera.forward·Near+toTop+toRight $$$$ BL=camera.forward·Near-toTop-toRight $$$$ BR=camera.forward·Near-toTop+toRight $$注意，上面求得的4个向量不仅包含了方向信息，它们的模对应了4个点到摄像机的空间距离。由于我们得到的线性深度值并非是点到摄像机的欧式距离，而是在z方向上的距离，因此，我们不能直接使用深度值和4个角的单位方向的乘积来计算它们到摄像机的偏移量，如图13.7所示。\n想要把深度值转换成到摄像机的欧式距离也很简单，我们以TL点为例，根据相似三角形原理，TL所在的射线上，像素的深度值和它到摄像机的实际距离的比等于近裁剪平面的距离和 TL向量的模的比，即\n由此可得，我们需要的T距离摄像机的欧氏距离 dist:\n由于4个点相互对称，因此其他3个向量的模和TL相等，即我们可以使用同一个因子和单位向量相乘，得到它们对应的向量值:\n屏幕后处理的原理是使用特定的材质去渲染一个刚好填充整个屏幕的四边形面片。这个四边形面片的4个顶点就对应了近裁剪平面的4个角。因此，我们可以把上面的计算结果传递给顶点着色器，顶点着色器根据当前的位置选择它所对应的向量，然后再将其输出，经插值后传递给片元着色器得到 interpolatedRay，我们就可以直接利用本节一开始提到的公式重建该像素在世界空间下的位置了。\n雾的计算 在简单的雾效实现中，我们需要计算一个雾效系数f,作为混合原始颜色和雾的颜色的混合系数:\n1 float3 afterFog = f * fogColor + ( 1 - f ) * origColor; 这个雾效系数 f 有很多计算方法。在Unity内置的雾效实现中,支持三种雾的计算方式——线性(Linear)、指数(Exponential)以及指数的平方(Exponential Squared)。当给定距离z后，f的计算公式分别如下:\nLinear：\nExponential:\nExponential Squared:\n在本节中，我们将使用类似线性雾的计算方式，计算基于高度的雾效。具体方法是，当给定一点在世界空间下的高度y后，f的计算公式为:\n实现 为了在 Unity中实现基于屏幕后处理的雾效，我们需要进行如下准备工作。\n(1)新建一个场景。在本书资源中，该场景名为Scene 133。在Unity5.2中，默认情况下场景将包含一个摄像机和一个平行光,并且使用了内置的天空盒子。在 Window-\u0026gt;Lighting-\u0026gt;Skybox中去掉场景中的天空盒子。\n(2)我们需要搭建一个测试雾效的场景。在本书资源的实现中，我们构建了一个包含3面墙的房间，并放置了两个立方体和两个球体，它们都使用了我们在9.5节中创建的标准材质。同时我们把本书资源中的 Translating.cs脚本拖曳给摄像机，让其在场景中不断运动。\n(3)新建一个脚本。在本书资源中，该脚本名为FogWithDepthTexture.cs。把该脚本拖曳到摄像机上。\n(4)新建一个 Unity Shader。在本书资源中，该Shader名为 Chapter13-FogWithDepthTexture。\nFogWithDepthTexture.cs脚本 我们首先来编写 FogWithDepthTexture.cs脚本。\n1 public class FogWithDepthTexture : PostEffectsBase { 声明该效果需要的Shader，并据此创建相应的材质：\n1 2 3 4 5 6 7 8 9 public Shader fogShader; private Material fogMaterial = null; public Material material { get { fogMaterial = CheckShaderAndCreateMaterial(fogShader, fogMaterial); return fogMaterial; } } (3)在本节中，我们需要获取摄像机的相关参数，如近裁剪平面的距离、FOV等，同时还需要获取摄像机在世界空间下的前方、上方和右方等方向，因此我们用两个变量存储摄像机的Camera 组件和Transform组件：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 private Camera myCamera; public Camera camera { get { if (myCamera == null) { myCamera = GetComponent\u0026lt;Camera\u0026gt;(); } return myCamera; } } private Transform myCameraTransform; public Transform cameraTransform { get { if (myCameraTransform == null) { myCameraTransform = camera.transform; } return myCameraTransform; } } （4)定义模拟雾效时使用的各个参数:\n1 2 3 4 5 6 7 [Range(0.0f, 3.0f)] public float fogDensity = 1.0f; public Color fogColor = Color.white; public float fogStart = 0.0f; public float fogEnd = 2.0f; fogDensity 用于控制雾的浓度，fogColor 用于控制雾的颜色。我们使用的雾效模拟函数是基于高度的，因此参数 fogStart 用于控制雾效的起始高度，fogEnd 用于控制雾效的终止高度。\n(5)由于本例需要获取摄像机的深度纹理,我们在脚本的 OnEnable 函数中设置摄像机的相应状态:\n1 2 3 void OnEnable() { camera.depthTextureMode |= DepthTextureMode.Depth; } 6）最后，我们实现了 OnRenderlmage 函数:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 void OnRenderImage (RenderTexture src, RenderTexture dest) { if (material != null) { Matrix4x4 frustumCorners = Matrix4x4.identity; float fov = camera.fieldOfView; float near = camera.nearClipPlane; float aspect = camera.aspect; float halfHeight = near * Mathf.Tan(fov * 0.5f * Mathf.Deg2Rad); Vector3 toRight = cameraTransform.right * halfHeight * aspect; Vector3 toTop = cameraTransform.up * halfHeight; Vector3 topLeft = cameraTransform.forward * near + toTop - toRight; float scale = topLeft.magnitude / near; topLeft.Normalize(); topLeft *= scale; Vector3 topRight = cameraTransform.forward * near + toRight + toTop; topRight.Normalize(); topRight *= scale; Vector3 bottomLeft = cameraTransform.forward * near - toTop - toRight; bottomLeft.Normalize(); bottomLeft *= scale; Vector3 bottomRight = cameraTransform.forward * near + toRight - toTop; bottomRight.Normalize(); bottomRight *= scale; frustumCorners.SetRow(0, bottomLeft); frustumCorners.SetRow(1, bottomRight); frustumCorners.SetRow(2, topRight); frustumCorners.SetRow(3, topLeft); material.SetMatrix(\u0026#34;_FrustumCornersRay\u0026#34;, frustumCorners); material.SetFloat(\u0026#34;_FogDensity\u0026#34;, fogDensity); material.SetColor(\u0026#34;_FogColor\u0026#34;, fogColor); material.SetFloat(\u0026#34;_FogStart\u0026#34;, fogStart); material.SetFloat(\u0026#34;_FogEnd\u0026#34;, fogEnd); Graphics.Blit (src, dest, material); } else { Graphics.Blit(src, dest); } } OnRenderlmage 首先计算了近裁剪平面的四个角对应的向量，并把它们存储在一个矩阵类型的变量(frustumComers)中。计算过程我们已经在13.3.1节中详细解释过了，代码只是套用了之前讲过的公式而已。我们按一定顺序把这四个方向存储到了fustumComers 不同的行中，这个顺序是非常重要的，因为这决定了我们在顶点着色器中使用哪一行作为该点的待插值向量。随后，我们把结果和其他参数传递给材质，并调用 Graphics.Blit(src,dest,material)把渲染结果显示在屏幕上。\n下面，我们来实现 Shader 的部分。打开 Chapter13-FogWithDepthTexture，进行如下修改。\n(1)我们首先需要声明本例使用的各个属性:\n1 2 3 4 5 6 7 Properties { _MainTex (\u0026#34;Base (RGB)\u0026#34;, 2D) = \u0026#34;white\u0026#34; {} _FogDensity (\u0026#34;Fog Density\u0026#34;, Float) = 1.0 _FogColor (\u0026#34;Fog Color\u0026#34;, Color) = (1, 1, 1, 1) _FogStart (\u0026#34;Fog Start\u0026#34;, Float) = 0.0 _FogEnd (\u0026#34;Fog End\u0026#34;, Float) = 1.0 } (2)在本节中,我们使用 CGINCLUDE来组织代码。我们在 SubShader块中利用CGINCLUDE和ENDCG 语义来定义一系列代码:\n1 2 3 4 5 6 SubShader { CGINCLUDE …… ENDCG …… } （3）声明代码中需要使用的各个变量:\n1 2 3 4 5 6 7 8 9 float4x4 _FrustumCornersRay; sampler2D _MainTex; half4 _MainTex_TexelSize; sampler2D _CameraDepthTexture; half _FogDensity; fixed4 _FogColor; float _FogStart; float _FogEnd; FrustumCormersRay虽然没有在Properties中声明，但仍可由脚本传递给 Shader。除了Properties 中声明的各个属性，我们还声明了深度纹理 CameraDepthTexture，Unity 会在背后把得到的深度纹理传递给该值。\n(4)定义顶点着色器:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 struct v2f { float4 pos : SV_POSITION; half2 uv : TEXCOORD0; half2 uv_depth : TEXCOORD1; float4 interpolatedRay : TEXCOORD2; }; v2f vert(appdata_img v) { v2f o; o.pos = UnityObjectToClipPos(v.vertex); o.uv = v.texcoord; o.uv_depth = v.texcoord; #if UNITY_UV_STARTS_AT_TOP if (_MainTex_TexelSize.y \u0026lt; 0) o.uv_depth.y = 1 - o.uv_depth.y; #endif int index = 0; if (v.texcoord.x \u0026lt; 0.5 \u0026amp;\u0026amp; v.texcoord.y \u0026lt; 0.5) { index = 0; } else if (v.texcoord.x \u0026gt; 0.5 \u0026amp;\u0026amp; v.texcoord.y \u0026lt; 0.5) { index = 1; } else if (v.texcoord.x \u0026gt; 0.5 \u0026amp;\u0026amp; v.texcoord.y \u0026gt; 0.5) { index = 2; } else { index = 3; } #if UNITY_UV_STARTS_AT_TOP if (_MainTex_TexelSize.y \u0026lt; 0) index = 3 - index; #endif o.interpolatedRay = _FrustumCornersRay[index]; return o; } 在 v2f 结构体中，我们除了定义顶点位置、屏幕图像和深度纹理的纹理坐标外，还定义了interpolatedRay 变量存储插值后的像素向量。\n在顶点着色器中，我们对深度纹理的采样坐标进行了平台差异化处理。更重要的是，我们要决定该点对应了4个角中的哪个角。我们采用的方法是判断它的纹理坐标。我们知道，在 Unity 中，纹理坐标的(0,0)点对应了左下角，而(1,1)点对应了右上角。我们据此来判断该顶点对应的索引，这个对应关系和我们在脚本中对frustumCorners 的赋值顺序是一致的。实际上，不同平台的纹理坐标不一定是满足上面的条件的，例如 DirectX 和Metal 这样的平台，左上角对应了(0,0)点，但大多数情况下 Unity 会把这些平台下的屏幕图像进行翻转，因此我们仍然可以利用这个条件。\n但如果在类似DirectX 的平台上开启了抗锯齿，Unity就不会进行这个翻转。为了此时仍然可以得到相应顶点位置的索引值，我们对索引值也进行了平台差异化处理(详见5.6.1节)，以便在必要时也对索引值进行翻转。最后，我们使用索引值来获取 FrustumCornersRay中对应的行作为该顶点的interpolatedRay 值。\n尽管我们这里使用了很多判断语句，但由于屏幕后处理所用的模型是一个四边形网格，只包含4个顶点，因此这些操作不会对性能造成很大影响。\n(5)我们定义了片元着色器来产生雾效:\n1 2 3 4 5 6 7 8 9 10 11 12 fixed4 frag(v2f i) : SV_Target { float linearDepth = LinearEyeDepth(SAMPLE_DEPTH_TEXTURE(_CameraDepthTexture, i.uv_depth)); float3 worldPos = _WorldSpaceCameraPos + linearDepth * i.interpolatedRay.xyz; float fogDensity = (_FogEnd - worldPos.y) / (_FogEnd - _FogStart); fogDensity = saturate(fogDensity * _FogDensity); fixed4 finalColor = tex2D(_MainTex, i.uv); finalColor.rgb = lerp(finalColor.rgb, _FogColor.rgb, fogDensity); return finalColor; } 首先,我们需要重建该像素在世界空间中的位置。为此,我们首先使用 SAMPLE DEPTH TEXTURE对深度纹理进行采样，再使用LinearEyeDepth 得到视角空间下的线性深度值。之后，与imnterpolatedRay 相乘后再和世界空间下的摄像机位置相加，即可得到世界空间下的位置。得到世界坐标后，模拟雾效就变得非常容易。在本例中，我们选择实现基于高度的雾效模拟，计算公式可参见13.3.2节。我们根据材质属性FogEnd和 FogStart计算当前的像素高度 worldPos.y对应的雾效系数 fogDensity，再和参数 FogDensity 相乘后，利用 saturate 函数截取到[0,1]范围内，作为最后的雾效系数。然后，我们使用该系数将雾的颜色和原始颜色进行混合后返回。读者也可以使用不同的公式来实现其他种类的雾效。\n6)随后，我们定义了雾效渲染所需的Pass:\n1 2 3 4 5 6 7 8 9 10 Pass { ZTest Always Cull Off ZWrite Off CGPROGRAM #pragma vertex vert #pragma fragment frag ENDCG } (7)最后，我们关闭了Shader的Fallback:\n1 Fallback Off 完成后返回编辑器,并把 Chapter13-FogWithDepthTexture 拖曳到摄像机的FogWithDepthTexture.cs脚本中的 fogShader 参数中。\n本节介绍的使用深度纹理重建像素的世界坐标的方法是非常有用的。但需要注意的是，这里的实现是基于摄像机的投影类型是透视投影的前提下。如果需要在正交投影的情况下重建世界坐标，需要使用不同的公式，但请读者相信，这个过程不会比透视投影的情况更加复杂。有兴趣的读者可以尝试自行推导，或参考这篇博客(http:/www.derschmale.com/2014/03/19/reconstructingpositions-from-the-depth-buffer-pt-2-perspective-and-orthographic-general-case/)来实现。\n4.（新）边缘检测 在 12.3节中，我们曾介绍如何使用 Sobel 算子对屏幕图像进行边缘检测，实现描边的效果。但是，这种直接利用颜色信息进行边缘检测的方法会产生很多我们不希望得到的边缘线，如图 13.8所示。可以看出，物体的纹理、阴影等位置也被描上黑边，而这往往不是我们希望看到的。在本节中，我们将学习如何在深度和法线纹理上进行边缘检测，这些图像不会受纹理和光照的影响，而仅仅保存了当前渲染物体的模型信息，通过这样的方式检测出来的边缘更加可靠。在学习完本节后，我们可以得到类似图13.9中的效果。\n与12.3节使用Sobel算子不同，本节将使用Roberts算子来进行边缘检测。它使用的卷积核如图13.10所示。\nRoberts 算子的本质就是计算左上角和右下角的差值，乘以右上角和左下角的差值，作为评估边缘的依据。在下面的实现中，我们也会按这样的方式，取对角方向的深度或法线值，比较它们之间的差值，如果超过某个阈值(可由参数控制)，就认为它们之间存在一条边。\n首先，我们需要进行如下准备工作。\n(1)新建一个场景。在本书资源中，该场景名为Scene 134。在Unity5.2中，默认情况下场景将包含一个摄像机和一个平行光，并且使用了内置的天空盒子。在 Window→Lighting→Skybox中去掉场景中的天空盒子。\n(2)我们需要搭建一个测试雾效的场景。在本书资源的实现中，我们构建了一个包含3面墙的房间，并放置了两个立方体和两个球体，它们都使用了我们在95节中创建的标准材质。同时，我们把本书资源中的 Translating.cs脚本拖曳给摄像机，让其在场景中不断运动。\n(3)新建一个脚本。在本书资源中，该脚本名为EdgeDetectNormalsAndDepth.cs。把该脚本拖曳到摄像机上。\n(4)新建一个Unity Shader。在本书资源中，该Shader 名为 Chapter13-\nEdgeDetection.cs 我们首先来编写 EdgeDetectNormalsAndDepth.cs脚本。该脚本与 12.3 节中实现的 EdgeDetection.cs脚本几乎完全一样，只是添加了一些新的属性。为了完整性，我们再次说明对该脚本进行的修改。\n(1)首先，继承12.1节中创建的基类:\n1 2 3 public class EdgeDetectNormalsAndDepth :PostEffectsBase{ …… } (2)声明该效果需要的Shader，并据此创建相应的材质:\n1 2 3 4 5 6 7 8 public Shader edgeDetectShader; private Material edgeDetectMaterial = null; public Material material { get { edgeDetectMaterial = CheckShaderAndCreateMaterial(edgeDetectShader, edgeDetectMaterial); return edgeDetectMaterial; } } (3)在脚本中提供了调整边缘线强度描边颜色以及背景颜色的参数。同时添加了控制采样距离以及对深度和法线进行边缘检测时的灵敏度参数:\n1 2 3 4 5 6 7 8 9 10 11 12 [Range(0.0f, 1.0f)] public float edgesOnly = 0.0f; public Color edgeColor = Color.black; public Color backgroundColor = Color.white; public float sampleDistance = 1.0f; public float sensitivityDepth = 1.0f; public float sensitivityNormals = 1.0f; sampleDistance用于控制对深度+法线纹理采样时，使用的采样距离。从视觉上来看，sampleDistance 值越大，描边越宽。sensitivityDepth 和 sensitivityNormals 将会影响当邻域的深度值或法线值相差多少时，会被认为存在一条边界。如果把灵敏度调得很大，那么可能即使是深度或法线上很小的变化也会形成一条边。\n(4)由于本例需要获取摄像机的深度+法线纹理，我们在脚本的 OnEnable 函数中设置摄像机的相应状态:\n1 2 3 void OnEnable(){ GetComponent\u0026lt;Camera\u0026gt;().depthTextureMode |= DepthTextureMode.DepthNormals; } (5)实现OnRenderlmage函数，把各个参数传递给材质:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 [ImageEffectOpaque] void OnRenderImage (RenderTexture src, RenderTexture dest) { if (material != null) { material.SetFloat(\u0026#34;_EdgeOnly\u0026#34;, edgesOnly); material.SetColor(\u0026#34;_EdgeColor\u0026#34;, edgeColor); material.SetColor(\u0026#34;_BackgroundColor\u0026#34;, backgroundColor); material.SetFloat(\u0026#34;_SampleDistance\u0026#34;, sampleDistance); material.SetVector(\u0026#34;_Sensitivity\u0026#34;, new Vector4(sensitivityNormals, sensitivityDepth, 0.0f, 0.0f)); Graphics.Blit(src, dest, material); } else { Graphics.Blit(src, dest); } } 需要注意的是，这里我们为OnRenderlmage 函数添加了[mageEfectOpaque]属性。我们曾在 12.1节中提到过该属性的含义。在默认情况下，OnRenderlmage 函数会在所有的不透明和透明的 Pass 执行完毕后被调用，以便对场景中所有游戏对象都产生影响。但有时，我们希望在不透明的Pass(即渲染队列小于等于2500的Pass，内置的 Background、Geometry和 AlphaTest 渲染队列均在此范围内)执行完毕后立即调用该函数，而不对透明物体(渲染队列为Transparent 的Pass)产生影响，此时，我们可以在 OnRenderlmage 函数前添加 ImageEfectOpaque 属性来实现这样的目的。在本例中，我们只希望对不透明物体进行描边，而不希望透明物体也被描边，因此需要添加该属性。\nChapter13-EdgeDetectNormalAndDepth， 1)声明\n1 2 3 4 5 6 7 8 Properties { _MainTex (\u0026#34;Base (RGB)\u0026#34;, 2D) = \u0026#34;white\u0026#34; {} _EdgeOnly (\u0026#34;Edge Only\u0026#34;, Float) = 1.0 _EdgeColor (\u0026#34;Edge Color\u0026#34;, Color) = (0, 0, 0, 1) _BackgroundColor (\u0026#34;Background Color\u0026#34;, Color) = (1, 1, 1, 1) _SampleDistance (\u0026#34;Sample Distance\u0026#34;, Float) = 1.0 _Sensitivity (\u0026#34;Sensitivity\u0026#34;, Vector) = (1, 1, 1, 1) } 其中， Sensitivity 的xy分量分别对应了法线和深度的检测灵敏度,zw 分量则没有实际用途。\n(2)在本节中,我们使用 CGINCLUDE来组织代码。我们在 SubShader 块中利用CGINCLUDE和ENDCG语义来定义一系列代码:\n1 2 3 4 5 6 7 SubShader { CGINCLUDE …… ENDCG …… } (3)为了在代码中访问各个属性，我们需要在CG代码块中声明对应的变量:\n1 2 3 4 5 6 7 8 9 sampler2D _MainTex; half4 _MainTex_TexelSize; fixed _EdgeOnly; fixed4 _EdgeColor; fixed4 _BackgroundColor; float _SampleDistance; half4 _Sensitivity; sampler2D _CameraDepthNormalsTexture; 在上面的代码中，我们声明了需要获取的深度+法线纹理CameraDepthNormalsTexture。由于我们需要对邻域像素进行纹理采样，所以还声明了存储纹素大小的变量MainTexTexelSize。\n(4)定义顶点着色器\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 struct v2f { float4 pos : SV_POSITION; half2 uv[5]: TEXCOORD0; }; v2f vert(appdata_img v) { v2f o; o.pos = UnityObjectToClipPos(v.vertex); half2 uv = v.texcoord; o.uv[0] = uv; #if UNITY_UV_STARTS_AT_TOP if (_MainTex_TexelSize.y \u0026lt; 0) uv.y = 1 - uv.y; #endif o.uv[1] = uv + _MainTex_TexelSize.xy * half2(1,1) * _SampleDistance; o.uv[2] = uv + _MainTex_TexelSize.xy * half2(-1,-1) * _SampleDistance; o.uv[3] = uv + _MainTex_TexelSize.xy * half2(-1,1) * _SampleDistance; o.uv[4] = uv + _MainTex_TexelSize.xy * half2(1,-1) * _SampleDistance; return o; } 我们在 v2f结构体中定义了一个维数为5的纹理坐标数组。这个数组的第一个坐标存储了屏幕颜色图像的采样纹理。我们对深度纹理的采样坐标进行了平台差异化处理，在必要情况下对它的竖直方向进行了翻转。数组中剩余的4个坐标则存储了使用Roberts 算子时需要采样的纹理坐标，我们还使用了 SampleDistance 来控制采样距离。通过把计算采样纹理坐标的代码从片元着色器中转移到顶点着色器中，可以减少运算，提高性能。由于从顶点着色器到片元着色器的插值是线性的，因此这样的转移并不会影响纹理坐标的计算结果。\n(5)然后，我们定义了片元着色器:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 fixed4 fragRobertsCrossDepthAndNormal(v2f i) : SV_Target { half4 sample1 = tex2D(_CameraDepthNormalsTexture, i.uv[1]); half4 sample2 = tex2D(_CameraDepthNormalsTexture, i.uv[2]); half4 sample3 = tex2D(_CameraDepthNormalsTexture, i.uv[3]); half4 sample4 = tex2D(_CameraDepthNormalsTexture, i.uv[4]); half edge = 1.0; edge *= CheckSame(sample1, sample2); edge *= CheckSame(sample3, sample4); fixed4 withEdgeColor = lerp(_EdgeColor, tex2D(_MainTex, i.uv[0]), edge); fixed4 onlyEdgeColor = lerp(_EdgeColor, _BackgroundColor, edge); return lerp(withEdgeColor, onlyEdgeColor, _EdgeOnly); } 我们首先使用4个纹理坐标对深度+法线纹理进行采样，再调用CheckSame函数来分别计算对角线上两个纹理值的差值。CheckSame函数的返回值要么是0，要么是1，返回0时表明这两点之间存在一条边界，反之则返回1。它的定义如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 half CheckSame(half4 center, half4 sample) { half2 centerNormal = center.xy; float centerDepth = DecodeFloatRG(center.zw); half2 sampleNormal = sample.xy; float sampleDepth = DecodeFloatRG(sample.zw); // difference in normals // do not bother decoding normals - there\u0026#39;s no need here half2 diffNormal = abs(centerNormal - sampleNormal) * _Sensitivity.x; int isSameNormal = (diffNormal.x + diffNormal.y) \u0026lt; 0.1; // difference in depth float diffDepth = abs(centerDepth - sampleDepth) * _Sensitivity.y; // scale the required threshold by the distance int isSameDepth = diffDepth \u0026lt; 0.1 * centerDepth; // return: // 1 - if normals and depth are similar enough // 0 - otherwise return isSameNormal * isSameDepth ? 1.0 : 0.0; } CheckSame 首先对输入参数进行处理，得到两个采样点的法线和深度值。值得注意的是，这里我们并没有解码得到真正的法线值，而是直接使用了x分量。这是因为我们只需要比较两个采样值之间的差异度，而并不需要知道它们真正的法线值。然后，我们把两个采样点的对应值相减并取绝对值，再乘以灵敏度参数，把差异值的每个分量相加再和一个值比较，如果它们的和小于阈值，则返回1，说明差异不明显，不存在一条边界;否则返回0。最后，我们把法线和深度的检查结果相乘，作为组合后的返回值。\n当通过CheckSame 函数得到边缘信息后，片元着色器就利用该值进行颜色混合，这和12.3节中的步骤一致.\n6)然后，我们定义了边缘检测需要使用的Pass:\n1 2 3 4 5 6 7 8 9 10 Pass { ZTest Always Cull Off ZWrite Off CGPROGRAM #pragma vertex vert #pragma fragment fragRobertsCrossDepthAndNormal ENDCG } (7)最后，我们关闭了该Shader的Fallback:\n1 Fallback Off 完成后返回编辑器，并把 Chapter13-EdgeDetectNormalAndDepth 拖曳到摄像机的 EdgeDetectNormalsAndDepth.cs 脚本中的 edgeDetectShader 参数中。当然，我们可以在 EdgeDetectNormalsAndDepth.cs 的脚本面板中将 edgeDetectShader 参数的默认值设置为 Chapter13-EdgeDetectNormaAndDept，这样就不需要以后使用时每次都手动拖曳了。\n本节实现的描边效果是基于整个屏幕空间进行的，也就是说，场景内的所有物体都会被添加描边效果。但有时，我们希望只对特定的物体进行描边，例如当玩家选中场景中的某个物体后，我们想要在该物体周围添加一层描边效果。这时，我们可以使用Unity提供的Graphics.DrawMesh或Graphics.DrawMeshNow 函数把需要描边的物体再次渲染一遍(在所有不透明物体渲染完毕之后)，然后再使用本节提到的边缘检测算法计算深度或法线纹理中每个像素的梯度值,判断它们是否小于某个阈值，如果是，就在 Shader 中使用 clipO)函数将该像素剔除掉，从而显示出原来的物体颜色。\n扩展阅读 在本章中，我们介绍了如何使用深度和法线纹理实现诸如全局雾效、边缘检测等效果。尽管我们只使用了深度和法线纹理，但实际上我们可以在Unity 中创建任何需要的缓存纹理。这可以通过使用 Unity 的着色器替换(Shader Replacement)功能(即调用 Camera,RenderWithShader(shader.replacementTag)函数)把整个场景再次渲染一遍来得到，而在很多时候，这实际也是 Unity 创建深度和法线纹理时使用的方法。\n深度和法线纹理在屏幕特效的实现中往往扮演了重要的角色。许多特殊的屏幕效果都需要依靠这两种纹理的帮助。Unity曾在2011年的SIGGRAPH(计算图形学的顶级会议)上做了一个关于使用深度纹理实现各种特效的演讲(http://blogs.unity3d,com/2011/09/08/special-efects-withdepth-talk-at-siggraph/)。在这个演讲中，Unity 的工作人员解释了如何利用深度纹理来实现特定物体的描边、角色护盾、相交线的高光模拟等效果。在 Unity的Image Effect(http://docs.unity3d.comManualcomp-ImageEfects.html)包中，读者也可以找到一些传统的使用深度纹理实现屏幕特效的例子，例如屏幕空间的环境遮挡(ScreenSpaceAmbientOcclusion，SSAO)等效果。\n","date":"2024-06-20T22:10:30Z","image":"https://Selaphina.github.io/p/13-%E6%B7%B1%E5%BA%A6%E5%92%8C%E6%B3%95%E7%BA%BF%E7%BA%B9%E7%90%86/cover1_hu_5e2014920a4d8f6.png","permalink":"https://Selaphina.github.io/p/13-%E6%B7%B1%E5%BA%A6%E5%92%8C%E6%B3%95%E7%BA%BF%E7%BA%B9%E7%90%86/","title":"13 深度和法线纹理"},{"content":"1. Blender导出带贴图模型 {{BV1384y1j7wt}}\n1.文件—— 外部数据 ——打包资源——（第二项）将文件写至当前目录\n2.着色器编辑器：\n所有shader节点的贴图文件处，都需要：解包项——将文件写至当前目录（覆盖现有文件）\n操作之后，确保图标变化：\n3.导出——fbx\n两处修改：\n路径模式——复制\n右侧小图标——激活\n!!注意：千万不要勾选【选中的物体】，除非你知道自己在做什么。\n2.形态键动画导出 1.分为两个动画：\n动作动画\n形态键动画\n要求：\n动作动画长度一定要\u0026gt;=形态键动画\n导出：\n1.\n2.\n3.实体化描边 （面试题）用blender等dcc软件实体化描边比在引擎中用shader实时描边的优势？\n原问：“你为什么用shader实时描边，而不是在blender中用实体化描边？这样不是性能更好吗？”\n我：……\n1）打开一个简单的 箭头网格体。将节点改成着色器-自发光（只是为方便查看效果，没有特别的作用）\n2）新建第二个材质，也是自发光节点。材质命名为”描边“，该材质定义描边颜色。\n注意：描边材质勾选 “背面剔除”（Backface Culling）。这能隐藏材质正对摄像机的正面，只让我们从模型侧面和背面看到它，这是形成描边的关键。\n3）添加修改器——生成——实体化\n1：法向翻转；\n翻转：在“法向”选项中勾选翻转。这会使模型背面可见，与“背面剔除”的描边材质配合形成描边\n2：添加材质偏移\n材质偏移：如果模型有多个材质槽，将此参数设置为1，可使实体化修改器使用材质列表中排在下一个（索引号+1）的材质，也就是我们刚刚创建的描边材质。\n最后，只需要调整厚（宽）度就可以查看到描边的效果。\n转到游戏引擎内使用标准着色器会更明显：\n法线外扩与背面显示：当勾选“翻转”法线后，模型表面的背面变得可见。实体化修改器根据厚度值，沿着模型顶点的法线方向创造出一个新的、略大一圈的“外壳”。\n材质赋予与正面剔除：我们将设置了“背面剔除”的黑色材质赋予这个新外壳。这意味着，从这个外壳的正面（即朝向摄像机的方向）看，它是“透明”的；但从其背面（即边缘部分）看，它显示为描边颜色。\n","date":"2025-11-22T10:20:12Z","image":"https://Selaphina.github.io/p/blender%E5%B8%B8%E7%94%A8%E9%9C%80%E6%B1%82/cover1_hu_5e2014920a4d8f6.png","permalink":"https://Selaphina.github.io/p/blender%E5%B8%B8%E7%94%A8%E9%9C%80%E6%B1%82/","title":"Blender常用需求"},{"content":"要把 Unity 的 .anim 动画文件转换成 FBX 内嵌动画，可以通过 Unity 官方提供的工具 FBX Exporter 来完成。操作流程如下：\n✅ 方法 1：使用 Unity FBX Exporter（推荐） Unity 的 FBX Exporter 支持把 Animator Controller 或 AnimationClip 导出成带动画的 FBX。\n步骤： 安装插件\nUnity → Window → Package Manager 搜索：FBX Exporter 点击 Install 准备模型和动画\n确保模型（带骨骼）已绑定 AnimationClip（.anim） 模型需要有 Animator 或 Animation Component 右键模型 → Export to FBX !\u0026gt; 必须确保动画能在 Inspector 中 Preview 播放，否则无法导出\n在导出窗口中勾选：\n1 2 3 Export Animations: ✔ Bake Animation: ✔ Sample Rate: 30 或默认 导出完成的 .fbx 就会包含动画，外部软件（Blender / Maya / MotionBuilder）可以直接看到。 ","date":"2025-11-24T10:12:30Z","image":"https://Selaphina.github.io/p/unity%E7%9A%84anim%E6%A0%BC%E5%BC%8F%E7%9A%84%E5%8A%A8%E7%94%BB%E8%BD%AC%E6%88%90fbx/CUC%E5%85%91%E6%8D%A2%E7%82%B92_hu_aee3a9923aa263e8.png","permalink":"https://Selaphina.github.io/p/unity%E7%9A%84anim%E6%A0%BC%E5%BC%8F%E7%9A%84%E5%8A%A8%E7%94%BB%E8%BD%AC%E6%88%90fbx/","title":"unity的anim格式的动画转成FBX"},{"content":"方法一：共享粒子配置文件（推荐） 1. 找到粒子文件 粒子特效通常包含两个文件：.plist文件（粒子配置）.png文件（粒子纹理图片） 2. 文件位置 1 2 3 4 Resources/ ├── Particles/ │ ├── explosion.plist # 粒子配置 │ └── particle_texture.png # 粒子纹理 3. 共享步骤 1 2 3 # 将以下文件打包发给别人 your_project/Resources/Particles/your_effect.plist your_project/Resources/Particles/your_texture.png 方法二：代码方式导出 1. 导出粒子配置代码 1 2 3 4 5 6 7 8 // 如果你的粒子是用代码创建的，可以导出配置 auto particle = ParticleSystem::createWithTotalParticles(100); particle-\u0026gt;setDuration(-1); particle-\u0026gt;setGravity(Point(0, -100)); particle-\u0026gt;setSpeed(50); // ... 其他参数 // 保存为可共享的代码片段 2. 在对方工程中使用 1 2 3 // 在对方项目中创建粒子 auto particle = ParticleSystem::create(\u0026#34;Particles/your_effect.plist\u0026#34;); this-\u0026gt;addChild(particle); 方法三：使用Cocos Creator的预制体 1. 如果使用Cocos Creator 将粒子系统保存为Prefab（预制体） 共享.prefab文件和相关资源 具体操作步骤 发送方操作： 找到粒子特效文件（.plist + .png） 确认纹理图片路径正确 打包发送给接收方 接收方操作： 将文件放入自己项目的Resources文件夹 在代码中加载使用： 1 2 3 4 // 加载粒子特效 auto particle = ParticleSystemQuad::create(\u0026#34;Particles/your_effect.plist\u0026#34;); particle-\u0026gt;setPosition(Vec2(visibleSize.width/2, visibleSize.height/2)); this-\u0026gt;addChild(particle); 注意事项 路径问题：确保纹理图片路径在plist文件中配置正确 资源依赖：不要遗漏纹理图片文件 版本兼容：确保Cocos2d版本兼容 绝对路径：避免使用绝对路径，使用相对路径 初次实现粒子特效：火焰 首先创建粒子：\nshape module： 右侧栏，下滑找到shape module：\n半径默认为1，调小改为0.2.\n开合角默认为25（圆锥）改为0（圆柱）。\n*半径改为负数可以实现向下运动的粒子。\n主模块： Duration(粒子系统运行时间)：1。默认是5，不需要那么久的运行时间。\nRateOverTime(每秒发射的粒子数)：设置为40，默认值是10.粒子太少了的话，达不到燃烧的效果。\n粒子初始大小(StartSize) ：两个常数的方式，设置为0.5，0.8，这样粒子的大小就会在0.5-0.8这个范围内随机。\n粒子初始速度(StartSpeed)： 设置为3，默认值为5，这样可以降低粒子的速度，也可以间接控制火焰的高度。\n粒子生命周期(StartLifetime)： 设置为0.8，默认值为5，这是粒子从生成到消失的时间，可以通过其控制火焰的高度。注意不要和 粒子系统运行时间(Duration)。\n大小模块（size overtimeMoudule）：\n设置一下火焰粒子的大小变化。让粒子从生成到消失慢慢变小。\n颜色模块(ColorOverLifetimeModule)，我们来设置一下火焰色颜色。\n选择 渐变模式(gradient)，设置为由黄到红的透明的渐变色。\n将设置好的粒子从层级管理器拖拽到资源管理器中，就会自动生成粒子的prefab。\n在资源管理器中右键——导出资源包。\n导入时，需要把压缩包拖进新工程文件的资源管理器，并右键导入资源包。\n选择需要的预制件导入即可。\n复现碎叶特效 需求：复现割草时的碎叶纷飞特效。\n参考教程：\nCocos Creator 3D 粒子系统初战(一),不要钱的酷炫火焰拿走不谢!-CSDN博客\n1.新建材质，选择particles/builtin-particle\n可以看到，可供粒子系统使用的 Effect 有红框中的三种，这里我们要使用的就是builtin-particle。\nbuiltin-particle:提供给粒子系统的 渲染模块(Renderer) 中的 ParticleMaterial 使用，当使用cpu渲染时，必须使用此Effect，这个也是默认选项。 builtin-particle-gpu:提供给粒子系统的 渲染模块(Renderer) 中的 ParticleMaterial 使用，当使用gpu渲染时，必须使用此Effect。 builtin-particle-trail:提供给粒子系统的 渲染模块(Renderer) 中的 TrailMaterial 使用 2.观察特效在游戏中的实际效果，叶片的形状比较随机且锋利，可以是三角形/长条形/不规则梯形为主。\n用一张2x2的纹理贴图来测试一下：\n尺寸：1024 x 1024\n分辨率：200\n背景：透明底png\n导入cocos，设置为texture\n将贴图导入material的Maintex。记得保存。\n3.将本粒子材质拖入放到渲染模块(Renderer) 的CPU Material 中。\n勾选贴图动画模块 。\nNum Tiles X/Y ：行列设置2x2。\nStart Frame：设置两个常数，代表起始和结束。注意0-4（而不是0-3）\ncycleCount：一个生命周期内播放几次循环，设置为1，默认值0.\n效果：\n4.接下来对 主模块 进行设置。调整：\n重力 速度/生命周期/数量（Rate over time）/……等等 调节Shape Module 效果：\n已迭代：\n![leaftex (2) (1)](leaftex (2) (1).png)\n","date":"2025-11-14T10:12:30Z","image":"https://Selaphina.github.io/p/%E6%80%8E%E4%B9%88%E5%85%B1%E4%BA%AB%E5%92%8C%E5%AF%BC%E5%85%A5%E5%AF%BC%E5%87%BAcocos%E7%B2%92%E5%AD%90/CUC%E5%85%91%E6%8D%A2%E7%82%B92_hu_aee3a9923aa263e8.png","permalink":"https://Selaphina.github.io/p/%E6%80%8E%E4%B9%88%E5%85%B1%E4%BA%AB%E5%92%8C%E5%AF%BC%E5%85%A5%E5%AF%BC%E5%87%BAcocos%E7%B2%92%E5%AD%90/","title":"怎么共享和导入导出cocos粒子"},{"content":"因为Git是分布式的，你的本地仓库和远程仓库（如GitHub, GitLab）可能不同步。直接git push可能会因为远程有比你本地更新的提交而被拒绝，或者更糟的是，造成复杂的冲突。\n简化流程（项目允许直接推送到主分支） 对于非常小的团队或个人项目，有时会直接在主分支上工作。即便如此，**“先拉取，再推送”**的原则依然不变。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # 1. 拉取远程最新代码，自动合并 git pull origin master # 2. （解决可能的冲突，如果有） # 3. 进行你的开发工作，添加、提交 git add . git commit -m \u0026#34;你的提交信息\u0026#34; # 4. 再次拉取，确保在推送前是最新的（期间可能有人提交了代码） git pull origin master # 5. 解决可能出现的冲突（第二次拉取时） # 6. 推送 git push origin master 规范的日常协作流程（分支工作流） 这个流程结合了分支策略，是最安全、最高效的方法。\n第零步：准备工作（一次性） 克隆仓库：如果是第一次参与项目，先将远程仓库克隆到本地。\n1 2 git clone \u0026lt;远程仓库地址\u0026gt; cd \u0026lt;项目文件夹名\u0026gt; 设置上游分支（可选）：克隆后，你的本地main（或master）分支通常已经自动跟踪了远程的origin/main分支。\n第一步：开始新功能或修复前——基于最新代码创建分支 这是关键的最佳实践！不要在本地的主分支上直接修改。\n切换到主分支：确保你从一个干净的起点开始。\n1 git checkout master 拉取远程最新变更：这保证了你的主分支是最新的。\n1 2 git pull origin master # 或者简单写 git pull （如果当前分支已跟踪远程分支） （初次）创建并切换到一个新分支：分支名要有描述性，例如feat/user-login或fix/header-alignment。\n1 git checkout -b feature/wsy-shadow-branch -b: 就是切换分支“branch”的意思。\n(非初次)切换到自己的分支 1 git checkout feature/wsy-new-branch 可选：改完代码后查看修改了那些文件\n1 git status 第二步：日常开发工作 在你的特性分支上进行开发。\n进行修改：添加、删除、修改代码文件。\n暂存更改：将需要提交的文件加入暂存区。\n1 2 3 git add \u0026lt;文件名\u0026gt; # 或添加所有更改 git add . 提交更改：在本地提交，写清晰的提交信息。\n1 git commit -m \u0026#34;feat: 新增用户登录接口\u0026#34; 重复：不断重复add和commit，直到功能完成。\n第三步：（可选）准备推送-合并（merge）最新代码 在将你的分支推送到远程之前，很可能主分支已经有了新的提交。你需要将这些新变化整合到你的特性分支中，以确保你的代码是基于最新代码的。\n切换到主分支并PULL（拉下来）最新代码：\n1 2 git checkout master git pull 切回你特性（feat）分支：\n1 git checkout feat/wsy-new-branch 将主分支的更新合并到你的分支（有两种主流方式）：\n方式A：合并（Merge）（更简单，推荐新手）\n1 git merge master 这会在你的特性分支上创建一个“合并提交”，记录整合点。\n方式B：变基（Rebase）（历史更整洁，但需谨慎）\n1 git rebase master 这会将你的所有提交“重新播放”在最新的主分支之上，使历史呈线性。注意：不要在公共分支上使用rebase。\n解决冲突（如果有）：如果合并或变基过程中发生代码冲突，Git会提示你。你需要手动打开冲突文件，解决冲突（选择保留谁的代码，或进行修改）。解决后，使用git add标记冲突已解决。如果使用merge，然后执行git commit（合并提交）。如果使用rebase，执行git rebase --continue。\n第四步：推送到远程并发起合并请求 推送特性分支到远程：\n1 2 git push -u origin feat/your-feature-name # -u 参数设置上游分支，之后可以直接用 git push 在GitHub/GitLab等平台上创建Pull Request（PR）：前往你的仓库页面，通常会有提示让你创建PR。选择你的特性分支作为源分支，选择main作为目标分支。在PR描述中清晰说明你的修改内容。\n1.左侧栏-branches\n2.New—自动跳转New Merge Request\n代码审查：团队成员在PR中进行讨论和代码审查。你可能需要根据反馈继续在本地分支上提交修改，然后再次push，PR会自动更新。\n合并PR：审查通过后，由有权限的人（可能就是你）将PR合并到主分支。通常平台会提供“Squash and Merge”（压缩合并）等选项，保持历史整洁。\n第五步：清理 合并成功后，你可以删除本地和远程的特性分支，保持整洁。\n删除远程分支（在PR页面或通过命令）：\n1 git push origin --delete feat/your-feature-name 切换回主分支：\n1 git checkout main 拉取最新的合并结果：\n1 git pull 删除本地分支：\n1 git branch -d feat/your-feature-name git 在新设备上配置新的SSH密钥 1. 检查现有的 SSH 密钥 打开 Git Bash 并运行以下命令，查看是否已存在 SSH 密钥：\n1 ls -al ~/.ssh 如果看到 id_ed25519和 id_ed25519.pub（或 id_rsa和 id_rsa.pub）等文件，说明你已有密钥。\n2. 生成新的 SSH 密钥（如果没有） 运行以下命令（替换为你的 GitHub 注册邮箱）：\n1 ssh-keygen -t ed25519 -C \u0026#34;1806527871@qq.com\u0026#34; 按 Enter 接受默认保存路径，并设置一个安全的密码（可选）。\n3. 将 SSH 密钥添加到 ssh-agent 启动 ssh-agent 并添加密钥：\n1 2 eval \u0026#34;$(ssh-agent -s)\u0026#34; ssh-add ~/.ssh/id_ed25519 4. 将公钥复制到 GitHub 显示公钥内容：\n1 cat ~/.ssh/id_ed25519.pub 复制输出的全部内容（以 ssh-ed25519开头）。\n登录 GitHub → Settings → SSH and GPG keys → New SSH key → 粘贴并保存。\ngit怎么同时管理多个账号 当本机上有大于等于2个github账号时，登录信息可能会错乱，特别时访问组织/企业的私有仓库拉取项目时，很容易报错。\n1 fatal: repository \u0026#39;https://github.com/xxx-xxxx-xxxx/xxx_xxxx_xxxxx.git/\u0026#39; not found 又或者，想切回自己的个人仓库提交东西，却发现：\n1 ERROR: Permission to Selaphina/Selaphina.github.io.git denied to WangSiya010. fatal: Could not read from remote repository. 仓库所有者：Selaphina\n你当前使用的 GitHub 账号：WangSiya010\n在 Windows + Git 下，身份可能来自 3 个地方之一：\nHTTPS Token（Windows Credential Manager） SSH key（~/.ssh/id_rsa 对应的 GitHub 账号） GitHub Desktop / gh CLI 登录状态 你现在这台机器 已经在用 WangSiya010 的凭据，所以 GitHub 拒绝你。\n标准解决方法 如果不是初次： 把 SSH Key 加入 ssh-agent（非常重要） 1 2 3 4 5 eval \u0026#34;$(ssh-agent -s)\u0026#34; ssh-add ~/.ssh/id_ed25519_selaphina ssh-add ~/.ssh/id_ed25519_wang 2.切换到selaphina账号\n1 ssh -T git@github-selaphina 3.接下来就可以正常操作了\n1 git push 一、为每个 GitHub 账号生成独立 SSH Key ⚠️ 不要复用 key，这是串号根源\n1.给 Selaphina 生成 key\n1 ssh-keygen -t ed25519 -C \u0026#34;selaphina@github\u0026#34; -f ~/.ssh/id_ed25519_selaphina 2.给 WangSiya010 生成 key\n1 ssh-keygen -t ed25519 -C \u0026#34;wangsiya010@github\u0026#34; -f ~/.ssh/id_ed25519_wang 二、把 SSH Key 加入 ssh-agent（非常重要） 1 2 3 4 5 eval \u0026#34;$(ssh-agent -s)\u0026#34; ssh-add ~/.ssh/id_ed25519_selaphina ssh-add ~/.ssh/id_ed25519_wang 验证：\n1 ssh-add -l 应该能看到 两个 key：\n1 2 3 4 admin@BJNBC-0010887 MINGW64 ~ $ ssh-add -l 256 SHA256:6hL998UnHpS0zeeu1kNoWsUWiF8Dj+sRaeKsdTnjOeg selaphina@github (ED25519) 256 SHA256:JhnksLYhb7OUNW1D1oOvKV/c5V5gTK8zeMNLDcl+Mew wangsiya010@github (ED25519) 三、配置 ~/.ssh/config（核心） 创建 / 编辑：\n1 nano ~/.ssh/config 填入 完整内容：\n1 2 3 4 5 6 7 8 9 10 11 12 13 # Selaphina 主账号 Host github-selaphina HostName github.com User git IdentityFile ~/.ssh/id_ed25519_selaphina IdentitiesOnly yes # WangSiya010 副账号 Host github-wang HostName github.com User git IdentityFile ~/.ssh/id_ed25519_wang IdentitiesOnly yes Ctrl + O保存，直接回车即可退出。\n四、把 SSH 公钥分别加到 GitHub Selaphina\n1 cat ~/.ssh/id_ed25519_selaphina.pub 复制 → GitHub → Settings → SSH and GPG keys → New SSH key\nWangSiya010\n1 cat ~/.ssh/id_ed25519_wang.pub 同样添加到 WangSiya010 账号\n五、测试（非常关键） 1 ssh -T git@github-selaphina 应看到：\n1 2 Hi Selaphina! You\u0026#39;ve successfully authenticated... ssh -T git@github-wang 应看到：\n1 Hi WangSiya010! You\u0026#39;ve successfully authenticated... ","date":"2025-11-12T10:12:30Z","image":"https://Selaphina.github.io/p/git%E6%96%B0%E6%89%8B%E4%BD%BF%E7%94%A8%E8%A7%84%E8%8C%83/cover_hu_5f38c48f5126f9f0.png","permalink":"https://Selaphina.github.io/p/git%E6%96%B0%E6%89%8B%E4%BD%BF%E7%94%A8%E8%A7%84%E8%8C%83/","title":"Git新手使用规范"},{"content":"作为Hugo建站新手，使用作者提供的 Stack starter template，并用 GitHub Actions 自动构建并部署到 GitHub Pages，最为方便。\n建站之初参考了很多博主的指南博客，都是手工方式在本地安装Hugo，把stack主题拉取到本地再部署到github上。然而实际操作过程中总有和博主相悖的情况发生，遂找G老师从头复盘一遍。最后一遍过。\nTodo：补充自备案域名的建站方式。\n一、先决条件 一个 GitHub 账号（有仓库权限）。 安装 Git（本地开发时需要）。 小提示：如果只是用 starter 模板并通过 GitHub Actions 构建部署，本地安装 Hugo 可以暂时跳过 — 但建议安装以便本地预览与调试。\n3.从Hugo资源页Releases · gohugoio/hugo下载amd版本（查看自己的电脑为**基于 x64 的处理器** -\u0026gt; 下载 amd64版本。）\n由于部分主题需要 Hugo 的 Extended 版本才可以正常使用，因此建议一步到位直接安装 Extended 版本的 Hugo，在将压缩包解压后一定不要忘记的是将 hugo.exe 所在的文件夹添加至用户的**环境变量**。\n若是以上步骤都正常完成，那么可以在输入 hugo version 命令后得到正常的版本号显示。\n4.初始化\n1 hugo new site XXX 5.快速预览\n浏览 http://localhost:1313 查看效果。注意：-D的意思是同时预览草稿。\n1 hugo server -D 在浏览器中访问http://localhost:1313/ ，如果正常就会显示出页面。\n*如果出现报错多半是没有下载go。https://go.dev/dl/下载go1.22.x.windows-amd64.msi。\n二、用 Hugo-Theme-Stack Starter（适合新手） Stack 作者维护了一个 starter 模板并自带 GitHub Actions 来部署，直接用模板能省很多配置工作。示例仓库与 workflow 可参考作者的 starter。Hugo-Theme-Stack\n步骤： 1.在 GitHub 上用模板创建仓库 打开 CaiJimmy/hugo-theme-stack-starter 仓库（或访问主题主页找到 starter 链接），点击 Use this template → Create a new repository from template。选择仓库名： 如果你想把博客作为个人主页（username.github.io），仓库名应为 your-github-username.github.io（这样 GitHub Pages 会直接在根域名托管）。 如果想作为项目页（例如 your-username/myblog），可以用任意仓库名（但发布 URL 会是 https://username.github.io/repo）。 （参考：starter 仓库自带 Actions 和样例配置。）GitHub 2. 把仓库 clone 到本地（可选但推荐） 1 2 git clone https://github.com/your-username/your-repo.git cd your-repo 3. 修改站点配置（最重要的两项） 打开仓库中的 config.toml（或 config/_default/config.toml，starter 可能把配置拆分），修改： baseURL：部署到 GitHub Pages 时必须设置为你站点的真实 URL，例如： 个人主页（username.github.io）: https://your-username.github.io/ 比如我：Selaphina 项目页：https://your-username.github.io/repo-name/ title、author 等按需修改。 Starter 通常已经把 theme 配好为 Hugo module（不用把主题源码放到 themes/），直接修改 baseURL 即可。参考 Stack 文档或 starter README。GitHub 4.启动本地预览 1 2 hugo server -D # 浏览 http://localhost:1313 查看效果 5.确认 GitHub Actions workflow（自动部署） starter 仓库中自带 .github/workflows/deploy.yml（或类似文件），该 workflow 会在你 push 到主分支后构建 Hugo 并把 public/ 内容发布到 GitHub Pages（通常是 gh-pages 分支或通过 pages 路径）。你可以打开该文件预览构建步骤并根据需要修改 Hugo 版本或 build 参数。示例和历史记录见作者 repo。GitHub 6.推送修改并等待自动部署 1 2 3 git add . git commit -m \u0026#34;Customize site config\u0026#34; git push -u origin master 推送后，GitHub Actions 会触发构建并将结果发布到 GitHub Pages。你可以在仓库的 Actions 标签页查看构建日志与历史。成功后，访问你设置的 baseURL 即可看到站点。 比如：https://selaphina.github.io/\n7.新建博客帖子：\n根目录\\content\\post\n附录 create a new repository on the command line 1 2 3 4 5 git remote add origin git@github.com:Selaphina/Selaphina.github.io.git git add . git commit -m \u0026#34;first commit\u0026#34; git push -u origin master push an existing repository from the command line 1 2 3 git remote add origin git@github.com:huoshaoweiba/Sera.github.io-.git git branch -M main git push -u origin main 怎么删除帖子？ 本想设置草稿帖子，即在网络上访问时不显示的帖子。但是无论设置draft: true还是private：true都失败了，草稿照常显示，非常不美观。对构建config.yml等进行修改也无效果。\n删除不需要的帖子。单纯从content中删除之后，本地hugo server正常删除，而公网上的帖子仍然存在。经检查，应在\\..\\【站点根目录】\\public\\p目录下的对应帖子也删除才可以。\n不知是否是构建工作流没有顾及到位。总之目前只能用此方法删除。\n怎么设置草稿？ 上述所示，没有常规标记draft : true的草稿方式。\n但是！由于工作流中规定的【不允许上传未来日期/过时的帖子】。因此，如果想要上传一个公网上不可见（相当于私密的）帖子，可以把日期改为未来的日期。比如2100年6月6日，这样的话起码75年内都不会有人能在公网上访问你的私密帖子了。\n构建失败：超时 1 2 3 4 5 6 7 8 9 10 11 12 13 14 un hugo --minify --gc --buildDrafts=false --buildFuture=false --buildExpired=false Start building sites hugo v0.154.5-a6f99cca223a29cad1d4cdaa6a1a90508ac1da71+extended linux/amd64 BuildDate=2026-01-11T20:53:23Z VendorInfo=gohugoio hugo: collected modules in 674 msERROR error building site: \u0026#34;/home/runner/ work/Selaphina.github.io/Selaphina.github.io/content/post/25-12-11 仿原神渲染 /index.md:1:1\u0026#34;: timed out rendering the page content. Extend the timeout limit in your Hugo config file: timeout after 1m0s Total in 219536 ms Error: Process completed with exit code 1. 这个原因在于：\nHugo 在渲染某一篇 Markdown 页面时，单页渲染耗时超过 60 秒，直接被强制中断。\n这篇帖子写的太长，Markdown 中包含 极大体量的代码块 / 数学公式 / 嵌套短代码\n典型高风险内容：\n超长 代码块 大量highlight 大量 LaTeX 数学公式（KaTeX / MathJax） 复杂 shortcode 嵌套（shortcode 里又调用 shortcode） 解决方案 ✅官解： 直接延长 Hugo timeout（必须做）\n在你的 config.toml / config.yaml 中加入：\n*我这里是toml\ntoml\n1 timeout = \u0026#34;5m\u0026#34; YAML\n1 timeout: 5m GitHub Actions 机器慢，1 分钟对中等偏长技术博客是完全不够的\nUpdate theme失败 ![年底比较忙，给我整这出,每天定时更新失败一次，邮箱堆满了](D:\\WSY\\Selaphina\\content\\post\\25-11-10 hugo\\image-20260204012338653.png)\n在Action查看失败原因：\n![](D:\\WSY\\Selaphina\\content\\post\\25-11-10 hugo\\image-20260204012502151.png)\n1 2 3 Run hugo mod tidy Error: \u0026#34;/tmp/hugo_cache_runner/modules/filecache/modules/pkg/mod/github.com/!cai!jimmy/hugo-theme-stack/v3@v3.34.0/layouts/partials/helper/color-from-str.html:4:1\u0026#34;: parse failed: template: partials/helper/color-from-str.html:4: function \u0026#34;hash\u0026#34; not defined Error: Process completed with exit code 1. GitHub Actions 使用的 Hugo 版本过旧，而 hugo-theme-stack v3.34.0 使用了新版 Hugo 才支持的 hash 模板函数。\n因此在 GitHub 上构建失败，但你本地可能是正常的。\n1 function \u0026#34;hash\u0026#34; not defined 说明：\nhash 是 Hugo 新增的 template function\n当前 GitHub Actions 里的 Hugo 还没这个函数\n但 hugo-theme-stack v3.34.0 已经开始用了\n出问题的文件：\n1 layouts/partials/helper/color-from-str.html 这是 Stack 主题 v3.34.0 新增的实现 解决方案：升级 GitHub Actions 里的 Hugo 如果你用的是官方 Hugo Action，打开你的 workflow，例如：\nSelaphina.github.io/.github/workflows /update-theme.yml\n目前workflow 做了两件危险组合的事：\n1 2 3 4 5 # ① 固定 Hugo 版本 hugo-version: 0.123.8 # ② 每天自动把 Stack 主题升级到“最新” hugo mod get -u github.com/CaiJimmy/hugo-theme-stack/v3 这在工程上是一个不稳定系统\n下游（主题）每天变，上游（Hugo）不变\n于是某一天：\nStack v3.34.0 → 开始使用 hash 模板函数 hash 需要 Hugo ≥ 0.124 你的 CI 仍然是 0.123.8 💥 定时任务自动升级主题 → 构建直接炸 → GitHub 给你疯狂发邮件 修改点： 1 hugo-version: \u0026#39;latest\u0026#39; 既然已经选择让主题每天自动升级 那 Hugo 就 必须 跟着升级 Stack 是 Hugo 官方生态里维护最规范的主题之一 “最新 Hugo + 最新 Stack” 是被作者默认支持的组合 📌 这是 Stack 作者自己在用的组合\n","date":"2025-11-09T10:12:30Z","image":"https://Selaphina.github.io/p/hugo%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E5%BB%BA%E7%AB%99%E6%8C%87%E5%8D%97/hugo_hu_9d163aef0215b7da.png","permalink":"https://Selaphina.github.io/p/hugo%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E5%BB%BA%E7%AB%99%E6%8C%87%E5%8D%97/","title":"Hugo：个人博客建站指南"},{"content":"12 屏幕后处理 屏幕后处理，顾名思义，通常是指在渲染完整个场景得到屏幕图像后，再对这个图像进行一系列操作，实现各种屏幕特效。例如：景深（Depth of Field），运动模糊（Motion Blur）等。\n因此，想要实现屏幕后处理的基础在于得到渲染后的屏幕图像，即抓取屏幕，而Unity为我们提供了这样一个方便的接口\u0026ndash;0nRenderImage函数。它的函数声明如下:\n1 MonoBehaviour.OnRenderImage(RenderTexture src, RenderTexture dest) 在 OnRenderlmage 函数中，我们通常是利用Graphics.Blit函数来完成对渲染纹理的:\n1 2 3 public static void Blit(Texture src,RenderTexture dest); public static void Blit(Texture src,RenderTexture dest,Material mat,int pass = -1); public static void Blit( Texture src,Material mat,int pass=-1); 参数src：\n源纹理，在屏幕后处理技术中，unity通常会将当前屏幕渲染纹理（或者上一步处理后的渲染纹理）存储在src源纹理中。\n参数dest：\n目标渲染纹理，如果它的值为null，会直接将结果显示在屏幕上。不为null则通过函数的一系列操作后，unity再将dest渲染到屏幕上。\n参数mat\n我们使用的材质，这个材质使用的unity shader会进行各种屏幕后处理操作。而src纹理将会被传递给shader中名为MainTex的代码.\n参数pass\n默认值为-1，表示将会依次调用shader内的所有pass，否则只会调用索引值的pass。\n在默认情况下，OnRenderImage函数会在所有的不透明和透明的Pass执行完毕后被调用，以便对场景中所有游戏对象都产生影响。但有时，我们希望在不透明的Pass(即渲染队列小于等于2500的Pass，内置的 Background、Geometry和 AlphaTest 渲染队列均在此范围内)执行完毕后立即调用 OnRenderlmage 函数,从而不对透明物体产生任何影响。此时,我们可以在 OnRenderlmage函数前添加ImageEfectOpaque 属性来实现这样的目的。13.4节展示了这样一个例子，在13.4节\n12.1 基础unity屏幕后处理脚本系统 因此，要在 Unity 中实现屏幕后处理效果，过程通常如下:\n1）首先需要在摄像机中添加一个用于屏幕后处理的脚本。在这个脚本中，我们会实现OnRenderlmage函数来获取当前屏幕的渲染纹理。\n2）然后，调用 Graphics.Blit 函数使用特定的 Unity Shader 来对当前图像进行处理，再把返回的渲染纹理显示到屏幕上。\n3）对于一些复杂的屏幕特效，我们可能需要多次调用Graphics.Blit 函数来对上一步的输出结果进行下一步处理。\n但是，在进行屏幕后处理之前，我们需要检查一系列条件是否满足，例如当前平台是否支持渲染纹理和屏幕特效，是否支持当前使用的UnityShader 等。为此，我们创建了一个用于屏幕后处理效果的基类，在实现各种屏幕特效时，我们只需要继承自该基类，再实现派生类中不同的操作即可。读者可在本书资源的 Assets/Scripts/Chapter12/PostEffectsBase.cs 中找到该脚本。\nPostEffecrsBase.cs\n(1)首先，所有屏幕后处理效果都需要绑定在某个摄像机上，并且我们希望在编辑器状态下也可以执行该脚本来查看效果:\n1 2 3 [ExecuteInEditMode] [RequireComponent (typeof(Camera))] public class PostEffectsBase : MonoBehaviour { (2)为了提前检查各种资源和条件是否满足，我们在Start函数中调用CheckResources 函数:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 // Called when start protected void CheckResources() { bool isSupported = CheckSupport(); if (isSupported == false) { NotSupported(); } } // Called in CheckResources to check support on this platform protected bool CheckSupport() { if (SystemInfo.supportsImageEffects == false || SystemInfo.supportsRenderTextures == false) { Debug.LogWarning(\u0026#34;This platform does not support image effects or render textures.\u0026#34;); return false; } return true; } // Called when the platform doesn\u0026#39;t support this effect protected void NotSupported() { enabled = false; } protected void Start() { CheckResources(); } (3)由于每个屏幕后处理效果通常都需要指定一个shader 来创建一个用于处理渲染纹理的材质，因此基类中也提供了这样的方法:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 protected Material CheckShaderAndCreateMaterial(Shader shader, Material material) { if (shader == null) { return null; } if (shader.isSupported \u0026amp;\u0026amp; material \u0026amp;\u0026amp; material.shader == shader) return material; if (!shader.isSupported) { return null; } else { material = new Material(shader); material.hideFlags = HideFlags.DontSave; if (material) return material; else return null; } } CheckShaderAndCreateMaterial函数接受两个参数，第一个参数指定了该特效需要使用的Shader，第二个参数则是用于后期处理的材质。该函数首先检査Shader 的可用性，检查通过后就返回一个使用了该 Shader的材质，否则返回null。\n12.2 调整屏幕的亮度/饱和度和对比度 1）新建场景，包括相机，平行光。去掉天空盒子。\n2）把本书资源中的 Assets/Textures/Chapter12/Sakura0,jpg拖曳到场景中，并调整其的位置使它可以填充整个场景。 3）新建一个脚本。在本书资源中，该脚本名为BrightnessSaturationAndContrast.cs。把该脚本拖曳到摄像机上。 4）新建一个Unity Shader。在本书资源中,该 Shader 名为Chapter12-BrightnessSaturationAndContrast.shader。\nBrightnessSaturationAndContrast.cs脚本 首先来编写 BrightnessSaturationAndContrast.cs脚本。打开该脚本，并进行如下修改：\n首先继承12.1的基类 1 public class BrightnessSaturationAndContrast : PostEffectsBase { 声明该效果需要的shader，并依据此创建相应的材质： 1 2 3 4 5 6 7 8 public Shader briSatConShader; private Material briSatConMaterial; public Material material { get { briSatConMaterial = CheckShaderAndCreateMaterial(briSatConShader, briSatConMaterial); return briSatConMaterial; } } 在上述代码中，briSatConShader是我们指定的Shader，对应了后面将会实现的Chapter12-BrightnessSaturationAndContrast。briSatConMaterial是创建的材质，我们提供了名为 material 的材质来访问它，material的get函数调用了基类的CheckShaderAndCreateMaterial 函数来得到对应的材质。\n我们还在脚本中提供了调整亮度、饱和度和对比度的参数: 1 2 3 4 5 6 7 8 [Range(0.0f, 3.0f)] public float brightness = 1.0f; [Range(0.0f, 3.0f)] public float saturation = 1.0f; [Range(0.0f, 3.0f)] public float contrast = 1.0f; 最后定义OnRenderImage来真正的后处理： 1 2 3 4 5 6 7 8 9 10 11 void OnRenderImage(RenderTexture src, RenderTexture dest) { if (material != null) { material.SetFloat(\u0026#34;_Brightness\u0026#34;, brightness); material.SetFloat(\u0026#34;_Saturation\u0026#34;, saturation); material.SetFloat(\u0026#34;_Contrast\u0026#34;, contrast); Graphics.Blit(src, dest, material); } else { Graphics.Blit(src, dest); } } 每当 OnRenderlmage函数被调用时，它会检查材质是否可用。如果可用，就把参数传递给材质，再调用 Graphics.Blit进行处理；否则，直接把原图像显示到屏幕上，不做任何处理。\nBrightnessSaturationAndContrast.shader 下面，我们来实现 Shader 的部分。打开Chapter12-BrightnessSaturationAndContrast，进行如下修改。\n1)声明\n1 2 3 4 5 6 Properties { _MainTex (\u0026#34;Base (RGB)\u0026#34;, 2D) = \u0026#34;white\u0026#34; {} _Brightness (\u0026#34;Brightness\u0026#34;, Float) = 1 _Saturation(\u0026#34;Saturation\u0026#34;, Float) = 1 _Contrast(\u0026#34;Contrast\u0026#34;, Float) = 1 } 在 12.1节中，我们提到 Graphics.Blit(src,dest,material)将把第一个参数传递给 Shader 中名为MainTex的属性。因此，我们必须声明一个名为MainTex的纹理属性。除此之外，我们还声明了用于调整亮度、饱和度和对比度的属性。这些值将会由脚本传递而得。事实上，我们可以省略Properties中的属性声明，Properties中声明的属性仅仅是为了显示在材质面板中，但对于屏幕特效来说，它们使用的材质都是临时创建的，我们也不需要在材质面板上调整参数，而是直接从脚本传递给 Unity Shader。\n2）定义用于屏幕后处理的pass\n1 2 3 SubShader { Pass { ZTest Always Cull Off ZWrite Off 3）为了在代码中访问各个属性，我们需要在CG代码块中声明对应的变量:\n1 2 3 4 sampler2D _MainTex; half _Brightness; half _Saturation; half _Contrast; 4）定义顶点着色器：屏幕特效使用的顶点着色器代码通常都比较简单，我们只需要进行必需的顶点变换，更重要的是，我们需要把正确的纹理坐标传递给片元着色器，以便对屏幕图像进行正确的采样:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 struct v2f { float4 pos : SV_POSITION; half2 uv: TEXCOORD0; }; v2f vert(appdata_img v) { v2f o; o.pos = UnityObjectToClipPos(v.vertex); o.uv = v.texcoord; return o; } **appdata_img结构体：**使用了Unity内置的appdata_img结构体作为顶点着色器的输入,读者可以在 UnityCGcginc 中找到该结构体的声明,它只包含了图像处理时必需的顶点坐标和纹理 坐标等变量\n5）接着，我们实现了用于调整亮度、饱和度和对比度的片元着色器:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 fixed4 frag(v2f i) : SV_Target { fixed4 renderTex = tex2D(_MainTex, i.uv); // Apply brightness fixed3 finalColor = renderTex.rgb * _Brightness; // Apply saturation fixed luminance = 0.2125 * renderTex.r + 0.7154 * renderTex.g + 0.0721 * renderTex.b; fixed3 luminanceColor = fixed3(luminance, luminance, luminance); finalColor = lerp(luminanceColor, finalColor, _Saturation); // Apply contrast fixed3 avgColor = fixed3(0.5, 0.5, 0.5); finalColor = lerp(avgColor, finalColor, _Contrast); return fixed4(finalColor, renderTex.a); } 我们得到对原屏幕图像**(存储在MainTex中)**的采样结果renderTex。然后，利用_Brightness属性来调整亮度。\n亮度的调整非常简单,我们只需要把原颜色乘以亮度系数 Brightness即可。\n然后，我们计算该像素对应的亮度值(luminance)，这是通过对每个颜色分量乘以一个特定的系数再相加得到的。\n我们使用该亮度值创建了一个饱和度为0的颜色值，并使用Saturation属性在其和上一步得到的颜色之间进行插值，从而得到希望的饱和度颜色。对比度的处理类似，我们首先创建一个对比度为0的颜色值(各分量均为0.5)，再使用Contrast属性在其和上一步得到的颜色之间进行插值，从而得到最终的处理结果\n6)回调\n1 Fallback Off 12.3 边缘检测 边缘检测的原理是利用一些边缘检测算子对图像进行卷积(convolution)操作。\n什么是卷积？\n在图像处理中，卷积操作指的就是使用一个卷积核(kernel)对一张图像中的每个像素进行一系列操作。卷积核通常是一个四方形网格结构(例如2x2、3x3的方形区域)，该区域内每个方格都有一个权重值。当对图像中的某个像素进行卷积时,我们会把卷积核的中心放置于该像素上，如图12.4所示，翻转核之后再依次计算核中每个元素和其覆盖的图像像素值的乘积并求和，得到的结果就是该位置的新像素值。\n这样的计算过程虽然简单，但可以实现很多常见的图像处理效果，例如图像模糊、边缘检测等。\n几种不同的边缘检测算子\n3种常见的边缘检测算子如图12.5所示，它们都包含了两个方向的卷积核，分别用于检测水平方向和竖直方向上的边缘信息。在进行边缘检测时，我们需要对每个像素分别进行一次卷积计算，得到两个方向上的梯度值G和G，而整体的梯度可按下面的公式计算而得:\n由于上述计算包含了开根号操作，出于性能的考虑，我们有时会使用绝对值操作来代替开根号操作:\n实现过程\nEdgeDetection.cs 脚本 1）新建场景，包括相机，平行光。去掉天空盒子。\n2）把本书资源中的 Assets/Textures/Chapter12/Sakura0,jpg拖曳到场景中，并调整其的位置使它可以填充整个场景。 3）新建一个脚本。在本书资源中，该脚本名为EdgeDetection.cs。把该脚本拖曳到摄像机上。 4）新建一个Unity Shader。在本书资源中,该 Shader 名为Chapter12-EdgeDetection.shader。\n首先来编写 EdgeDetection.cs 脚本。打开该脚本，并进行如下修改：\n首先继承12.1的基类 1 public class BrightnessSaturationAndContrast : PostEffectsBase { 声明该效果需要的shader，并据此创建相应的材质 1 2 3 4 5 6 7 8 public Shader edgeDetectShader; private Material edgeDetectMaterial = null; public Material material { get { edgeDetectMaterial = CheckShaderAndCreateMaterial(edgeDetectShader, edgeDetectMaterial); return edgeDetectMaterial; } } 在上述代码中，edgeDetectShader是我们指定的Shader，对应了后面将会实现的Chapter12-EdgeDetection。\n在脚本中提供用于调整边缘线强度、描边颜色以及背景颜色的参数: 1 2 3 4 5 6 [Range(0.0f, 1.0f)] public float edgesOnly = 0.0f; public Color edgeColor = Color.black; public Color backgroundColor = Color.white; 当edgesOnly值为0时，边缘将会叠加在原渲染图像上;当edgesOnly值为1时，则会只显示边缘，不显示原渲染图像。其中，背景颜色由backgroundColor指定，边缘颜色由edgeColor 指定。\n最后，我们定义OnRenderlmage函数来进行真正的特效处理: 1 2 3 4 5 6 7 8 9 10 11 void OnRenderImage (RenderTexture src, RenderTexture dest) { if (material != null) { material.SetFloat(\u0026#34;_EdgeOnly\u0026#34;, edgesOnly); material.SetColor(\u0026#34;_EdgeColor\u0026#34;, edgeColor); material.SetColor(\u0026#34;_BackgroundColor\u0026#34;, backgroundColor); Graphics.Blit(src, dest, material); } else { Graphics.Blit(src, dest); } } 每当 OnRenderlmage 函数被调用时，它会检查材质是否可用。如果可用，就把参数传递给材质，再调用 Graphics.Blit进行处理;否则，直接把原图像显示到屏幕上，不做任何处理。\nChapter12-EdgeDetection 下面，我们来实现Shader 的部分。打开Chapter12-EdgeDetection，进行如下修改。\n定义 1 2 3 4 5 6 Properties { _MainTex (\u0026#34;Base (RGB)\u0026#34;, 2D) = \u0026#34;white\u0026#34; {} _EdgeOnly (\u0026#34;Edge Only\u0026#34;, Float) = 1.0 _EdgeColor (\u0026#34;Edge Color\u0026#34;, Color) = (0, 0, 0, 1) _BackgroundColor (\u0026#34;Background Color\u0026#34;, Color) = (1, 1, 1, 1) } 2） 设置屏幕后处理的 Pass\n1 2 3 SubShader { Pass { ZTest Always Cull Off ZWrite Off 3）为了代码中访问各个属性，需要在CG代码块中声明变量：\n1 2 3 4 5 sampler2D _MainTex; uniform half4 _MainTex_TexelSize; fixed _EdgeOnly; fixed4 _EdgeColor; fixed4 _BackgroundColor; 在上面的代码中，我们还声明了一个新的变量MainTexTexelSize。xxxTexelSize是 Unity为我们提供的访问 xxx 纹理对应的每个纹素的大小。例如，一张 512x512大小的纹理，该值大约为0.001953(即112)。由于卷积需要对相邻区域内的纹理进行采样，因此我们需要利用MainTex TexelSize 来计算各个相邻区域的纹理坐标。\n4）在顶点着色器的代码中，我们计算了边缘检测时需要的纹理坐标:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 struct v2f { float4 pos : SV_POSITION; half2 uv[9] : TEXCOORD0; }; v2f vert(appdata_img v) { v2f o; o.pos = UnityObjectToClipPos(v.vertex); half2 uv = v.texcoord; o.uv[0] = uv + _MainTex_TexelSize.xy * half2(-1, -1); o.uv[1] = uv + _MainTex_TexelSize.xy * half2(0, -1); o.uv[2] = uv + _MainTex_TexelSize.xy * half2(1, -1); o.uv[3] = uv + _MainTex_TexelSize.xy * half2(-1, 0); o.uv[4] = uv + _MainTex_TexelSize.xy * half2(0, 0); o.uv[5] = uv + _MainTex_TexelSize.xy * half2(1, 0); o.uv[6] = uv + _MainTex_TexelSize.xy * half2(-1, 1); o.uv[7] = uv + _MainTex_TexelSize.xy * half2(0, 1); o.uv[8] = uv + _MainTex_TexelSize.xy * half2(1, 1); return o; } 我们在v2f结构体中定义了一个维数为9的纹理数组，对应了使用Sobel算子采样时需要的9个邻域纹理坐标。通过把计算采样纹理坐标的代码从片元着色器中转移到顶点着色器中，可以减少运算，提高性能。由于从顶点着色器到片元着色器的插值是线性的，因此这样的转移并不会影响纹理坐标的计算结果。\n5）片元着色器是我们的重点\n1 2 3 4 5 6 7 fixed4 fragSobel(v2f i) : SV_Target { half edge = Sobel(i); fixed4 withEdgeColor = lerp(_EdgeColor, tex2D(_MainTex, i.uv[4]), edge); fixed4 onlyEdgeColor = lerp(_EdgeColor, _BackgroundColor, edge); return lerp(withEdgeColor, onlyEdgeColor, _EdgeOnly); } 我们首先调用Sobel函数计算当前像素的梯度值edge，并利用该值分别计算了**背景为原图和纯色下的颜色值,**然后利用 EdgeOnly 在两者之间插值得到最终的像素值。Sobel 函数将利用 Sobel算子对原图进行边缘检测，它的定义如下:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 fixed luminance(fixed4 color) { return 0.2125 * color.r + 0.7154 * color.g + 0.0721 * color.b; } half Sobel(v2f i) { const half Gx[9] = {-1, 0, 1, -2, 0, 2, -1, 0, 1}; const half Gy[9] = {-1, -2, -1, 0, 0, 0, 1, 2, 1};\thalf texColor; half edgeX = 0; half edgeY = 0; for (int it = 0; it \u0026lt; 9; it++) { texColor = luminance(tex2D(_MainTex, i.uv[it])); edgeX += texColor * Gx[it]; edgeY += texColor * Gy[it]; } half edge = 1 - abs(edgeX) - abs(edgeY); return edge; } 我们首先定义了水平方向和竖直方向使用的卷积核G和G。接着，我们依次对9个像素行采样，计算它们的亮度值，再与卷积核G和G,中对应的权重相乘后，叠加到各自的梯度值上最后，我们从1中减去水平方向和竖直方向的梯度值的绝对值，得到edge。edge值越小，表明位置越可能是一个边缘点。至此，边缘检测过程结束。 (6)当然，我们也关闭了该Shader的Fallback:\n1 Fallback Off 12.4 高斯模糊 高斯模糊同样利用了卷积计算，它使用的卷积核名为高斯核。高斯核是一个正方形大小的滤波核，其中每个元素的计算都是基于下面的高斯方程:\n高斯方程很好地模拟了邻域每个像素对当前处理像素的影响程度\u0026ndash;距离越近，影响越大。\n高斯核的维数越高,模糊程度越大。使用一个NxN 的高斯核对图像进行卷积滤波,就需要NxNxWxH(W 和H分别是图像的宽和高)次纹理采样。当N的大小不断增加时，采样次数会变得非常巨大。幸运的是，我们可以把这个二维高斯函数拆分成两个一维函数。也就是说，我们可以使用两个一维的高斯核(图12.8中的右图)先后对图像进行滤波，它们得到的结果和直接使用二维高斯核是一样的，但采样次数只需要 2xNxWxH。我们可以进一步观察到，两个一维高斯核中包含了很多重复的权重。对于一个大小为5的一维高斯核，我们实际只需要记录3个权重值即可。\n实验过程\n1）新建场景，包括相机，平行光。去掉天空盒子。\n2）把本书资源中的 Assets/Textures/Chapter12/Sakura0,jpg拖曳到场景中，并调整其的位置使它可以填充整个场景。 3）新建一个脚本。在本书资源中，该脚本名为GaussianBlur.cs。把该脚本拖曳到摄像机上。 4）新建一个Unity Shader。在本书资源中,该 Shader 名为Chapter12-GaussianBlur.shader。\nGaussianBlur.cs脚本 首先来编写 BrightnessSaturationAndContrast.cs脚本。打开该脚本，并进行如下修改：\n首先继承12.1的基类 1 public class BrightnessSaturationAndContrast : PostEffectsBase { 声明需要用到的shader并创建相应材质 1 2 3 4 5 6 7 8 9 public Shader gaussianBlurShader; private Material gaussianBlurMaterial = null; public Material material { get { gaussianBlurMaterial = CheckShaderAndCreateMaterial(gaussianBlurShader, gaussianBlurMaterial); return gaussianBlurMaterial; } } 在脚本中，我们还提供了调整高斯模糊的迭代次数 ，模糊范围和缩放系数的参数： 1 2 3 4 5 6 7 8 9 10 11 // Blur iterations - larger number means more blur. [Range(0, 4)] public int iterations = 3; // Blur spread for each iteration - larger value means more blur [Range(0.2f, 3.0f)] public float blurSpread = 0.6f; [Range(1, 8)] public int downSample = 2; blurSpread 和 downSample 都是出于性能的考虑。在高斯核维数不变的情况下，BlurSize 越大，模糊程度越高，但采样数却不会受到影响。但过大的BlurSize值会造成虚影，这可能并不是我们希望的。而 downSample 越大，需要处理的像素数越少，同时也能进一步提高模糊程度，但过大的 downSample 可能会使图像像素化。\n最后，我们需要定义关键的 OnRenderlmage函数。我们首先来看第一个版本，也就是最简单的 OnRenderImage 的实现: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 void OnRenderImage (RenderTexture src, RenderTexture dest) { if (material != null) { int rtW = src.width/downSample; int rtH = src.height/downSample; RenderTexture buffer0 = RenderTexture.GetTemporary(rtW, rtH, 0); buffer0.filterMode = FilterMode.Bilinear; Graphics.Blit(src, buffer0); for (int i = 0; i \u0026lt; iterations; i++) { material.SetFloat(\u0026#34;_BlurSize\u0026#34;, 1.0f + i * blurSpread); RenderTexture buffer1 = RenderTexture.GetTemporary(rtW, rtH, 0); // Render the vertical pass Graphics.Blit(buffer0, buffer1, material, 0); RenderTexture.ReleaseTemporary(buffer0); buffer0 = buffer1; buffer1 = RenderTexture.GetTemporary(rtW, rtH, 0); // Render the horizontal pass Graphics.Blit(buffer0, buffer1, material, 1); RenderTexture.ReleaseTemporary(buffer0); buffer0 = buffer1; } Graphics.Blit(buffer0, dest); RenderTexture.ReleaseTemporary(buffer0); } else { Graphics.Blit(src, dest); } } 与上两节的实现不同,我们这里利用 RenderTexture.GetTemporary 函数分配了一块与屏幕图像大小相同的缓冲区。这是因为，高斯模糊需要调用两个Pass，我们需要使用一块中间缓存来存储第一个 Pass 执行完毕后得到的模糊结果。如代码所示，我们首先调用 Graphics.Blit(src,buffer,material,0)，使用 Shader 中的第一个Pass(即使用竖直方向的一维高斯核进行滤波)对 src 进行处理，并将结果存储在了buffer中。然后,再调用 Graphics.Blit(bufer,dest,material,1),使用 Shader中的第二个 Pass(即使用水平方向的一维高斯核进行滤波)对 buffer 进行处理，返回最终的屏幕图像。最后，我们还需要调用RenderTexture.ReleaseTemporary 来释放之前分配的缓存。\n在理解了上述代码后，我们可以实现第二个版本的 OnRenderlmage 函数。在这个版本中，我们将利用缩放对图像进行降采样，从而减少需要处理的像素个数，提高性能。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 void OnRenderImage (RenderTexture src, RenderTexture dest) { if (material != null) { int rtW = src.width/downSample; int rtH = src.height/downSample; RenderTexture buffer = RenderTexture.GetTemporary(rtW, rtH, 0); buffer.filterMode = FilterMode.Bilinear; // Render the vertical pass Graphics.Blit(src, buffer, material, 0); // Render the horizontal pass Graphics.Blit(buffer, dest, material, 1); RenderTexture.ReleaseTemporary(buffer); } else { Graphics.Blit(src, dest); } } 与第一个版本代码不同的是，我们在声明缓冲区的大小时，使用了小于原屏幕分辨率的尺寸,并将该临时渲染纹理的滤波模式设置为双线性。这样，在调用第一个Pass时，我们需要处理的像素个数就是原来的几分之一。对图像进行降采样不仅可以减少需要处理的像素个数，提高性能,而且适当的降采样往往还可以得到更好的模糊效果。尽管 downSample 值越大，性能越好，但过大的 downSample 可能会造成图像像素化。\n最后一个版本的代码还考虑了高斯模糊的迭代次数: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 // / 1st edition: just apply blur void OnRenderImage(RenderTexture src, RenderTexture dest) { if (material != null) { int rtW = src.width; int rtH = src.height; RenderTexture buffer = RenderTexture.GetTemporary(rtW, rtH, 0); // Render the vertical pass Graphics.Blit(src, buffer, material, 0); // Render the horizontal pass Graphics.Blit(buffer, dest, material, 1); RenderTexture.ReleaseTemporary(buffer); } else { Graphics.Blit(src, dest); } } 上面的代码显示了如何利用两个临时缓存在迭代之间进行交替的过程。在迭代开始前，我们首先定义了第一个缓存 bufer0，并把src 中的图像缩放后存储到 bufer0中。在迭代过程中，我们又定义了第二个缓存 bufer1。在执行第一个Pass时，输入是 buffer0，输出是buffer1，完毕后首先把 bufer0 释放，再把结果值 bufer1存储到 buffer0中，重新分配 bufer1，然后再调用第二个Pass，重复上述过程。迭代完成后，bufer0将存储最终的图像，我们再利用Graphics.Blit(bufer0.dest)把结果显示到屏幕上，并释放缓存。\nChapter12- GaussianBlur.shader 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 // Upgrade NOTE: replaced \u0026#39;mul(UNITY_MATRIX_MVP,*)\u0026#39; with \u0026#39;UnityObjectToClipPos(*)\u0026#39; Shader \u0026#34;Unity Shaders Book/Chapter 12/Gaussian Blur\u0026#34; { Properties { _MainTex (\u0026#34;Base (RGB)\u0026#34;, 2D) = \u0026#34;white\u0026#34; {} _BlurSize (\u0026#34;Blur Size\u0026#34;, Float) = 1.0 } SubShader { CGINCLUDE #include \u0026#34;UnityCG.cginc\u0026#34; sampler2D _MainTex; half4 _MainTex_TexelSize; float _BlurSize; struct v2f { float4 pos : SV_POSITION; half2 uv[5]: TEXCOORD0; }; v2f vertBlurVertical(appdata_img v) { v2f o; o.pos = UnityObjectToClipPos(v.vertex); half2 uv = v.texcoord; o.uv[0] = uv; o.uv[1] = uv + float2(0.0, _MainTex_TexelSize.y * 1.0) * _BlurSize; o.uv[2] = uv - float2(0.0, _MainTex_TexelSize.y * 1.0) * _BlurSize; o.uv[3] = uv + float2(0.0, _MainTex_TexelSize.y * 2.0) * _BlurSize; o.uv[4] = uv - float2(0.0, _MainTex_TexelSize.y * 2.0) * _BlurSize; return o; } v2f vertBlurHorizontal(appdata_img v) { v2f o; o.pos = UnityObjectToClipPos(v.vertex); half2 uv = v.texcoord; o.uv[0] = uv; o.uv[1] = uv + float2(_MainTex_TexelSize.x * 1.0, 0.0) * _BlurSize; o.uv[2] = uv - float2(_MainTex_TexelSize.x * 1.0, 0.0) * _BlurSize; o.uv[3] = uv + float2(_MainTex_TexelSize.x * 2.0, 0.0) * _BlurSize; o.uv[4] = uv - float2(_MainTex_TexelSize.x * 2.0, 0.0) * _BlurSize; return o; } fixed4 fragBlur(v2f i) : SV_Target { float weight[3] = {0.4026, 0.2442, 0.0545}; fixed3 sum = tex2D(_MainTex, i.uv[0]).rgb * weight[0]; for (int it = 1; it \u0026lt; 3; it++) { sum += tex2D(_MainTex, i.uv[it*2-1]).rgb * weight[it]; sum += tex2D(_MainTex, i.uv[it*2]).rgb * weight[it]; } return fixed4(sum, 1.0); } ENDCG ZTest Always Cull Off ZWrite Off Pass { NAME \u0026#34;GAUSSIAN_BLUR_VERTICAL\u0026#34; CGPROGRAM #pragma vertex vertBlurVertical #pragma fragment fragBlur ENDCG } Pass { NAME \u0026#34;GAUSSIAN_BLUR_HORIZONTAL\u0026#34; CGPROGRAM #pragma vertex vertBlurHorizontal #pragma fragment fragBlur ENDCG } } FallBack \u0026#34;Diffuse\u0026#34; } 12.5 Bloom效果 Bloom 特效是游戏中常见的一种屏幕效果。这种特效可以模拟真实摄像机的一种图像效果,它让画面中较亮的区域“扩散”到周围的区域中，造成一种朦胧的效果。\nBloom 的实现原理非常简单:我们首先根据一个值提取出图像中的较亮区域，把它们存储在一张渲染纹理中，再利用高斯模糊对这张渲染纹理进行模糊处理，模拟光线扩散的效果，最后再将其和原图像进行混合，得到最终的效果。\nChapter12-Bloom.shader\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 // Upgrade NOTE: replaced \u0026#39;mul(UNITY_MATRIX_MVP,*)\u0026#39; with \u0026#39;UnityObjectToClipPos(*)\u0026#39; Shader \u0026#34;Unity Shaders Book/Chapter 12/Bloom\u0026#34; { Properties { _MainTex (\u0026#34;Base (RGB)\u0026#34;, 2D) = \u0026#34;white\u0026#34; {} _Bloom (\u0026#34;Bloom (RGB)\u0026#34;, 2D) = \u0026#34;black\u0026#34; {} _LuminanceThreshold (\u0026#34;Luminance Threshold\u0026#34;, Float) = 0.5 _BlurSize (\u0026#34;Blur Size\u0026#34;, Float) = 1.0 } SubShader { CGINCLUDE #include \u0026#34;UnityCG.cginc\u0026#34; sampler2D _MainTex; half4 _MainTex_TexelSize; sampler2D _Bloom; float _LuminanceThreshold; float _BlurSize; struct v2f { float4 pos : SV_POSITION; half2 uv : TEXCOORD0; };\tv2f vertExtractBright(appdata_img v) { v2f o; o.pos = UnityObjectToClipPos(v.vertex); o.uv = v.texcoord; return o; } fixed luminance(fixed4 color) { return 0.2125 * color.r + 0.7154 * color.g + 0.0721 * color.b; } fixed4 fragExtractBright(v2f i) : SV_Target { fixed4 c = tex2D(_MainTex, i.uv); fixed val = clamp(luminance(c) - _LuminanceThreshold, 0.0, 1.0); return c * val; } struct v2fBloom { float4 pos : SV_POSITION; half4 uv : TEXCOORD0; }; v2fBloom vertBloom(appdata_img v) { v2fBloom o; o.pos = UnityObjectToClipPos (v.vertex); o.uv.xy = v.texcoord;\to.uv.zw = v.texcoord; #if UNITY_UV_STARTS_AT_TOP\tif (_MainTex_TexelSize.y \u0026lt; 0.0) o.uv.w = 1.0 - o.uv.w; #endif return o; } fixed4 fragBloom(v2fBloom i) : SV_Target { return tex2D(_MainTex, i.uv.xy) + tex2D(_Bloom, i.uv.zw); } ENDCG ZTest Always Cull Off ZWrite Off Pass { CGPROGRAM #pragma vertex vertExtractBright #pragma fragment fragExtractBright ENDCG } UsePass \u0026#34;Unity Shaders Book/Chapter 12/Gaussian Blur/GAUSSIAN_BLUR_VERTICAL\u0026#34; UsePass \u0026#34;Unity Shaders Book/Chapter 12/Gaussian Blur/GAUSSIAN_BLUR_HORIZONTAL\u0026#34; Pass { CGPROGRAM #pragma vertex vertBloom #pragma fragment fragBloom ENDCG } } FallBack Off } Bloom.cs\n12.6 运动模糊 运动模糊是真实世界中的摄像机的一种效果。如果在摄像机曝光时，拍摄场景发生了变化，就会产生模糊的画面。\n运动模糊的实现有多种方法。一种实现方法是利用一块累积缓存(accumulationbuffer) 来混合多张连续的图像。当物体快速移动产生多张图像后,我们取它们之间的平均值作为最后的运动模糊图像。然而，这种暴力的方法对性能的消耗很大，因为想要获取多张帧图像往往意味着我们需要在同一帧里渲染多次场景。另一种应用广泛的方法是创建和使用速度缓存(velocitybufer)，这个缓存中存储了各个像素当前的运动速度，然后利用该值来决定模糊的方向和大小。\n在本节中，我们将使用类似上述第一种方法的实现来模拟运动模糊的效果。我们不需要在一帧中把场景渲染多次，但需要保存之前的渲染结果，不断把当前的渲染图像叠加到之前的渲染图像中，从而产生一种运动轨迹的视觉效果。这种方法与原始的利用累计缓存的方法相比性能更好但模糊效果可能会略有影响。\n为此，我们需要进行如下准备工作。\n(1)新建一个场景。在本书资源中，该场景名为Scene126。在Unity5.2中，默认情况下场景将包含一个摄像机和一个平行光，并且使用了内置的天空盒子。在Window→Lighting→Skybox中去掉场景中的天空盒子。\n(2)我们需要搭建一个测试运动模糊的场景。在本书资源的实现中，我们构建了一个包含3面墙的房间，并放置了4个立方体，它们均使用了我们在9.5节中创建的标准材质。同时，我们把本书资源中的 Translating.cs脚本拖曳给摄像机，让其在场景中不断运动。\n(3)新建一个脚本。在本书资源中，该脚本名为MotionBlur.cs。把该脚本拖曳到摄像机上。\n(4)新建一个 Unity Shader。在本书资源中，该 Shader名为 Chapter12-MotionBlur。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 // Upgrade NOTE: replaced \u0026#39;mul(UNITY_MATRIX_MVP,*)\u0026#39; with \u0026#39;UnityObjectToClipPos(*)\u0026#39; Shader \u0026#34;Unity Shaders Book/Chapter 12/Motion Blur\u0026#34; { Properties { _MainTex (\u0026#34;Base (RGB)\u0026#34;, 2D) = \u0026#34;white\u0026#34; {} _BlurAmount (\u0026#34;Blur Amount\u0026#34;, Float) = 1.0 } SubShader { CGINCLUDE #include \u0026#34;UnityCG.cginc\u0026#34; sampler2D _MainTex; fixed _BlurAmount; struct v2f { float4 pos : SV_POSITION; half2 uv : TEXCOORD0; }; v2f vert(appdata_img v) { v2f o; o.pos = UnityObjectToClipPos(v.vertex); o.uv = v.texcoord; return o; } fixed4 fragRGB (v2f i) : SV_Target { return fixed4(tex2D(_MainTex, i.uv).rgb, _BlurAmount); } half4 fragA (v2f i) : SV_Target { return tex2D(_MainTex, i.uv); } ENDCG ZTest Always Cull Off ZWrite Off Pass { Blend SrcAlpha OneMinusSrcAlpha ColorMask RGB CGPROGRAM #pragma vertex vert #pragma fragment fragRGB ENDCG } Pass { Blend One Zero ColorMask A CGPROGRAM #pragma vertex vert #pragma fragment fragA ENDCG } } FallBack Off } MotionBlur.cs\n注意：Translating.cs脚本拖曳给摄像机，让其在场景中不断运动。\nTranslating.cs\n","date":"2024-05-21T22:10:30Z","image":"https://Selaphina.github.io/p/12-%E5%B1%8F%E5%B9%95%E5%90%8E%E5%A4%84%E7%90%86/cover1_hu_5e2014920a4d8f6.png","permalink":"https://Selaphina.github.io/p/12-%E5%B1%8F%E5%B9%95%E5%90%8E%E5%A4%84%E7%90%86/","title":"12 屏幕后处理"},{"content":"Video Recoloring via Spatial-Temporal Geometric Palettes 论文研读总结 文献来源：SIGGRAPH 2021（清华大学） 论文标题：Video Recoloring via Spatial-Temporal Geometric Palettes **中文翻译：**通过时空几何调色板进行视频重新着色 摘要 传统基于调色板的图像重着色方法使用静态调色板，无法有效处理视频中随时间变化的颜色。本文提出一种创新的时空几何调色板方法，通过构建4D倾斜多胞形（颜色+时间维度）隐式定义时变调色板，实现自然、无伪影的视频重着色。\n核心贡献： 扩展调色板框架至视频场景 引入4D几何调色板概念 自然处理颜色合并/分裂的拓扑变化 提供直观的编辑界面支持关键帧动画 工作流程 初始化：逐帧生成RGB凸包，粘合相邻帧形成初始4D倾斜多胞形 块合并：按拓扑一致性分块并合并，减少冗余 顶点移除：迭代移除冗余顶点至损失阈值（η₂=3） 顶点优化：微调顶点位置，优化重构精度与时间平滑性 关键技术细节 1. 数学模型基础 加性混合权重公式： 1 p = Σ wᵢvᵢ (∀像素p ∈ 图像I) 像素颜色可表示为调色板颜色的加权组合 损失函数： 重建误差：像素到调色板凸包的平均距离R(V,I) = (1/|I|) Σ ||p - proj(p)|| 紧凑性损失：调色板颜色与邻近颜色的差异C(V,I) = (1/|V|) Σ ||v - vₙ|| 总损失：L(V,I) = λR(V,I) + C(V,I) 生成一个“好”的调色板需要满足的数学目标和约束： 三个理想属性：\n顶点数量少：便于用户编辑。 重建精度高：即重建误差R(V,I)（公式2）要小，表示用调色板重建的图像与原图差异小。 调色板紧凑：即紧凑性损失C(V,I)（公式3）要小，表示调色板中的每个颜色都与图像中的真实像素颜色非常接近，这样调色板更具代表性，用户控制更直接。 四个约束条件：\n帧间度规则1：每个顶点的帧间度必须≥1。不允许为零。 帧间度规则2：对于每条帧间边，其两个顶点中最多只能有一个的帧间度≥2。 拓扑规则1：若一个顶点有多个帧间边，则它连接到的目标顶点之间必须已经存在一条帧内边。 拓扑规则2：对于一条帧内边，其两个端点分别连接到的目标顶点必须是同一个顶点，或者它们之间也必须存在一条帧内边。我们在满足上述条件的前提下，寻找一组能最小化色彩差异的帧间边集合E。 2. 4D几何调色板定义 将视频像素投影至RGBT空间（RGB颜色+时间维度） 时变调色板通过4D多胞形的时间切片隐式定义 引入帧间度规则与拓扑规则确保顶点对应关系清晰 3. 核心算法步骤 （1）块合并 将视频分段为拓扑一致的\u0026quot;块\u0026quot; 合并相邻块至损失增长超过η₁=3.5倍时停止 平衡拓扑复杂度与控制便利性 （2）顶点移除 用\u0026quot;幽灵顶点\u0026quot;替代冗余顶点 保留分裂/合并顶点及边界顶点以保护拓扑结构 （3）顶点优化 引入平滑项惩罚颜色突变：K(P) = (1/|E|) Σ |(uRGB - vRGB)/(ut - vt)| 总损失函数：Lᵥ' = Lᵥ + γK(P) 通过非线性优化微调顶点位置 实验与分析 1. 参数调优 参数 最优值 作用 α 10 控制块合并的拓扑复杂度 η₁ 3.5 块合并的损失阈值 β 1.0 影响顶点数量平衡 η₂ 3.0 顶点移除的损失阈值 2. 对比实验 对比方法 缺陷 本方法优势 RGB凸包法 静态调色板无法处理时变颜色 动态调色板适应颜色变化 聚类法 缺乏时间维度精确控制 提供时间连贯的顶点对应 RGBT凸包法 调色板过于复杂难解释 紧凑且易于解释的几何结构 量化结果：本方法紧凑性损失（0.036）显著优于对比方法（0.0649/0.2635）\n用户界面与反馈 界面特性：时间轴视图+颜色选择器，支持关键帧编辑 用户研究（16人含专业调色师）：100%对UI满意87.5%认为调色板准确反映颜色分布93.5%认为结果符合预期 总结 本方法通过4D时空几何调色板解决了视频重着色的核心挑战：\n动态性：适应颜色随时间变化 紧凑性：顶点数少且代表性强 直观性：提供拓扑一致的编辑接口 自然度：平滑处理颜色合并/分裂的拓扑变化 应用前景：影视调色、视频滤镜、动态视觉效果制作等领域具有重要应用价值。\n","date":"2025-11-08T11:10:30Z","image":"https://Selaphina.github.io/p/siggraph-2021-video-recoloring-via-spatial-temporal-geometric-palettes/cover_hu_e10414b1da33ddd5.jpg","permalink":"https://Selaphina.github.io/p/siggraph-2021-video-recoloring-via-spatial-temporal-geometric-palettes/","title":"SIGGRAPH 2021 | Video Recoloring via Spatial-Temporal Geometric Palettes"},{"content":"11 动态纹理 Unity shader内置的时间变量\n名称 类型 描述 _Time float4 t是自该场景加载开始所经过的时间，4个分量的值分别是(t/20,t,2t,3t)。 _SinTime float4 t是时间的正弦值，4个分量的值分别是(t/8,t/4,t/2,t) _CosTime float4 t是时间的余弦值，4个分量的值分别是(t/8,t/4,t/2,t) unity_DeltaTime float4 dt是时间增量，4个分量的值分别是(dt,1/dt,smoothDt,1/smoothDt) 11.1 播放序列帧动画 首先，新建material ，新建shader，并赋给一个新建的quad（四边形）。\n上述序列帧动画的精髓在于，我们需要在每个时刻计算该时刻下应该播放的关键帧的位置，并对该关键帧进行纹理采样。打开新建的 Chapter11-ImageSequenceAnimation，删除原有的代码，并添加如下关键代码。\n声明。 1 2 3 4 5 6 7 Properties { _Color (\u0026#34;Color Tint\u0026#34;, Color) = (1, 1, 1, 1) _MainTex (\u0026#34;Image Sequence\u0026#34;, 2D) = \u0026#34;white\u0026#34; {} _HorizontalAmount (\u0026#34;Horizontal Amount\u0026#34;, Float) = 4 _VerticalAmount (\u0026#34;Vertical Amount\u0026#34;, Float) = 4 _Speed (\u0026#34;Speed\u0026#34;, Range(1, 100)) = 30 } MainTex 就是包含了所有关键帧图像的纹理。 HorizontalAmount和 VerticalAmount 分别代表了该图像在水平方向和竖直方向包含的关键帧图像的个数。而Speed 属性用于控制序列帧动画 的播放速度。\n2）序列帧通常透明底，需要渲染透明效果\n1 2 3 4 5 6 7 Tags {\u0026#34;Queue\u0026#34;=\u0026#34;Transparent\u0026#34; \u0026#34;IgnoreProjector\u0026#34;=\u0026#34;True\u0026#34; \u0026#34;RenderType\u0026#34;=\u0026#34;Transparent\u0026#34;} Pass { Tags { \u0026#34;LightMode\u0026#34;=\u0026#34;ForwardBase\u0026#34; } ZWrite Off Blend SrcAlpha OneMinusSrcAlpha 通过blend命令来开启设置透明模式，并关闭深度写入。\n3）顶点着色器\n1 2 3 4 5 6 v2f vert (a2v v) { v2f o; o.pos = UnityObjectToClipPos(v.vertex); o.uv = TRANSFORM_TEX(v.texcoord, _MainTex); return o; } 4）片元着色器\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 fixed4 frag (v2f i) : SV_Target { float time = floor(_Time.y * _Speed); float row = floor(time / _HorizontalAmount); float column = time - row * _HorizontalAmount; //\thalf2 uv = float2(i.uv.x /_HorizontalAmount, i.uv.y / _VerticalAmount); //\tuv.x += column / _HorizontalAmount; //\tuv.y -= row / _VerticalAmount; half2 uv = i.uv + half2(column, -row); uv.x /= _HorizontalAmount; uv.y /= _VerticalAmount; fixed4 c = tex2D(_MainTex, uv); c.rgb *= _Color; return c; } 我们需要计算出每个时刻需要播放的关键帧在纹理中的位置。而由于序列帧纹理都是按行按列排列的，因此这个位置可以认为是该关键帧所在的行列索引数。因此，在上面的代码的前3行中我们计算了行列数，其中使用了Unity的内置时间变量Time。由11.1节可以知道， Time.y就是自该场景加载后所经过的时间。我们首先把Time.y 和速度属性Speed 相乘来得到模拟的时间，并使用CG的oor 函数对结果值取整来得到整数时间time。然后，我们使用time除以HorizontalAmount 的结果值的商来作为当前对应的行索引，除法结果的余数则是列索引。\n5)回调\n1 FallBack \u0026#34;Transparent/VertexLit\u0026#34; 11.2 视差效果 很多2D游戏都使用了不断滚动的背景来模拟游戏角色在场景中的穿梭，这些背景往往包含了多个层(layers)来模拟一种视差效果。\n在Window→LightingSkybox中去掉场景中的天空盒子。由于本例模拟的是 2D游戏中的滚动背景，因此我们需要把摄像机的投影模式设置为正交投影。 (2) 新建一个材质。在本书资源中，该材质名为ScrollingBackgroundMat，新建一个 UnityShader。在本书资源中,该 Shader 名为 Chapter11-ScrollingBackground。把新的 Shader 赋给第2步中创建的材质。 1）声明\n1 2 3 4 5 6 7 8 Properties { _MainTex (\u0026#34;Base Layer (RGB)\u0026#34;, 2D) = \u0026#34;white\u0026#34; {} _DetailTex (\u0026#34;2nd Layer (RGB)\u0026#34;, 2D) = \u0026#34;white\u0026#34; {} _ScrollX (\u0026#34;Base Layer speed\u0026#34;, Float) = 1.0 _Scroll2X (\u0026#34;2nd Layer speed\u0026#34;, Float) = 1.0 _Multiplier(\u0026#34;Layer Multiplier\u0026#34;,Float) = 1 } 其中，MainTex和 DetailTex分别是第一层(较远)和第二层(较近)的背景纹理，而 ScrollX和 Scrol2X对应了各自的水平滚动速度。 Multiplier 参数则用于控制纹理的整体亮度。\n2）顶点着色器\n1 2 3 4 5 6 7 8 9 v2f vert (a2v v) { v2f o; o.pos = UnityObjectToClipPos(v.vertex); o.uv.xy = TRANSFORM_TEX(v.texcoord, _MainTex) + frac(float2(_ScrollX, 0.0) * _Time.y); o.uv.zw = TRANSFORM_TEX(v.texcoord, _DetailTex) + frac(float2(_Scroll2X, 0.0) * _Time.y); return o; } frac是一个 HLSL/Cg 中的内置数学函数，它的作用是提取一个数值的小数部分。frac(x)的数学定义是 x - floor(x)，其中 floor(x)函数返回的是不大于 x的最大整数\n例如：frac（3.7） = 0.7；\n作用：\n随时间偏移：_ScrollX是一个控制滚动速度的属性，_Time.y是自游戏开始以来经过的时间（以秒为单位）。两者相乘 (_ScrollX * _Time.y)得到一个持续线性增大的偏移量。 利用frac实现循环：直接使用这个持续增大的偏移量，纹理坐标会变得非常大，可能导致精度问题或非预期的采样。用 frac对这个偏移量取小数部分，能确保用于实际偏移的值始终被限制在 [0, 1)的范围内。 无缝滚动：当偏移量的整数部分被 frac函数“剥离”后，小数部分会从0增长到接近1，然后瞬间跳回0，接着重新开始增长。这个过程循环往复，就形成了纹理在指定方向上平滑、无限循环的滚动效果，而不会出现任何跳跃或断层。 片元着色器 1 2 3 4 5 6 7 8 9 fixed4 frag (v2f i) : SV_Target { fixed4 firstLayer = tex2D(_MainTex, i.uv.xy); fixed4 secondLayer = tex2D(_DetailTex, i.uv.zw); fixed4 c = lerp(firstLayer, secondLayer, secondLayer.a); c.rgb *= _Multiplier; return c; } lerp函数会根据第三个参数（混合因子，这里用的是secondLayer.a）的值，在前两个参数（firstLayer和secondLayer）之间进行平滑的插值。\n这意味着：\n在secondLayer.a值为0（完全透明）的地方，lerp函数返回firstLayer的颜色，即完全显示第一层背景。 在secondLayer.a值为1（完全不透明）的地方，lerp函数返回secondLayer的颜色，即完全显示第二层背景。 在secondLayer.a值介于0和1之间的地方，则显示两层背景的混合色，实现平滑的过渡效果。这相当于把第二层纹理的Alpha通道当作了一个蒙版，来控制第二层显示多少以及如何与第一层融合 11.3 顶点动画 1）声明\n其中， MainTex是河流纹理，Color用于控制整体颜色，Magnitude 用于控制水流波动的幅度，Frequency用于控制波动频率，InvWaveLength用于控制波长的倒数(InvWaveLength 越大波长越小)，Speed用于控制河流纹理的移动速度。\n我们需要为透明效果设置合适的subshader标签： 在上面的设置中，我们除了为透明效果设置Queue、IgnoreProjector 和RenderType 外，还设置了一个新的标签\u0026ndash;DisableBatching。我们在 3.3.3节中介绍过该标签的含义:一些 Subshader 在使用 Unity 的批处理功能时会出现问题，这时可以通过该标签来直接指明是否对该 SubShader 使用批处理。而这些需要特殊处理的 Shader通常就是指包含了模型空间的顶点动画的 Shader。这是因为，批处理会合并所有相关的模型，而这些模型各自的模型空间就会丢失。而在本例中，我们需要在物体的模型空间下对顶点位置进行偏移。因此，在这里需要取消对该Shader 的批处理操作。\n3）设置pass的状态\n这里关闭了深度写入，开启并设置了混合模式，并关闭了剔除功能。这是为了让水流的每个 面都能显示。\n4）然后，我们在顶点着色器中进行了相关的顶点动画:\n1 2 3 4 5 6 7 8 9 10 11 12 13 v2f vert(a2v v) { v2f o; float4 offset; offset.yzw = float3(0.0, 0.0, 0.0); offset.x = sin(_Frequency * _Time.y + v.vertex.x * _InvWaveLength + v.vertex.y * _InvWaveLength + v.vertex.z * _InvWaveLength) * _Magnitude; o.pos = UnityObjectToClipPos(v.vertex + offset); o.uv = TRANSFORM_TEX(v.texcoord, _MainTex); o.uv += float2(0.0, _Time.y * _Speed); return o; } 我们首先计算顶点位移量。我们只希望对顶点的x方向进行位移，因此yzw的位移量被设置为0。然后，我们利用Frequency属性和内置的Time.y变量来控制正弦函数的频率。为了让不同位置具有不同的位移，我们对上述结果加上了模型空间下的位置分量，并乘以nvWaveLength来控制波长。最后，我们对结果值乘以 Magnitude属性来控制波动幅度，得到最终的位移。剩下的工作，我们只需要把位移量添加到顶点位置上，再进行正常的顶点变换即可。 在上面的代码中，我们还进行了纹理动画，即使用Time.y和 Speed 来控制在水平方向上的纹理动画。\n片元着色 1 2 3 4 5 6 fixed4 frag(v2f i) : SV_Target { fixed4 c = tex2D(_MainTex, i.uv); c.rgb *= _Color.rgb; return c; } 6）最后，我们把 Fallback设置为内置的Transparent/VertexLit(也可以选择关闭Fallback):\n1 Fallback \u0026#34;Transparent/VertexLit\u0026#34; 保存后返回场景，把 Assets/Textures/Chapter11/Water.psd 拖曳到材质的 Main Tex属性上，并调整相关参数。为了让河流更加美观，我们可以复制多个材质并使用不同的参数，再赋给不同的Water模型，就可以得到类似图11.4中的效果。\n顶点动画阴影 如果我们想要对包含了顶点动画的物体添加阴影，那么如果仍然像9.4节中那样使用内置的 Difuse 等包含的阴影 Pass 来渲染，就得不到正确的阴影效果(这里指的是无法向其他物体正确地投射阴影)。\n这是因为，我们讲过Unity的阴影绘制需要调用一个 ShadowCaster Pass，而如果直接使用这些内置的 ShadowCaster Pass,这个 Pass 中并没有进行相关的顶点动画,因此 Unity会仍然按照原来的顶点位置来计算阴影，这并不是我们希望看到的。\n这时，我们就需要提供一个自定义的 ShadowCaster Pass，在这个Pass中，我们将进行同样的顶点变换过程。需要注意的是，在前面的实现中，如果涉及半透明物体我们都把Fallback设置成了Transparent/VertexLit，而[ransparent/VertexLit 没有定义 ShadowCaster Pass，因此也就不会产生阴影(详见9.4.5节)。\n此时没有正确的阴影效果。\n为了正确绘制变形对象的阴影，我们就需要提供自定义的ShadowCasterPass。读者可以在本书资源的Chapter11-VertexAnimationWithShadow中找到对应的Unity Shader。\n核心的shadow caster Pass部分：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 Pass { Tags { \u0026#34;LightMode\u0026#34; = \u0026#34;ShadowCaster\u0026#34; } CGPROGRAM #pragma vertex vert #pragma fragment frag #pragma multi_compile_shadowcaster #include \u0026#34;UnityCG.cginc\u0026#34; float _Magnitude; float _Frequency; float _InvWaveLength; float _Speed; struct v2f { V2F_SHADOW_CASTER; }; v2f vert(appdata_base v) { v2f o; float4 offset; offset.yzw = float3(0.0, 0.0, 0.0); offset.x = sin(_Frequency * _Time.y + v.vertex.x * _InvWaveLength + v.vertex.y * _InvWaveLength + v.vertex.z * _InvWaveLength) * _Magnitude; v.vertex = v.vertex + offset; TRANSFER_SHADOW_CASTER_NORMALOFFSET(o) return o; } fixed4 frag(v2f i) : SV_Target { SHADOW_CASTER_FRAGMENT(i) } ENDCG } 阴影投射的重点在于我们需要按正常Pass的处理来剔除片元或进行顶点动画，以便阴影可以和物体正常渲染的结果相匹配。\n在自定义的阴影投射的Pass 中，我们通常会使用 Unity 提供的内置宏V2F SHADOW CASTER、TRANSFER SHADOW CASTER NORMALOFFSET(旧版本中会使用TRANSFERSHADOWCASTER)和SHADOWCASTERFRAGMENT来计算阴影投射时需要的各种变量，而我们可以只关注自定义计算的部分。在上面的代码中，我们首先在v2f结构体中利用V2F SHADOW CASTER来定义阴影投射需要定义的变量。随后，在顶点着色器中我们首先按之前对顶点的处理方法计算顶点的偏移量，不同的是，我们直接把偏移值加到顶点位置变量中，再使用TRANSFERSHADOWCASTERNORMALOFFSET来让Unity为我们完成剩下的事情。在片元着色器中，我们直接使用SHADOWCASTERFRAGMENT来让Unity自动完成阴影投射的部分，把结果输出到深度图和阴影映射纹理中。\n11.4 广告牌 常见的顶点动画就是广告牌技术(Biboarding)。\n什么是广告牌（BillBoarding）？\n广告牌技术会根据视角方向来旋转一个被纹理着色的多边形(通常就是简单的四边形，这个多边形就是广告牌)，使得多边形看起来好像总是面对着摄像机。\n应用：\n广告牌技术被用于很多应用，比如渲染烟雾、云朵、闪光效果等。\n计算：\n广告牌技术的难点在于，如何根据需求来构建3个相互正交的基向量。\n计算过程通常是，我们首先会通过初始计算得到目标的表面法线(例如就是视角方向)和指向上的方向，而两者往往是不垂直的。但是，两者其中之一是固定的。\n例如当模拟草丛时，我们希望广告牌的指向上的方向永远是(0,1,0)，而法线方向应该随视角变化;而当模拟粒子效果时，我们希望广告牌的法线方向是固定的，即总是指向视角方向，指向上的方向则可以发生变化。\n我们假设法线方向是固定的，首先，我们根据初始的表面法线和指向上的方向来计算出目标方向的指向右的方向 (通过叉积操作):\n1 right = up × normal 对其归一化后，再由法线方向和指向右的方向计算出正交的指向上的方向L:\n1 up\u0026#39; = normal × right 效果：\n正常准备material和shader即可。\n1）声明\n1 2 3 4 5 Properties { _MainTex (\u0026#34;Main Tex\u0026#34;, 2D) = \u0026#34;white\u0026#34; {} _Color (\u0026#34;Color Tint\u0026#34;, Color) = (1, 1, 1, 1) _VerticalBillboarding (\u0026#34;Vertical Restraints\u0026#34;, Range(0, 1)) = 1 } 其中，MainTex是广告牌显示的透明纹理，Color用于控制显示整体颜色 VerticalBilboarding 则用于调整是固定法线还是固定指向上的方向，即约束垂直方向的程度。\n2）在本例中，我们需要为透明效果设置合适的Subshader标签\n1 2 3 4 SubShader { Tags {\u0026#34;Queue\u0026#34;=\u0026#34;Transparent\u0026#34;,\u0026#34;DisableBatching\u0026#34;=\u0026#34;True\u0026#34;, \u0026#34;IqnoreProjector\u0026#34;=\u0026#34;True\u0026#34;,\u0026#34;RenderType\u0026#34;=\u0026#34;Transparent\u0026#34;} 在上面的设置中，我们除了为透明效果设置Queue、IgnoreProjector和RenderType 外，还设置了一个新的标签——DisableBatching。我们在3.3.3节中介绍过该标签的含义。\n为什么不用批处理？\n我们在3.3.3节中介绍过该标签的含义：\n一些SubShader在使用 Unity的批处理功能时会出现问题，这时可以通过该标签来直接指明是否对该SubShader使用批处理。而这些需要特殊处理的 Shader通常就是指包含了模型空间的顶点动画的Shader。这是因为，批处理会合并所有相关的模型，而这些模型各自的模型空间就会被丢失。而在广告牌技术中，我们需要使用物体的模型空间下的位置来作为锚点进行计算。因此。在这里需要取消对该Shader的批处理操作。\n(3)接着，我们设置了Pass 的渲染状态:\n1 2 3 4 5 6 Pass { Tags { \u0026#34;LightMode\u0026#34;=\u0026#34;ForwardBase\u0026#34; } ZWrite Off Blend SrcAlpha OneMinusSrcAlpha Cull Off 关闭了深度写入，开启混合模式，关闭了剔除功能。目的：让广告牌的每个面都能显示。\n4）顶点着色器（核心）。**所有的计算都是在模型空间下进行的。**首先选择模型空间的原点作为广告牌的锚点，并利用内置变量获取模型空间下的视角位置。\n1 2 3 4 5 6 v2f vert (a2v v) { v2f o; // Suppose the center in object space is fixed float3 center = float3(0, 0, 0); float3 viewer = mul(unity_WorldToObject,float4(_WorldSpaceCameraPos, 1)); 然后，我们开始计算3个正交矢量。首先，我们根据观察位置和锚点计算目标法线方向，并根据 VerticalBillboarding属性来控制垂直方向上的约束度。\n1 2 3 4 5 6 7 float3 normalDir = viewer - center; // If _VerticalBillboarding equals 1, we use the desired view dir as the normal dir // Which means the normal dir is fixed // Or if _VerticalBillboarding equals 0, the y of normal is 0 // Which means the up dir is fixed normalDir.y =normalDir.y * _VerticalBillboarding; normalDir = normalize(normalDir); 当 VerticalBillboarding为1时，意味着法线方向固定为视角方向;当VerticalBillboarding 为0时，意味着向上方向固定为(0,1,0)。最后，我们需要对计算得到的法线方向进行归一化操作来得到单位矢量。 接着，我们得到了粗略的向上方向。为了防止法线方向和向上方向平行(如果平行，那么叉积得到的结果将是错误的)，我们对法线方向的y分量进行判断，以得到合适的向上方向。然后，根据法线方向和粗略的向上方向得到向右方向，并对结果进行归一化。但由于此时向上的方向还是不准确的，我们又根据准确的法线方向和向右方向得到最后的向上方向:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 // Get the approximate up dir // If normal dir is already towards up, then the up dir is towards front float3 upDir = abs(normalDir.y) \u0026gt; 0.999 ? float3(0, 0, 1) : float3(0, 1, 0); float3 rightDir = normalize(cross(upDir, normalDir)); upDir = normalize(cross(normalDir, rightDir)); // Use the three vectors to rotate the quad float3 centerOffs = v.vertex.xyz - center; float3 localPos = center + rightDir * centerOffs.x + upDir * centerOffs.y + normalDir * centerOffs.z; o.pos = UnityObjectToClipPos(float4(localPos, 1)); o.uv = TRANSFORM_TEX(v.texcoord,_MainTex); return o; } 5）片元着色器\n1 2 3 4 5 6 fixed4 frag (v2f i) : SV_Target { fixed4 c = tex2D (_MainTex, i.uv); c.rgb *= _Color.rgb; return c; } 6）最后，我们把 Fallback设置为内置的Transparent/VertexLit(也可以选择关闭Fallback):\n1 Fallback \u0026#34;Transparent/VertexLit\u0026#34; 需要说明的是，在上面的例子中，我们使用的是Unity自带的四边形(Quad)来作为广告牌,而不能使用自带的平面(Plane)。\n这是因为，我们的代码是建立在一个竖直摆放的多边形的基础上的，也就是说，这个多边形的顶点结构需要满足在模型空间下是竖直排列的。只有这样，我们才能使用 v.vertex 来计算得到正确的相对于中心的位置偏移量。保存后返回场景，把本书资源中的 Assets/Textures/Chapter11/star.png拖曳到材质的 Main Tex中，即可得到类似图 11.6中的效果。\n","date":"2024-04-20T19:10:20Z","image":"https://Selaphina.github.io/p/11-%E5%8A%A8%E6%80%81%E7%BA%B9%E7%90%86/cover1_hu_5e2014920a4d8f6.png","permalink":"https://Selaphina.github.io/p/11-%E5%8A%A8%E6%80%81%E7%BA%B9%E7%90%86/","title":"11 动态纹理"},{"content":"10 高级纹理 1. 立方体纹理 (Cubemap) 核心概念：\n一种由6张二维纹理构成的特珠资源，分别对应立方体的六个面。采样时提供一个三维方向向量，从立方体中心出发，与某个面相交以获取颜色值。\n应用场景：\n天空盒（Skybox） 环境反射（模拟金属、镜面） 环境折射（模拟透明材质如水、玻璃）. 1.1 天空盒 新建一个材质Skyboxmal，在shader的下拉菜单选择Unity自带的Skybox/6 Sides\n当然，为了更灵活地使用天空盒，可以为不同摄像机设置不同的天空盒颜色。\n将摄像机的camera组件中的clear flags设置为sky box，才可以正确显示天空盒子。\nClear Flags 包含以下4种方式： Skybox：天空盒(默认项)。在屏幕空白处显示当前摄像机的天空盒，如果没有指定天空盒，则会显示默认背景色。\nSolid Color：空白处将显示默认此处设置的背景色。\nDepth only：仅深度，该模式用于对象不被裁剪。\nDon\u0026rsquo;t Clear：不清除，该模式不清楚任何颜色和或深度缓存，但这样做每帧渲染的结果都会叠加在下一帧之上。\n1.2 反射 使用反射效果的物体像镀了一层金属。想要模拟反射效果很简单。我们只需要通过放入射光线和表面法线来计算反射方向，再利用反射方向采样立方体纹理即可。\n首先在unity的脚本手册中给出了如何使用canmera.RenderToCubemap来建立立方体纹理的代码。\n..\\Assets\\Editor\\RenderCubemapWizard.cs\n由于该代码需要添加菜单栏条目，因此需要把它放在Editor文件夹才可运行。\n从Unity菜单栏选择GameObject—\u0026gt;Render into Cubemap.\n可以在窗口看到存进Editor文件夹的脚本。把新建的空物体和Legacy——cubemap（要勾选readable）拖进去。\n点击Render按钮就把场景中指定位置的视角cubemap渲染到cubemap_0中了。\n这时，新建一个material和shader，打开一个Teapot模型，按照正常顺序把material和shader设置给茶壶，同时把cubemap0放进material里即可。\n1.3 折射 折射可以使用斯涅尔定律来计算折射角：\n1）先声明四个新属性\n1 2 3 4 5 6 7 Properties { _Color (\u0026#34;Color Tint\u0026#34;, Color) = (1, 1, 1, 1) _RefractColor (\u0026#34;Refraction Color\u0026#34;, Color) = (1, 1, 1, 1) _RefractAmount (\u0026#34;Refraction Amount\u0026#34;, Range(0, 1)) = 1 _RefractRatio (\u0026#34;Refraction Ratio\u0026#34;, Range(0.1, 1)) = 0.5 _Cubemap (\u0026#34;Refraction Cubemap\u0026#34;, Cube) = \u0026#34;_Skybox\u0026#34; {} } 2）在顶点着色器中，计算折射方向：\n参数1：归一化入射光，参数2：归一化法线，参数3：折射率比值\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 v2f vert(a2v v) { v2f o; o.pos = UnityObjectToClipPos(v.vertex); o.worldNormal = UnityObjectToWorldNormal(v.normal); o.worldPos = mul(unity_ObjectToWorld, v.vertex).xyz; o.worldViewDir = UnityWorldSpaceViewDir(o.worldPos); // Compute the refract dir in world space o.worldRefr = refract(-normalize(o.worldViewDir), normalize(o.worldNormal), _RefractRatio); TRANSFER_SHADOW(o); return o; } 3）在片元着色器中使用折射方向对立方体纹理采样：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 fixed4 frag(v2f i) : SV_Target { fixed3 worldNormal = normalize(i.worldNormal); fixed3 worldLightDir = normalize(UnityWorldSpaceLightDir(i.worldPos)); fixed3 worldViewDir = normalize(i.worldViewDir); fixed3 ambient = UNITY_LIGHTMODEL_AMBIENT.xyz; fixed3 diffuse = _LightColor0.rgb * _Color.rgb * max(0, dot(worldNormal, worldLightDir)); // Use the refract dir in world space to access the cubemap fixed3 refraction = texCUBE(_Cubemap, i.worldRefr).rgb * _RefractColor.rgb; UNITY_LIGHT_ATTENUATION(atten, i, i.worldPos); // Mix the diffuse color with the refract color fixed3 color = ambient + lerp(diffuse, refraction, _RefractAmount) * atten; return fixed4(color, 1.0); } 2. 渲染纹理 (Render Texture) 2.1 渲染目标纹理(Render Target Texture) 现代的 GPU 允许我们把整个三维场景渲染到一个**中间缓冲中，即渲染目标纹理(Render TargeTexture，RTT)**，而不是传统的帧缓冲或后备缓冲(backbufer)。\n与之相关的是多重渲染目标(Multiple Render Target，MRT)，这种技术指的是GPU允许我们把场景同时渲染到多个渲染目标纹理中，而不再需要为每个渲染目标纹理单独渲染完整的场景。延迟渲染就是使用多重渲染目标的一个应用。\n一般来说，延迟渲染使用的Pass数量就是两个。\n第一个Pass：几何通道 (Geometry Pass)\n这个阶段不进行光照计算，收集场景中所有不透明物体表面的原始几何和材质信息，并将其存储到一系列称为G-Buffer（几何缓冲区） 的纹理中。\n第二个Pass：光照通道 (Lighting Pass)\n在这个阶段，利用G-buffer里的各个片元信息，如法线，视角方向，漫反射系数等等，进行真正的光照计算。\n由于 G-Buffer 只存储了最终可见的像素信息，那些被遮挡的（不可见）片段不会进入此阶段，从而避免了大量无效的光照计算\n核心概念：\n一种特殊的纹理类型，其内容并非来自图片文件，而是由一个摄像机的实时渲染结果直接填充。可以理解为一张动态的“画布”。\n应用场景：\n镜子、监控屏幕、画面内显示屏、玻璃折射效果（通过抓取屏幕图像实现扭曲）、延迟渲染等高级特性。\n2.2 镜子效果 Unity定义了一种专门的纹理类型，渲染纹理（Render Target）。Unity中使用渲染纹理的两种方式：\n摄像机的渲染结果实时更新到渲染纹理中。 在屏幕后处理时用GrabPass命令和OnRenderImage函数来获取当前屏幕图像。 这里第一次新建的RT不知为何有bug，会无限重叠渲染。第二次新建的RT就正常了。把源代码的o.uv.x = 1 - o.uv.x;改成o.uv.y = 1 - o.uv.y;了。\n1）新建一个shader和材质MirrorMat，把shader赋给它。\n2）新建一个四边形Quad（Plane之类也可以）作为镜子，把材质MirrorMat赋给它。\n3）Project视图下创建一个渲染纹理（Create——Render Texture）, 该渲染纹理名为 MirrorTexture。\n从镜子平面方向往反射方向看，新建摄像机，把渲染纹理赋给它。 5）把 MirrorTexture赋给镜子材质的main tex中，即可实现。\nshader\u0026rsquo;中，需要对x（根据具体情况可能是y或z）进行反转\n1 2 3 4 5 6 7 8 9 10 v2f vert(a2v v) { v2f o; o.pos = UnityObjectToClipPos(v.vertex); o.uv = v.texcoord; // Mirror needs to filp x o.uv.y = 1 - o.uv.y; return o; } 2.3 玻璃 Cubemap依据1.2中的canmera.RenderToCubemap.cs脚本来实现。\n(1)首先，我们需要声明该Shader使用的各个属性:\n1 2 3 4 5 6 7 Properties { _MainTex (\u0026#34;Main Tex\u0026#34;, 2D) = \u0026#34;white\u0026#34; {} _BumpMap (\u0026#34;Normal Map\u0026#34;, 2D) = \u0026#34;bump\u0026#34; {} _Cubemap (\u0026#34;Environment Cubemap\u0026#34;, Cube) = \u0026#34;_Skybox\u0026#34; {} _Distortion (\u0026#34;Distortion\u0026#34;, Range(0, 100)) = 10 _RefractAmount (\u0026#34;Refract Amount\u0026#34;, Range(0.0, 1.0)) = 1.0 } 其中，MainTex是该玻璃的材质纹理，默认为白色纹理;BumpMap是玻的法线纹理:Cubemap是用于模拟反射的环境纹理;Distortion 则用于控制模拟折射时图像的扭曲程度:RefractAmount 用于控制折射程度，当RefractAmount 值为0时，该玻璃只包含反射效果，当RefractAmount值为1时，该玻璃只包括折射效果。\n(2)定义相应的渲染队列，并使用GrabPass来获取屏幕图像:\n1 2 3 4 5 6 7 SubShader { // We must be transparent, so other objects are drawn before this one. Tags { \u0026#34;Queue\u0026#34;=\u0026#34;Transparent\u0026#34; \u0026#34;RenderType\u0026#34;=\u0026#34;Opaque\u0026#34; } // This pass grabs the screen behind the object into a texture. // We can access the result in the next pass as _RefractionTex GrabPass { \u0026#34;_RefractionTex\u0026#34; } 我们首先在 SubShader 的标签中将渲染队列设置成 Transparent，尽管在后面的 RenderType 被设置为了 Opaque。这两者看似矛盾，但实际上服务于不同的需求。我们在之前说过，把 Queue设置成 Transparent 可以确保该物体渲染时，其他所有不透明物体都已经被渲染到屏幕上了，否则就可能无法正确得到“透过玻璃看到的图像”。而设置RenderType则是为了在使用着色器替换(Shader Replacement)时，该物体可以在需要时被正确渲染。\n3)定义渲染玻璃所需的Pass。为了在Shader 中访问各个属性，我们首先需要定义它们对应的变量。\n1 2 3 4 5 6 7 8 9 sampler2D _MainTex; float4 _MainTex_ST; sampler2D _BumpMap; float4 _BumpMap_ST; samplerCUBE _Cubemap; float _Distortion; fixed _RefractAmount; sampler2D _RefractionTex; float4 _RefractionTex_TexelSize; 需要注意的是，我们还定义了 RefractionTex和RefractionTex TexelSize 变量，这对应了在使用 GrabPass 时指定的纹理名称。RefractionTex TexelSize 可以让我们得到该纹理的纹素大小，例如一个大小为 256x512的纹理，它的纹素大小为(17256,1/512)。我们需要在对屏幕图像的采样坐标进行偏移时使用该变量。\n4）顶点着色器\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 v2f vert (a2v v) { v2f o; o.pos = UnityObjectToClipPos(v.vertex); o.scrPos = ComputeGrabScreenPos(o.pos); o.uv.xy = TRANSFORM_TEX(v.texcoord, _MainTex); o.uv.zw = TRANSFORM_TEX(v.texcoord, _BumpMap); float3 worldPos = mul(unity_ObjectToWorld, v.vertex).xyz; fixed3 worldNormal = UnityObjectToWorldNormal(v.normal); fixed3 worldTangent = UnityObjectToWorldDir(v.tangent.xyz); fixed3 worldBinormal = cross(worldNormal, worldTangent) * v.tangent.w; o.TtoW0 = float4(worldTangent.x, worldBinormal.x, worldNormal.x, worldPos.x); o.TtoW1 = float4(worldTangent.y, worldBinormal.y, worldNormal.y, worldPos.y); o.TtoW2 = float4(worldTangent.z, worldBinormal.z, worldNormal.z, worldPos.z); return o; } 5）片元着色器\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 fixed4 frag (v2f i) : SV_Target {\tfloat3 worldPos = float3(i.TtoW0.w, i.TtoW1.w, i.TtoW2.w); fixed3 worldViewDir = normalize(UnityWorldSpaceViewDir(worldPos)); // Get the normal in tangent space fixed3 bump = UnpackNormal(tex2D(_BumpMap, i.uv.zw));\t// Compute the offset in tangent space float2 offset = bump.xy * _Distortion * _RefractionTex_TexelSize.xy; i.scrPos.xy = offset * i.scrPos.z + i.scrPos.xy; fixed3 refrCol = tex2D(_RefractionTex, i.scrPos.xy/i.scrPos.w).rgb; // Convert the normal to world space bump = normalize(half3(dot(i.TtoW0.xyz, bump), dot(i.TtoW1.xyz, bump), dot(i.TtoW2.xyz, bump))); fixed3 reflDir = reflect(-worldViewDir, bump); fixed4 texColor = tex2D(_MainTex, i.uv.xy); fixed3 reflCol = texCUBE(_Cubemap, reflDir).rgb * texColor.rgb; fixed3 finalColor = reflCol * (1 - _RefractAmount) + refrCol * _RefractAmount; return fixed4(finalColor, 1); } 我们首先通过 TtoW0等变量的w分量得到世界坐标，并用该值得到该片元对应的视角方向。随后，我们对法线纹理进行采样，得到切线空间下的法线方向。我们使用该值和Distortion 属性以及 RefractionTex TexelSize 来对屏幕图像的采样坐标进行偏移，模拟折射效果。 Distortion 值越大，偏移量越大，玻璃背后的物体看起来变形程度越大。在这里，我们选择使用切线空间下的法线方向来进行偏移，是因为该空间下的法线可以反映顶点局部空间下的法线方向。随后，我们对 scrPos透视除法得到真正的屏幕坐标(原理可参见4.9.3节)，再使用该坐标对抓取的屏幕图像RefactionTex进行采样，得到模拟的折射颜色。\n3. 程序纹理 (Procedural Texture) 核心概念：\n其纹理像素颜色并非预先绘制，而是通过计算机算法和数学公式动态生成。搜索结果中未详细展开，但这是其核心特征。\n应用场景：\n生成自然界的不规则图案（如木材、大理石、云朵纹理）、创建无缝贴图、动态效果（如动态变化的能量场）、减少内存占用（无需存储大尺寸位图）。\n在 Unity中，有一类专门使用程序纹理的材质，叫做程序材质(Procedural Materials)。这类材质和我们之前使用的那些材质在本质上是一样的，不同的是，它们使用的纹理不是普通的纹理,而是程序纹理。需要注意的是，程序材质和它使用的程序纹理并不是在Unity中创建的，而是使用了一个名为Substance Designer 的软件在 Unity 外部生成的。\nSubstance Designer 是一个非常出色的纹理生成工具,很多 3A的游戏项目都使用了由它生成的材质。我们可以从Unity的资源商店或网络中获取到很多免费或付费的Substance 材质。这些材质都是以.sbsar 为后缀的，如图10.17所示(资源来源于htps:!/www.assetstore.unity3d.com/en/#!content1352)。我们可以直接把这些材质像其他资源一样拖入Unity项目中。\n当把这些文件导入 Unity 后，Unity 就会生成一个程序纹理资源(Procedural Material Asset)。程序纹理资源可以包含一个或多个程序材质，例如图10.18中就包含了两个程序纹理\u0026ndash;Cereals和 Cereals1，每个程序纹理使用了不同的纹理参数，因此 Unity 为它们生成了不同的程序纹理，例如Cereals Diffuse和Cereals l Diffuse 等。\n","date":"2024-04-02T13:02:00Z","image":"https://Selaphina.github.io/p/10-%E9%AB%98%E7%BA%A7%E7%BA%B9%E7%90%86/cover1_hu_5e2014920a4d8f6.png","permalink":"https://Selaphina.github.io/p/10-%E9%AB%98%E7%BA%A7%E7%BA%B9%E7%90%86/","title":"10 高级纹理"},{"content":"9 复杂光照 9.1 渲染路径 Unity主要有两种渲染路径：\n1.前向渲染路径（Forward Rendering Path）\n2.延迟渲染路径（Deferred Rendering Path）\n3.(已弃用)顶点照明渲染路径（Vertex Lit Rendering Path）\n大多数情况下，一个项目只用一种渲染路径，所以在全局的Edit—Project Setting—Player—Other Settings—Rendering Path中选择。如果有需要使用多个渲染路径，可用多个摄像机的设置中选择渲染路径：\n比如摄像机A渲染的物体使用前向渲染路径，摄像机B渲染的物体使用延迟渲染路径。\n这样就可以覆盖掉Project Setting的设置。\n完成摄像机的设置之后，我们就可以在每个Pass中使用标签来指定该Pass的渲染路径。\n1 2 3 Pass{ Tags { \u0026#34;LightMode\u0026#34; = \u0026#34;ForwardBase\u0026#34; } } 表9.1 LightMode 标签支持的渲染路径设置选项\n标签名 描述 Always 不管使用哪种渲染路径，该 Pass 总是会被渲染，但不会计算任何光照。 ForwardBase 用于前向渲染。该 Pass 会计算环境光、最重要的平行光、逐顶点 / SH 光源和 Lightmaps。 ForwardAdd 用于前向渲染。该 Pass 会计算额外的逐像素光源，每个 Pass 对应一个光源。 Deferred 用于延迟渲染。该 Pass 会渲染 G 缓冲 (G-buffer)。 ShadowCaster 把物体的深度信息渲染到阴影映射纹理 (shadowmap) 或一张深度纹理。 PrepassBase 用于遗留的延迟渲染。该 Pass 会渲染法线和高光反射的指数部分。 PrepassFinal 用于遗留的延迟渲染。该 Pass 通过合并纹理、光照和自发光来渲染得到最后的颜色。 Vertex/VertexLMRGBM和VertexLM 用于遗留的顶点照明渲染 9.2 前向渲染路径（Forwad Rendering Path） 前向渲染路径时传统的渲染方式，也是最常用的渲染路径。\n过程\n每进行一次完整的前向渲染，我们需要渲染该图像的渲染图元，并计算两个缓冲区的信息：\n1 2 3 1.深度缓冲区：利用深度缓冲来决定一个片元是否可见，如果可见就更新颜色缓冲区的颜色值 2.颜色缓冲区：如上。 其中，对每个逐像素光源都要进行一次上述完整渲染流程，如果N个物体，M个光源，则渲染整个场景需要N*M个Pass。因此渲染引擎通常会限制逐像素光照的数目。\n处理光照方式\n在Unity中前向渲染路径有3种处理光照的方式：\n1.逐像素处理\n2.逐顶点处理\n3.球谐函数（SH，Spherical Harmonics）\n一个光源如何决定使用哪种处理光照的方式？\n决定一个光源使用哪种处理模式取决于它的光源类型和渲染模式。\n光源类型：平行光，点光源，等等。\n渲染模式：是指该光源是否重要（Important）。如果把一个光照的模式设置为Important，意味着告诉Unity：嘿老兄，这个光源很重要，我需要你认真对待，把它当成一个逐像素光源来处理。\nUnity的判断规则：\n场景中最亮的平行光总是按逐像素处理。 渲染模式被设置成Not Important的光源，会按逐顶点处理或SH处理。 渲染模式被设置成Important的光源，会按逐像素处理。 如果根据以上规则得到的逐像素光源数量小于Quality Setting里的逐像素光源数量（Pixel Light Count），会有更多的光源以逐像素的方式进行渲染。 通常，对前向渲染来说，一个Unity Shader通常会定义一个Base Pass（Base pass也可以定义多次，例如需要双面渲染的情况。）和一个Additional Pass。\n一个Base Pass仅会执行一次（定义多个Base Pass除外），一个Additional Pass根据逐像素光源的数目多次调用（当然，是能影响到本物体的逐像素光源）。\n9.3 延迟渲染路径（Deferred Pass） 延迟渲染是一种更古老的方法，但为了解决前向渲染的性能瓶颈问题，近几年又流行起来。\n前向渲染的性能瓶颈：计算复杂度随物体和光源数量的乘积增长，实时光源增多时性能下降显著。\n原理\n其核心思想可以概括为“先存储，后计算”，即将几何信息与光照计算分离。除了前向渲染中的颜色缓冲区和深度缓冲区，延迟渲染利用了额外的一个缓冲区：G-buffer（G，Geometry，几何缓冲区）\nG-Buffer 通常包含的数据：\n位置 (Position): 像素在世界空间中的坐标\n法线 (Normal): 物体表面的朝向，用于计算光照角度\n漫反射颜色 (Albedo): 物体表面的基础颜色，不包含光照信息\n高光属性 (Specular): 如高光颜色和光滑度，控制材质反光能力\n一般来说，延迟渲染使用的Pass数量就是两个。\n第一个Pass：几何通道 (Geometry Pass)\n这个阶段不进行光照计算，收集场景中所有不透明物体表面的原始几何和材质信息，并将其存储到一系列称为G-Buffer（几何缓冲区） 的纹理中。\n第二个Pass：光照通道 (Lighting Pass)\n在这个阶段，利用G-buffer里的各个片元信息，如法线，视角方向，漫反射系数等等，进行真正的光照计算。\n由于 G-Buffer 只存储了最终可见的像素信息，那些被遮挡的（不可见）片段不会进入此阶段，从而避免了大量无效的光照计算\n主要局限：\n半透明物体渲染困难。 显存带宽占用高：G-Buffer 包含多张高精度纹理，读写这些数据会消耗大量显存带宽，可能成为性能瓶颈。 不支持真正的抗锯齿功能。 9.2 光源类型 平行光 (Directional Light) 平行光被模拟为来自无穷远处的光源，其发出的所有光线都是相互平行的，类似于太阳光照射到地球的效果。由于光源被视为无限远，所以光线没有衰减，场景中所有物体接收到光照的方向和强度都是均匀一致的\n在图形学和照明设计中，平行光的主要属性是其方向，而非位置。它非常适合用于模拟日光等全局照明，能够为场景提供基础而均匀的照明效果\n点光源 (Point Light) 点光源是从空间中的一个特定点向所有方向均匀发光的光源，类似于一个白炽灯泡。点光源有明确的位置属性，其光照强度会随着传播距离的增加而衰减，通常遵循平方反比定律之类的规律\n点光源是理想化的物理概念，现实中并不存在严格意义上的点光源，但像LED灯珠这样的发光体可以近似看作点光源\n在照明应用中，多个点光源可以组合成阵列，用于建筑轮廓勾勒、大屏幕显示等，实现丰富的动态效果\n聚光灯 (Spot Light) 聚光灯从一个点出发，向特定方向发射出一个锥形的光束，类似于舞台追光灯或手电筒。它同时具有位置和明确的照射方向两个核心属性。\n其光照特性包括锥形光束的照射角度（光束张角）和边缘衰减（光束边缘的光线如何逐渐变弱）\n聚光灯能够产生强烈的中心亮斑和清晰的阴影，非常适合于突出场景中的特定物体或区域，营造戏剧性的视觉效果。在摄影、舞台照明和建筑亮化中应用广泛。\n9.3 光照衰减 如果是平行光的话，衰减值为1.0。如果是其他光源类型，计算会更复杂，包括开根号/除法等等。应对这种计算量相对较大的操作，Unity选择了用一张纹理作为查找表**（Lookup Table, LUT）**.\nLUT计算衰减的局限：\n1.灵活性和直观性上的牺牲。一旦衰减数据被预计算并存储为纹理，你就很难在Shader中动态地改用其他数学公式来实时调整衰减曲线。调试也变得相对不便。\n2.需要预处理得到采样纹理。衰减的精度依赖于预计算纹理的分辨率，低分辨率纹理可能导致精度不足或带状瑕疵；而预处理纹理本身也会占用一定的存储和内存资源\n9.4 Unity的阴影 阴影的原因：\n当一个光源发射的一条光线遇到一个不透明的物体，这条光线就不可以继续照亮其他物体（这里不考虑反射），因此，这个物体就会向它旁边的物体投射阴影。阴影区域的产生是因为光线无法到达这些区域。\n阴影映射纹理（shadowmap）\n在前向渲染路径中，如果场景中最重要的平行光开启了阴影，那么unity为该光源计算它的阴影映射纹理**（shadowmap）。这张阴影映射纹理本质上也是一张深度图，记录了该光源位置出发—能看到的场景中距离它最近的表面位置（深度信息）**。\n如何计算shadowmap呢？\nUnity用一个额外的Pass来专门更新光源的阴影映射纹理。LightMode设置为：ShadowCaster的pass。\n过程：\n1. Unity首先将摄像头放到光源的位置上，判定距离它最近的表面，调用该pass，据此来输出深度信息到阴影纹理映射中。\n2. 因此开启光源的阴影效果后，底层渲染引擎会首先在当前渲染物体的shader中寻找LightMode为ShadowCaster的Pass，如果没有 会在Fallback指定的UnityShader中继续找\n3. 如果没有找到，该物体就无法向其他物体投射阴影。但它依然可以接收来自其他物体的阴影。\n4. 当找到了一个物体确实使用shadowCaster的Pass后，unity就会用该Pass来更新光源的阴影映射纹理。\n总结\n一个物体接收来自其他物体的阴影：就必须在Shader中对阴影映射纹理进行采样，并将采样结果和光照计算结果相乘来得到最终的阴影效果。 一个物体向其他物体投射阴影：必须把该物体加入光源的阴影映射纹理的计算中，从而让其他物体在对阴影映射纹理采样时可以得到该物体的相关信息。 在Unity中：1.首先要开启光源的阴影效果。2.其次要开启物体的投射阴影和接收阴影。\n注意，在默认情况下阴影映射纹理会剔除掉物体的背面，但是对于内置的平面只有一个面，所以，需要设置成tow sides的阴影模式，平面才能正确地向其他物体投射阴影。\n打开帧调试器：windows——analysis——frame debug\n可以看出，绘制该场景一共花19个渲染事件，每一步骤的绘制过程可查看：\n没有在shader中写有关ShadowCaster的Pass，也可以投射阴影，这是因为，回调函数\n1 2 FallBack “Specular” FallBack “Diffuse” 等这些回调内部又会回调：Fallback \u0026quot;VertexLit\u0026quot;顶点光照路径，它可以提供给阴影的计算。\n单独阴影 1.在base pass中内置头文件\n1 2 3 4 5 6 7 8 9 Pass { // Pass for ambient light \u0026amp; first pixel light (directional light) Tags { \u0026#34;LightMode\u0026#34;=\u0026#34;ForwardBase\u0026#34; } // CGPROGRAM // Apparently need to add this declaration // 声明包含阴影纹理映射的头文件 #include \u0026#34;AutoLight.cginc\u0026#34; 2.在结构体v2f中声明一个阴影纹理坐标\n1 2 3 4 5 6 7 struct v2f { float4 pos : SV_POSITION; float3 worldNormal : TEXCOORD0; float3 worldPos : TEXCOORD1; // 内置宏，作用是声明一个阴影纹理坐标 SHADOW_COORDS(2) }; 3.顶点着色器中计算\n1 2 3 4 5 6 7 v2f vert(a2v v) { //…… // 计算阴影纹理坐标 TRANSFER_SHADOW(o); return o; } 4.片元着色器中采样\n1 2 3 4 5 6 7 8 fixed4 frag(v2f i) : SV_Target { //…… //用一张阴影纹理采样阴影映射纹理 fixed shadow = SHADOW_ATTENUATION(i); //将阴影映射结果漫反射+高光的结果相乘。 return fixed4(ambient + (diffuse + specular) * shadow, 1.0); } 统一管理衰减和阴影 1.包含进头文件\n1 2 3 // Need these files to get built-in macros #include \u0026#34;Lighting.cginc\u0026#34; #include \u0026#34;AutoLight.cginc\u0026#34; 2.声明阴影纹理坐标\n1 2 3 4 5 6 7 v2f vert(a2v v) { //…… // Pass shadow coordinates to pixel shader TRANSFER_SHADOW(o); return o; } 3.内置宏TRANSFER_SHADOW\n1 2 3 4 5 6 7 v2f vert(a2v v) { //…… // 计算阴影纹理坐标 TRANSFER_SHADOW(o); return o; } 4.与上一部分阴影计算不同，在内置宏UNITY_LIGHT_ATTENUATION\t来计算阴影+光照衰减\n1 2 3 4 5 6 fixed4 frag(v2f i) : SV_Target { // UNITY_LIGHT_ATTENUATION not only compute attenuation, but also shadow infos UNITY_LIGHT_ATTENUATION(atten, i, i.worldPos); return fixed4(ambient + (diffuse + specular) * atten, 1.0); } 注意这里没有在代码中声明atten，因为内置宏UNITY_LIGHT_ATTENUATION会帮我们声明这个变量。最后我们将（漫反射+高光）*（atten），atten就是阴影\\*光照衰减的结果。\n透明物体的阴影 正常：\n而VertexLit内置的阴影投射无法计算出正确透明物体的阴影。包括透明度测试和透明度混合。\n这是不对的。\n透明度测试 要怎么针对透明物体写阴影投射呢？\n1.声明头文件\n1 2 #include \u0026#34;Lighting.cginc\u0026#34; #include \u0026#34;AutoLight.cginc\u0026#34; 2.在v2f中使用，注意我们在前面的三个纹理坐标声明中，已经占用了3个插值寄存器（TEXCOORD0/TEXCOORD1/TEXCOORD2）。\n1 2 3 4 5 6 7 struct v2f { float4 pos : SV_POSITION; float3 worldNormal : TEXCOORD0; float3 worldPos : TEXCOORD1; float2 uv : TEXCOORD2; SHADOW_COORDS(3) }; 所以在SHADOW_COORDS(3)传入的是3.意味着占用的是第四个插值寄存器TEXCOORD3。\n3.顶点着色器中计算阴影映射纹理并传给片元着色器\n1 2 3 4 5 6 7 v2f vert(a2v v) { //…… // Pass shadow coordinates to pixel shader TRANSFER_SHADOW(o); return o; } 4.片元着色器中，使用内置宏UNITY_LIGHT_ATTENUATION计算阴影和光照衰减\n1 2 3 4 5 6 fixed4 frag(v2f i) : SV_Target { //计算光照衰减和阴影 UNITY_LIGHT_ATTENUATION(atten, i, i.worldPos); return fixed4(ambient + diffuse * atten, 1.0); } 5.注意修改回调函数\n1 2 // FallBack \u0026#34;VertexLit\u0026#34; FallBack \u0026#34;Transparent/Cutout/VertexLit\u0026#34; 而透明度混合 Unity的Transparent回调中，关于半透明的物体的阴影效果为：不会投射阴影。这是Unity因为深度写入和透明度混合之间复杂的问题决定的，个人编写者自然可以通过dirty work来手动为透明度混合的物体写阴影效果，但unity提供的解法就两种：\n1 2 3 FallBack \u0026#34;VertexLit\u0026#34; 或 FallBack \u0026#34;Diffuse\u0026#34;等 像普通不透明物体一样投射阴影。\n1 FallBack \u0026#34;Transparent/VertexLit\u0026#34; 不投射阴影。\n","date":"2024-03-09T10:20:12Z","image":"https://Selaphina.github.io/p/9-%E5%A4%8D%E6%9D%82%E5%85%89%E7%85%A7/cover1_hu_5e2014920a4d8f6.png","permalink":"https://Selaphina.github.io/p/9-%E5%A4%8D%E6%9D%82%E5%85%89%E7%85%A7/","title":"9 复杂光照"},{"content":"8 透明效果 unity中常用两种实现透明效果的方法：\n透明度测试（Alpha Test）\n一种极端霸道的机制。一个片元的透明度判定是否满足条件，如果不满足，则直接舍弃，否则就留下，即按普通的不透明物体处理。要么完全透明（看不到），要么完全不透明。所以透明度测试是不需要关闭深度写入的。\n透明度混合（Alpha Blending）\n这种方法可以得到真正的半透明效果。它会使用当前偏远的透明度作为混合因子，与已经存在颜色缓冲中的颜色值进行混合，得到新颜色。\n注意：透明度混合只关闭了深度写入，没有关闭深度测试。（即深度缓冲对于透明度混合来说，是只读不写的）因为：当透明度混合渲染一个片元时，还是会比较它的深度值和当前深度缓冲中的深度值，如果它的深度距离摄像机比较远，而近处有一个不透明的物体，那么它的颜色就不会在进行混合操作。\n8.1 渲染顺序 渲染顺序很重要。既然如此重要，那为什么需要关闭深度写入呢？\n1 2 3 原因： 如果不关闭深度写入，在一个半透明表面背后的表面本来是可以透过它被我们看见的，但是，在深度测试环节的判定结果是：该半透明表面距离摄像机近。导致该表面背后的所有面都会被剔除。导致我们无法透过半透明表面看到后面的物体了。 Unity 提前定义5个渲染队列\n名称 队列索引号 描述 Background 1000 这个渲染队列会在任何其他队列之前被渲染，我们通常使用该队列来渲染那么些需要绘制在背景上的物体 Geometry 2000 默认的渲染队列。大多数物体都用这个队列，不透明物体用这个队列。 Alpha Test 2450 透明度测试的物体用的队列。Unity 5以后把他从geometry队列中分出，因为在所有不透明物体渲染之后再渲染它会更高效。 Transparent 3000 这个队列中的物体会在所有Geometry和Alphatest队列之后，按照从后往前的顺序渲染。所有使用了透明度混合（关闭深度写入）的物体都应该使用该队列。 Overlay 4000 这个队列用于实现一些叠加效果。任何需要在最后渲染的物体都应该使用该队列。 8.7 双面渲染的透明效果 1. 透明度测试的双面渲染 透明度测试的双面渲染：\n代码见：Chapter8-AlphaTest.shader\n在透明度测试的基础上加上Cull Off即可。\n2. 透明度混合的双面渲染 代码：Chapter8-AlphaBlendBothSided.shader\n因为透明度混合没有深度写入，为了防止渲染顺序出错，用两个Pass，先渲染背面，后渲染前面，把控一下正确的混合顺序。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 Shader \u0026#34;Unity Shaders Book/Chapter 8/Alpha Blend\u0026#34; { Properties { _Color (\u0026#34;Color Tint\u0026#34;, Color) = (1, 1, 1, 1) _MainTex (\u0026#34;Main Tex\u0026#34;, 2D) = \u0026#34;white\u0026#34; {} _AlphaScale (\u0026#34;Alpha Scale\u0026#34;, Range(0, 1)) = 1 } SubShader { Tags {\u0026#34;Queue\u0026#34;=\u0026#34;Transparent\u0026#34; \u0026#34;IgnoreProjector\u0026#34;=\u0026#34;True\u0026#34; \u0026#34;RenderType\u0026#34;=\u0026#34;Transparent\u0026#34;} Pass { Tags { \u0026#34;LightMode\u0026#34;=\u0026#34;ForwardBase\u0026#34; } //先渲染背面（剔除前面） Cull Front // ………… // 和之前一样的代码 } Pass { Tags { \u0026#34;LightMode\u0026#34;=\u0026#34;ForwardBase\u0026#34; } //后渲染前面（剔除后面） Cull Back // ………… // 和之前一样的代码 } FallBack \u0026#34;Transparent/VertexLit\u0026#34; } ","date":"2024-03-06T00:00:00Z","image":"https://Selaphina.github.io/p/hello-world/cover1_hu_5e2014920a4d8f6.png","permalink":"https://Selaphina.github.io/p/hello-world/","title":"8 透明效果"},{"content":"使用 Ziperello https://qweree.cn/index.php/416/ 对加了密码的压缩包进行密码破解，教程如下：\n第一步，双击运行 Ziperello双击我打开程序.exe，如下图：\n第二步，打开一个加了密的 ZIP 压缩包，再选择一个里面的文件，再点击下一步，如图：\n第三步，这里可以选择「暴力破解」方式，继续下一步，如图：\n第四步，如果对压缩包来源没有一点头绪，建议可以选数字、小写字母、大写字母这三种方式组合，毕竟这三种是大家常用的组合，继续下一步，如图：\n第五步，如下图界面我们直接点击开始，程序会自动组合各种密码进行暴力尝试，当然，密码越长、越复杂破解时间也会更长！\n第六步，只需静静等待，直到弹框提示正确密码，大功告成，如图：\n原文链接：https://blog.csdn.net/qq_29856169/article/details/139879447\n","date":"2025-11-14T10:12:30Z","image":"https://Selaphina.github.io/p/%E5%8E%8B%E7%BC%A9%E5%8C%85%E5%AF%86%E7%A0%81%E7%A0%B4%E8%A7%A3-ziperello/CUC%E5%85%91%E6%8D%A2%E7%82%B92_hu_aee3a9923aa263e8.png","permalink":"https://Selaphina.github.io/p/%E5%8E%8B%E7%BC%A9%E5%8C%85%E5%AF%86%E7%A0%81%E7%A0%B4%E8%A7%A3-ziperello/","title":"压缩包密码破解 Ziperello"},{"content":"**非真实感渲染(Non-Photorealistic Rendering , NPR )**的一个主要目标是，使用一些渲染方法使得画面达到和某些特殊的绘画风格相似的效果，例如卡通、水彩风格等。\n1.卡通风格的渲染 卡通风格是游戏中常见的一种渲染风格。使用这种风格的游戏画面通常有一些共有的特点，例如物体都被黑色的线条描边，以及分明的明暗变化等。由日本卡普空(英文名:Capcom)株式会社开发的游戏《大神》(英文名:Okami)就使用了水墨+卡通风格来渲染整个画面。\n除了光照模型不同外，卡通风格通常还需要在物体边缘部分绘制轮廓。在之前的章节中，我们曾介绍使用屏幕后处理技术对屏幕图像进行描边。\n在本节，我们将会介绍基于模型的描边方法，这种方法的实现更加简单，而且在很多情况下也能得到不错的效果。\n要实现卡通渲染有很多方法，其中之一就是使用基于色调的着色技术(tone-based shading)。Gooch 等人在他们1998年的一篇论文!中提出并实现了基于色调的光照模型。\n在实现中，我们往往会使用漫反射系数对一张一维纹理进行采样，以控制漫反射的色调。我们曾在7.3节使用渐变纹理实现过这样的效果。卡通风格的高光效果也和我们之前学习的光照不同。在卡通风格中，模型的高光往往是一块块分界明显的纯色区域。\n1.1 渲染轮廓线的方法 在《Real Time Rendering, 3th》一书中，作者把绘制模型轮廓线的方法分成了 5种类型。\n**基于观察角度和表面法线的轮廓线渲染。**这种方法使用视角方向和表面法线的点乘结果来得到轮廓线的信息。这种方法简单快速，可以在一个Pass中就得到渲染结果，但局限性很大，很多模型渲染出来的描边效果都不尽如人意。 **过程式几何轮廓线渲染。这种方法的核心是使用两个Pass**渲染。第一个Pass渲染背面的面片，并使用某些技术让它的轮廓可见;第二个Pass再正常渲染正面的面片。这种方法的优点在于快速有效，并且适用于绝大多数表面平滑的模型，但它的缺点是不适合类似于立方体这样平整的模型。 **基于图像处理的轮廓线渲染。**我们在第12、13章介绍的边缘检测的方法就属于这个类别。这种方法的优点在于，可以适用于任何种类的模型。但它也有自身的局限所在，一些深度和法线变化很小的轮廓无法被检测出来，例如桌子上的纸张。 **基于轮廓边检测的轮廓线渲染。**上面提到的各种方法，一个最大的问题是，无法控制轮廓线的风格渲染。对于一些情况,我们希望可以渲染出独特风格的轮廓线,例如水墨风格等。为此，我们希望可以检测出精确的轮廓边，然后直接渲染它们。检测一条边是否是轮廓边的公式很简单，我们只需要检查和这条边相邻的两个三角面片是否满足以下条件: $$ (n_0 \\cdot v \u003e 0) \\neq (n_1 \\cdot v \u003e 0) $$其中，n0和n1分别表示两个相邻三角面片的法向，v是从视角到该边上任意顶点的方向。上述公式的本质在于检查两个相邻的三角面片是否一个朝正面、一个朝背面。我们可以在几何着色器(Geometry Shader)的帮助下实现上面的检测过程。\n当然，这种方法也有缺点，除了实现相对复杂外，它还会有动画连贯性的问题。也就是说，由于是逐帧单独提取轮廓，所以在帧与帧之间会出现跳跃性。\n**混合上述的几种渲染方法。**例如，首先找到精确的轮边，把模型和轮廓边渲染到纹理中，再使用图像处理的方法识别出轮廓线，并在图像空间下进行风格化渲染。 本节方法中，使用到的是【过程式几何轮廓线渲染】\n在第一个Pass中，，我们会使用轮廓线颜色来渲染整个背面的面片，并在视角空间下把模型顶点沿着法线方向向外扩张一段距离：\n1 viewPos = viewPos + viewNormal * _Outline; 但是直接使用法线外扩展，对于一些内凹的模型，就可能发生背面面片遮住正面面片的情况。为了避免这种情况，我们要：\n1）对顶点法线在z分量进行处理，等于一个定值。\n2）把法线归一化，然后再对顶点进行扩张。\n1 2 3 viewNormal.z = -0.5; viewNormal = normalize(viewNormal); viewPos = viewPos + viewNormal * _Outline; 1.2 添加高光 前面提到过，卡通风格中的高光往往是模型上一块块分界明显的纯色区域。为了实现这种效果，我们就不能再使用之前学习的光照模型。\n回顾一下，在之前实现 Blinn-Phong 模型的过程中:\n法线 dot 1/2（光照方向+视角方向）+ _Gloss\n1 float spec = pow(max(0,dot(normal,halfDir)), _Gloss); 由于卡通渲染的高光区域是分界明显的纯色区域，我们不能用原先标准的光照自然过渡的样子，而是要把高光截断。具体而言： 1）计算normal和halfDir的结果\n2）把该值和一个阈值比较，如果小于该阈值，则高光反射系数为0；否则返回1.\n1 2 float spec = dot(worldNormal,worldHalfDir); spec = step(threshold, spec); 在上面的代码中，我们使用CG的step函数来实现和值比较的目的。step函数接受两个参数，第一个参数是参考值，第二个参数是待比较的数值。如果第二个参数大于等于第一个参数，则返回1，否则返回0。\n但是，这种粗暴的判断方法会在高光区域的边界造成锯齿，如图14.3左图所示。出现这种问题的原因在于，高光区域的边缘不是平滑渐变的，而是由0突变到1。要想对其进行抗锯齿处理我们可以在边界处很小的一块区域内，进行平滑处理。代码如下:\n1 2 float spec = dot(worldNormal,worldHalfDir); spec=lerp(0,1,smoothstep(-w，w，spec-threshold)); 在上面的代码中，我们没有像之前一样直接使用step函数返回0或1，而是首先使用了CG的 smoothstep函数。其中,w是一个很小的值，当spec-threshold小于-w时，返回0,大于w时,返回1，否则在0到1之间进行插值。这样的效果是，我们可以在[-w,w]区间内，即高光区域的边界处，得到一个从0到1平滑变化的spec值，从而实现抗锯齿的目的。尽管我们可以把w设为一个很小的定值，但在本例中，我们选择使用邻域像素之间的近似导数值，这可以通过CG的fwidth 函数来得到。\n扩展： 如果希望随意伸缩，方块化光照区域，可以参考乐乐姐的这篇：\nhttp://blog.csdn.net/candycat1992/article/details/47284289\nToonShading.shader 1）声明\n1 2 3 4 5 6 7 8 9 10 11 12 Properties{ // ===固有色=== _Color (\u0026#34;Color Tint\u0026#34;,Color) = (1,1,1,1) _MainTex (\u0026#34;Main Tex\u0026#34;,2D) = \u0026#34;white\u0026#34; {} _Ramp (\u0026#34;Ramp Texture\u0026#34;,2D) = \u0026#34;white\u0026#34; {} // ===描边=== _Outline (\u0026#34;Outline\u0026#34;,Range(0,1)) = 0.1 _OutlineColor (\u0026#34;Outline Color\u0026#34;,Color) = (0,0,0,1) // ===高光=== _SpecularColor (\u0026#34;SpecularColor\u0026#34;,Color) = (1,1,1,1) _SpecularScale (\u0026#34;Specular Scale\u0026#34;,Range(0,0.1)) = 0.01 } 其中，Ramp是用于控制漫反射色调的渐变纹理，Outline用于控制轮线宽度，OutlineColor对应了轮廓线颜色，Specular是高光反射颜色，SpecularScale 用于控制计算高光反射时使用的阈值。\n2）定义渲染轮廓线需要的pass，这个pass只渲染北面的三角面片\n1 2 3 4 5 6 7 SubShader { Tags { \u0026#34;RenderType\u0026#34;=\u0026#34;Opaque\u0026#34; \u0026#34;Queue\u0026#34;=\u0026#34;Geometry\u0026#34;} Pass { NAME \u0026#34;OUTLINE\u0026#34; Cull Front Cull Front使用cull指令把正面的三角面片剔除，只渲染背面。值得注意的是，描边在非真实感渲染中是非常常见的，为该Pass定义名称可以让我们在后面的使用中不需要再重复编写此Pass，而只需要调用它的名字即可。\n3）顶点和片元着色器\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 v2f vert (a2v v) { v2f o; float4 pos = mul(UNITY_MATRIX_MV, v.vertex); float3 normal = mul((float3x3)UNITY_MATRIX_IT_MV, v.normal); normal.z = -0.5; pos = pos + float4(normalize(normal), 0) * _Outline; o.pos = mul(UNITY_MATRIX_P, pos); return o; } float4 frag(v2f i) : SV_Target { return float4(_OutlineColor.rgb, 1); } ENDCG } 在顶点着色器中我们首先把顶点和法线变换到视角空间下，这是为了让描边可以在观察空间达到最好的效果。随后，我们设置法线的z分量，对其归一化后再将顶点沿其方向扩张，得到扩张后的顶点坐标。对法线的处理是为了尽可能避免背面扩张后的顶点挡住正面的面片。最后，我们把顶点从视角空间变换到裁剪空间。\n4）定义光照模型所需要的Pass，以渲染模型的正面。\n1 2 3 4 5 6 7 8 9 10 11 12 Pass { Tags { \u0026#34;LightMode\u0026#34;=\u0026#34;ForwardBase\u0026#34; } Cull Back CGPROGRAM #pragma vertex vert #pragma fragment frag #pragma multi_compile_fwdbase 在上面的代码中，我们将LightMode设置为ForwardBase，指令，这些都是为了让Shader中的光照变量可以被正确赋值。并且使用#pragma语句设置了编译。\n5）顶点着色器\n1 2 3 4 5 6 7 8 9 10 11 12 v2f vert (a2v v) { v2f o; o.pos = mul( UNITY_MATRIX_MVP, v.vertex); o.uv = TRANSFORM_TEX (v.texcoord, _MainTex); o.worldNormal = UnityObjectToWorldNormal(v.normal); o.worldPos = mul(_Object2World, v.vertex).xyz; TRANSFER_SHADOW(o); return o; } 在上面的代码中，我们计算了世界空间下的法线方向和顶点位置，并使用Unity提供的内置宏SHADOWCOORDS和TRANSFERSHADOW来计算阴影所需的各个变量。这些宏的实现原理可以参见 9.4节。\n6）片元着色器中包含了计算光照模型的关键代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 float4 frag(v2f i) : SV_Target { fixed3 worldNormal = normalize(i.worldNormal); fixed3 worldLightDir = normalize(UnityWorldSpaceLightDir(i.worldPos)); fixed3 worldViewDir = normalize(UnityWorldSpaceViewDir(i.worldPos)); fixed3 worldHalfDir = normalize(worldLightDir + worldViewDir); fixed4 c = tex2D (_MainTex, i.uv); fixed3 albedo = c.rgb * _Color.rgb; fixed3 ambient = UNITY_LIGHTMODEL_AMBIENT.xyz * albedo; UNITY_LIGHT_ATTENUATION(atten, i, i.worldPos); fixed diff = dot(worldNormal, worldLightDir); diff = (diff * 0.5 + 0.5) * atten; fixed3 diffuse = _LightColor0.rgb * albedo * tex2D(_Ramp, float2(diff, diff)).rgb; fixed spec = dot(worldNormal, worldHalfDir); fixed w = fwidth(spec) * 2.0; fixed3 specular = _Specular.rgb * lerp(0, 1, smoothstep(-w, w, spec + _SpecularScale - 1)) * step(0.0001, _SpecularScale); return fixed4(ambient + diffuse + specular, 1.0); } 7）最后为shader设置了合适的fallback\n1 FallBack \u0026#34;Diffuse\u0026#34; 类似于cocos 的引擎内，其实已经内置了toon shader，可以快速实现这种二分明确/有描边的卡通渲染。\n贴图设置问题，如果出现亮部有黑点，是贴图设置问题。\n微软研究院的Praun等人在2001年的SIGGRAPH 上发表了一篇非常著名的论文。在这篇文章中，他们使用了提前生成的素描纹理来实现实时的素描风格渲染,这些纹理组成了一个色调艺术映射(TonalArt Map,TAM),如图 14.4所示。在图14.4中，从左到右纹理中的笔触逐渐增多，用于模拟不同光照下的漫反射效果，从上到下则对应了每张纹理的多级渐远纹理(mipmaps)。这些多级渐远纹理的生成并不是简单的对上层纹理进行降采样，而是需要保持笔触之间的间隔，以便更真实地模拟素描效果。\n**1.**本文简化了论文的操作方式，不需要用mipmap，使用6张纹理即可\n1 2 3 4 5 6 7 8 9 10 11 Properties { _Color (\u0026#34;Color Tint\u0026#34;, Color) = (1, 1, 1, 1) _TileFactor (\u0026#34;Tile Factor\u0026#34;, Float) = 1 _Outline (\u0026#34;Outline\u0026#34;, Range(0, 1)) = 0.1 _Hatch0 (\u0026#34;Hatch 0\u0026#34;, 2D) = \u0026#34;white\u0026#34; {} _Hatch1 (\u0026#34;Hatch 1\u0026#34;, 2D) = \u0026#34;white\u0026#34; {} _Hatch2 (\u0026#34;Hatch 2\u0026#34;, 2D) = \u0026#34;white\u0026#34; {} _Hatch3 (\u0026#34;Hatch 3\u0026#34;, 2D) = \u0026#34;white\u0026#34; {} _Hatch4 (\u0026#34;Hatch 4\u0026#34;, 2D) = \u0026#34;white\u0026#34; {} _Hatch5 (\u0026#34;Hatch 5\u0026#34;, 2D) = \u0026#34;white\u0026#34; {} } 其中， Color 是用于控制模型颜色的属性。 TileFactor 是纹理的平铺系数， TileFactor 越大,模型上的素描线条越密,在实现图14.5的过程中,我们把 TileFactor设置为8。Hatch0至 Hatch5对应了渲染时使用的6张素描纹理，它们的线条密度依次增大。\n**2.**由于素描风格往往也需要在物体周围渲染轮廓线，因此我们直接使用14.1节中渲染轮廓\n1 2 3 4 5 SubShader { Tags { \u0026#34;RenderType\u0026#34;=\u0026#34;Opaque\u0026#34; \u0026#34;Queue\u0026#34;=\u0026#34;Geometry\u0026#34;} UsePass \u0026#34;Unity Shaders Book/Chapter 14/Toon Shading/OUTLINE\u0026#34; 我们使用 UsePass 命令调用了 14.1节中实现的轮廓线渲染的 Pass,Unity Shaders Book/Chapter14/Toon Shading 对应了14.1 节中 Chapter14-ToonShading 文件里 Shader 的名字，而 Unity 内部会把 Pass的名称全部转成大写格式，所以我们需要在 UsePass 中使用大写格式的 Pass 名称。\n下面，我们需要定义光照模型所在的Pass。为了能够正确获取各个光照变量，我们设置了Pass的标签和相关的编译指令 1 2 3 4 5 6 7 8 9 Pass { Tags { \u0026#34;LightMode\u0026#34;=\u0026#34;ForwardBase\u0026#34; } CGPROGRAM #pragma vertex vert #pragma fragment frag #pragma multi_compile_fwdbase (4)由于我们需要在顶点着色器中计算6张纹理的混合权重，我们首先需要在 v2f结构体中添加相应的变量:\n1 2 3 4 5 6 7 8 struct v2f { float4 pos : SV_POSITION; float2 uv : TEXCOORD0; fixed3 hatchWeights0 : TEXCOORD1; fixed3 hatchWeights1 : TEXCOORD2; float3 worldPos : TEXCOORD3; SHADOW_COORDS(4) }; 由于一共声明了6张纹理，这意味着需要6个混合权重，我们把它们存储在两个fixed3 类型的变量(hatchWeights0 和 hatchWeights1)中。为了添加阴影效果，我们还声明了 worldPos 变量,并使用 SHADOWCOORDS宏声明了阴影纹理的采样坐标。\n5)然后，我们定义了关键的顶点着色器:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 v2f vert(a2v v) { v2f o; o.pos = UnityObjectToClipPos(v.vertex); o.uv = v.texcoord.xy * _TileFactor; fixed3 worldLightDir = normalize(WorldSpaceLightDir(v.vertex)); fixed3 worldNormal = UnityObjectToWorldNormal(v.normal); fixed diff = max(0, dot(worldLightDir, worldNormal)); o.hatchWeights0 = fixed3(0, 0, 0); o.hatchWeights1 = fixed3(0, 0, 0); float hatchFactor = diff * 7.0; if (hatchFactor \u0026gt; 6.0) { // Pure white, do nothing } else if (hatchFactor \u0026gt; 5.0) { o.hatchWeights0.x = hatchFactor - 5.0; } else if (hatchFactor \u0026gt; 4.0) { o.hatchWeights0.x = hatchFactor - 4.0; o.hatchWeights0.y = 1.0 - o.hatchWeights0.x; } else if (hatchFactor \u0026gt; 3.0) { o.hatchWeights0.y = hatchFactor - 3.0; o.hatchWeights0.z = 1.0 - o.hatchWeights0.y; } else if (hatchFactor \u0026gt; 2.0) { o.hatchWeights0.z = hatchFactor - 2.0; o.hatchWeights1.x = 1.0 - o.hatchWeights0.z; } else if (hatchFactor \u0026gt; 1.0) { o.hatchWeights1.x = hatchFactor - 1.0; o.hatchWeights1.y = 1.0 - o.hatchWeights1.x; } else { o.hatchWeights1.y = hatchFactor; o.hatchWeights1.z = 1.0 - o.hatchWeights1.y; } o.worldPos = mul(unity_ObjectToWorld, v.vertex).xyz; TRANSFER_SHADOW(o); return o; } 我们首先对顶点进行了基本的坐标变换。然后，使用TileFactor 得到了纹理采样坐标。在计算6张纹理的混合权重之前，我们首先需要计算逐顶点光照。因此，我们使用世界空间下的光照方向和法线方向得到漫反射系数 diff。\n之后,我们把权重值初始化为0,并把 diff缩放到[0,7]范围，得到 hatchFactor。\n我们把[0,7]的区间均匀划分为7个子区间，通过判断 hatchFactor 所处的子区间来计算对应的纹理混合权重。\n最后，我们计算了顶点的世界坐标，并使用TRANSFERSHADOW宏来计算阴影纹理的采样坐标。\n6.片元着色器\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 fixed4 frag(v2f i) : SV_Target {\tfixed4 hatchTex0 = tex2D(_Hatch0, i.uv) * i.hatchWeights0.x; fixed4 hatchTex1 = tex2D(_Hatch1, i.uv) * i.hatchWeights0.y; fixed4 hatchTex2 = tex2D(_Hatch2, i.uv) * i.hatchWeights0.z; fixed4 hatchTex3 = tex2D(_Hatch3, i.uv) * i.hatchWeights1.x; fixed4 hatchTex4 = tex2D(_Hatch4, i.uv) * i.hatchWeights1.y; fixed4 hatchTex5 = tex2D(_Hatch5, i.uv) * i.hatchWeights1.z; fixed4 whiteColor = fixed4(1, 1, 1, 1) * (1 - i.hatchWeights0.x - i.hatchWeights0.y - i.hatchWeights0.z - i.hatchWeights1.x - i.hatchWeights1.y - i.hatchWeights1.z); fixed4 hatchColor = hatchTex0 + hatchTex1 + hatchTex2 + hatchTex3 + hatchTex4 + hatchTex5 + whiteColor; UNITY_LIGHT_ATTENUATION(atten, i, i.worldPos); return fixed4(hatchColor.rgb * _Color.rgb * atten, 1.0); } 7.合适的fallback\n1 Fallback “Diffuse” ","date":"2024-07-11T22:10:30Z","image":"https://Selaphina.github.io/p/14-%E9%9D%9E%E7%9C%9F%E5%AE%9E%E6%84%9F%E6%B8%B2%E6%9F%93/cover1_hu_5e2014920a4d8f6.png","permalink":"https://Selaphina.github.io/p/14-%E9%9D%9E%E7%9C%9F%E5%AE%9E%E6%84%9F%E6%B8%B2%E6%9F%93/","title":"14 非真实感渲染"},{"content":"记录： 2025.12.03 ColorTransferLib 的主要贡献：“三合一、一站式”\n1.算法大全 把 2001-2023 年公开发表的 11 种颜色迁移、5 种风格迁移、3 种上色算法全部收进一个库，并统一重写成相同接口；既有经典（Reinhard 等）也有最新 SOTA（HistoGAN、DDColor 等）。\n2.数据类型全覆盖 同一套 API 可直接处理图像、视频、光场、体积视频、点云、纹理网格、3D Gaussian Splatting，免去了为不同模态反复改写代码的麻烦。\n3.客观评价基准 内置 20 项无参考/全参考指标（PSNR、LPIPS、VSI、CTQM、NIMA 等），可一次性输出“与参考图颜色一致性、与源图结构保真度、整体感知质量”三类量化结果，方便论文对比和调参。\n一句话：ColorTransferLib 让颜色迁移研究从“零散脚本”变成“统一平台”，开发、对比、评估都能一条命令完成。\n记录：\n(1) 在远端服务器上添加环境目录（因为多用户的conda环境中，非管理员用户没有对conda环境（位于/opt/miniconda3）的写权限。）\n1 2 3 4 EnvironmentNotWritableError: The current user does not have write permissions to the target environment. environment location: /opt/miniconda3 uid: 1004 gid: 1004 正确的做法是：\n1 2 3 4 5 6 7 8 # 1. 创建用户conda目录 mkdir -p ~/.conda/envs # 2. 添加用户目录到conda环境路径（使用--prepend） conda config --prepend envs_dirs ~/.conda/envs # 3. 查看当前的conda环境路径配置 conda config --show envs_dirs (2)直接创建环境并激活安装包\n1 2 3 4 5 6 7 8 9 10 11 12 # 1. 尝试直接创建环境（最简单的方案） conda create -n colorTS python=3.12 # 2. 如果成功，激活并安装包 conda activate colorTS # 3.pip安装colortransfer pip install colortransferlib # 4. conda install -c conda-forge octave ffmpeg 检查一下：\n1 2 3 # 3. 验证安装 octave --version ffmpeg -version 3）git原文\n1 2 3 4 5 # activate octave environment octave # install packages octave:1\u0026gt; pkg install -forge image octave:2\u0026gt; pkg install -forge statistics 实际上直接运行会报错：\n1 2 3 4 5 6 octave:1\u0026gt; pkg install -forge image configure: error: in `/tmp/oct-Z0XxUS/image-2.18.1/src\u0026#39;: configure: error: C++ compiler cannot create executables See `config.log\u0026#39; for more details checking for mkoctfile... /home/wsy/.conda/envs/colortransfer/bin/mkoctfile-10.3.0 checking whether the C++ compiler works... no 这个错误表明在安装 Octave 的 image 包时，缺少必要的 C++ 编译环境。\n所以：\n1 2 # 在 conda 环境中安装必要的编译工具 conda install -c conda-forge gxx_linux-64 make cmake 安装 Octave 开发依赖\n1 2 # 安装 Octave 开发依赖 conda install -c conda-forge octave-devel 安装 image 包所需的依赖\n1 2 # 安装图像处理库 conda install -c conda-forge libtiff libjpeg-turbo libpng 尝试重新安装 image 包\n(3) Run the gbvs_install.m to make the evaluation metric VSI runnable:\n1 2 user@pc:~/\u0026lt;Project Path\u0026gt;/ColorTransferLib/Evaluation/VIS/gbvs$ ocatve octave:1\u0026gt; gbvs_install.m Install via PyPI 1 2 pip install colortransferlib pip install git+https://github.com/facebookresearch/detectron2.git@main Install from source 1 2 3 4 pip install -r requirements/requirements.txt python setup.py bdist_wheel pip install ../ColorTransferLib/dist/ColorTransferLib-2.0.3-py3-none-any.whl pip install git+https://github.com/facebookresearch/detectron2.git@main Usage Color Transfer 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from ColorTransferLib.ColorTransfer import ColorTransfer from ColorTransferLib.DataTypes.Image import Image src = Image(file_path=\u0026#39;/media/source.png\u0026#39;) ref = Image(file_path=\u0026#39;/media/reference.png\u0026#39;) algo = \u0026#34;GLO\u0026#34; ct = ColorTransfer(src, ref, algo) out = ct.apply() # No output file extension has to be given if out[\u0026#34;status_code\u0026#34;] == 0: out[\u0026#34;object\u0026#34;].write(\u0026#34;/media/out\u0026#34;) else: print(\u0026#34;Error: \u0026#34; + out[\u0026#34;response\u0026#34;]) Evaluation 1 2 3 4 5 6 7 8 9 10 from ColorTransferLib.ColorTransfer import ColorTransferEvaluation from ColorTransferLib.DataTypes.Image import Image src = Image(file_path=\u0026#39;/media/source.png\u0026#39;) ref = Image(file_path=\u0026#39;/media/reference.png\u0026#39;) out = Image(file_path=\u0026#39;/media/output.png\u0026#39;) cte = ColorTransferEvaluation(src, ref, out) eva = cte.apply(method) print(eva) Test 1 2 3 4 5 6 7 8 9 10 11 # Test all Color Transfer algorithms with all data type combinations python main.py --test all_CT --out_path \u0026#34;/media/out\u0026#34; # Test all Style Transfer algorithms with all data type combinations python main.py --test all_ST --out_path \u0026#34;/media/out\u0026#34; # Test all Colorization algorithms with all data type combinations python main.py --test all_CT --out_path \u0026#34;/media/out\u0026#34; # Test all evaluation metric on src, ref and out images python main.py --test all_EVAL 2025-12-12 1 2 # 重新安装 detectron2 pip install git+https://github.com/facebookresearch/detectron2.git 还是卡在 detectron2。无进度。\n网络问题实在安装不上，直接下载zip了\n把zip拖进文件夹：\n1 2 # 回到根文件夹 cd ~ 移动文件夹（这个命令甚至是meshy ai的面试题）\n1 mv detectron2-main.zip /workspace/wsy/ 解压\n1 unzip detectron2-main.zip 2.开始远程的本地安装\n1 cd detectron2-main 2025-12-13 新建虚拟环境video_grade\n1 conda create -n video_grade python=3.10 -y ","date":"2025-12-03T10:12:30Z","image":"https://Selaphina.github.io/p/colortransferlab%E8%89%B0%E9%9A%BE%E9%85%8D%E7%8E%AF%E5%A2%83%E4%B8%AD/CUC%E5%85%91%E6%8D%A2%E7%82%B92_hu_aee3a9923aa263e8.png","permalink":"https://Selaphina.github.io/p/colortransferlab%E8%89%B0%E9%9A%BE%E9%85%8D%E7%8E%AF%E5%A2%83%E4%B8%AD/","title":"colorTransferLab艰难配环境中"},{"content":"2023.6.13 【条漫】钢琴湖精灵 2023年，宣推的条漫组创立之初时，我的第一个作品。现在看来青涩而有趣。不过当时绘制这条漫画的高漫板子已经被我出售了，购置了更昂贵的wacom板子，却至今没有像样的作品。两年后的今天，宣推越来越好，我却停在了某处。\n","date":"2025-03-25T22:10:30Z","image":"https://Selaphina.github.io/p/%E9%98%BF%E4%BD%A9%E8%8E%B1%E6%96%AF%E8%B5%B0%E5%BB%8A/cover_hu_854a82e6cab09c6b.png","permalink":"https://Selaphina.github.io/p/%E9%98%BF%E4%BD%A9%E8%8E%B1%E6%96%AF%E8%B5%B0%E5%BB%8A/","title":"阿佩莱斯走廊"}]